<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Physics of Feature Learning 1 – A Perspective from Nonlinearity | Ziming Liu </title> <meta name="author" content="Ziming Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kindxiaoming.github.io/blog/2026/feature-learning-1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script>window.MathJax={tex:{tags:"ams"}};</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziming</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media coverage </a> </li> <li class="nav-item "> <a class="nav-link" href="/talk/">talk </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Physics of Feature Learning 1 – A Perspective from Nonlinearity</h1> <p class="post-meta"> Created in January 01, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/physics-of-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Physics-of-AI</a>   <a href="/blog/tag/non-linearity"> <i class="fa-solid fa-hashtag fa-sm"></i> Non-linearity</a>   <a href="/blog/tag/feature-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature-learning</a>     ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）</strong></p> <hr> <h2 id="motivation">Motivation</h2> <p>Please Read our latest post <a href="/blog/2026/physics-of-ai/">Physics of AI Requires Mindset Shifts</a> for general philosophy.</p> <p>While reading <a href="https://arxiv.org/abs/2509.21519" rel="external nofollow noopener" target="_blank">Yuandong Tian’s work on explaining grokking through feature learning</a>, I found the proposed <em>three-stage dynamics of feature learning</em> particularly intriguing. However, grokking itself has some special characteristics (e.g., the modular addition dataset), which led me to wonder whether feature learning could be studied in a more <em>standard</em> setting. This motivated the low-dimensional regression example explored below.</p> <p>Rather than starting from mathematical derivations, I decided to go straight to experiments. Beyond plotting the loss, we also wanted to define observables that characterize <em>features</em>. One of the most basic observables is the <strong>number of nonlinear neurons</strong>. This measure does not care about the specific form of the features, only whether they are nonlinear.</p> <p>To define neuron activation cleanly, we use <strong>ReLU</strong> activations. A neuron is considered <em>nonlinear</em> if it is activated for some inputs and inactive for others.</p> <hr> <h2 id="problem-setup">Problem Setup</h2> <p>We consider the following regression function:</p> \[y = \sum_{i=1}^{d} \sin^2(f x_i), \quad d = 10,\; f = 10.\] <p>We use an MLP with 10-dimensional input, 100 neurons in the first hidden layer, 100 neurons in the second hidden layer, and a single output. We denote the architecture as ([10, 100, 100, 1]). Inputs are sampled from a standard Gaussian distribution, with a total of 500 samples. The training objective is MSE loss, optimized using <strong>Adam</strong> (learning rate \(10^{-3}\), full batch).</p> <hr> <h2 id="observation-four-phases-during-training">Observation: Four Phases During Training</h2> <p>We find that all 100 neurons in the first hidden layer remain nonlinear throughout training. In contrast, the number of nonlinear neurons in the second hidden layer exhibits <strong>non-trivial dynamics</strong>. Therefore, all plots below focus on the evolution of nonlinear neurons in the second hidden layer.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss-480.webp 480w,/assets/img/blogs/feature-learning-1/loss-800.webp 800w,/assets/img/blogs/feature-learning-1/loss-1400.webp 1400w,/assets/img/blogs/feature-learning-1/loss-1920.webp 1920w,/assets/img/blogs/feature-learning-1/loss-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/feature-learning-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Empirically, the training dynamics appear to consist of four phases:</p> <ul> <li> <p><strong>Phase I (first ~50 steps)</strong><br> The loss decreases rapidly, while the number of active neurons drops sharply.<br> <em>Hypothesis</em>: the model is removing irrelevant features.</p> </li> <li> <p><strong>Phase II (50–500 steps)</strong><br> The loss plateaus around 1, and the number of active neurons remains roughly constant (around 14).<br> <em>Hypothesis</em>: the model is stuck near a saddle point, leading to slow learning.</p> </li> <li> <p><strong>Phase III (500–1500 steps)</strong><br> The loss decreases slowly, while the number of active neurons increases.<br> <em>Hypothesis</em>: this corresponds to genuine feature learning.</p> </li> <li> <p><strong>Phase IV (after ~1500 steps)</strong><br> The loss decreases rapidly (exponential convergence), while the number of active neurons stays constant.<br> <em>Hypothesis</em>: the model is fine-tuning features, or fine-tuning the final linear layer.</p> </li> </ul> <p>There are many interesting questions one could study here.</p> <hr> <h2 id="curiosity-driven-questions">Curiosity-Driven Questions</h2> <ul> <li> <p>Why does Phase I remove so many features?<br> <em>Speculation</em>: is this related to optimization tricks in LLMs? For example, do weight decay and learning-rate warmup mainly serve to prevent excessive feature removal in this phase?</p> </li> <li> <p>Phase II appears “sticky.” How should we better understand the meaning of this solution?<br> <em>Speculation</em>: is the loss plateau simply the result of linear regression?</p> </li> <li> <p>How do nonlinear features emerge in Phase III?</p> </li> <li> <p>What determines the exponential convergence rate in Phase IV?<br> <em>Speculation</em>: once features are learned, does the problem effectively reduce to linear regression, which we know converges exponentially?</p> </li> </ul> <hr> <h2 id="a-linear-regression-perspective">A Linear Regression Perspective</h2> <p>To better understand the dynamics, we analyze the model from a linear regression viewpoint. We compute three linear regression baselines:</p> \[X \to Y,\quad f_1 \to Y,\quad f_2 \to Y,\] <p>where \(f_1\) and \(f_2\) denote the features from the first and second hidden layers, respectively, which evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss_linear_regression-480.webp 480w,/assets/img/blogs/feature-learning-1/loss_linear_regression-800.webp 800w,/assets/img/blogs/feature-learning-1/loss_linear_regression-1400.webp 1400w,/assets/img/blogs/feature-learning-1/loss_linear_regression-1920.webp 1920w,/assets/img/blogs/feature-learning-1/loss_linear_regression-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/feature-learning-1/loss_linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The loss plateau around 1 corresponds closely to the solution of linear regression.</li> <li>In the late stage of training, the blue and orange curves nearly coincide, indicating that the final-layer weights are essentially optimal at every moment, while the features are still being fine-tuned. As a result, the loss continues to decrease.</li> </ul> <p>This is not standard linear regression (where features are fixed and weights evolve), but rather a <strong>dual version</strong>: features evolve while weights remain near their instantaneous optimum. Interestingly, this still leads to exponential decay. A natural next step is to understand the convergence rate quantitatively.</p> <hr> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/17gyczGrRuUAuWYuD_NFB7ZlCAA37p1cK?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Observing interesting phenomena and asking good questions is already half the battle. We are still working toward explaining more of these effects. If you have any idea regarding how to understand some of the phenomena, please email me at lzmsldmjxm@gmail.com</p> <hr> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026feature-learning-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of Feature Learning 1 -- A Perspective from Nonlinearity}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/feature-learning-1/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu (January 2026). Physics of Feature Learning 1 – A Perspective from Nonlinearity. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2026/feature-learning-1/</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/unigram-toy-1/">Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/single-relu-neuron/">Training dynamics of A Single ReLU Neuron</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/induction-head-lr-schedule/">Emergence of Induction Head Depends on Learning Rate Schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/physics-of-ai-recipe/">Physics of AI – How to Begin</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/physics-ml-theory/">A Good ML Theory is Like Physics -- A Physicist's Analysis of Grokking</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ziming Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" crossorigin="anonymous"></script> <script>window.addEventListener("load",function(){console.log("Page loaded. MathJax available:",void 0!==window.MathJax),void 0!==window.MathJax?console.log("MathJax version:",window.MathJax.version):console.error("MathJax failed to load!")});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-media-coverage",title:"media coverage",description:"",section:"Navigation",handler:()=>{window.location.href="/media/"}},{id:"nav-talk",title:"talk",description:"",section:"Navigation",handler:()=>{window.location.href="/talk/"}},{id:"post-transformers-don-39-t-learn-newton-39-s-laws-they-learn-kepler-39-s-laws",title:"Transformers don&#39;t learn Newton&#39;s laws? They learn Kepler&#39;s laws!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/kepler-newton/"}},{id:"post-when-i-say-quot-toy-models-quot-what-do-i-mean",title:"When I say &quot;toy models&quot;, what do I mean?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/toy/"}},{id:"post-on-the-physical-interpretation-of-drifting-generative-models",title:"On the physical interpretation of drifting generative models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-3/"}},{id:"post-physics-2-transformers-fail-to-maintain-physical-cosistency-for-circular-motion",title:"Physics 2 -- Transformers fail to maintain physical cosistency for circular motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-2/"}},{id:"post-physics-1-attention-can-39-t-exactly-simulate-uniform-linear-motion",title:"Physics 1 -- Attention can&#39;t exactly simulate uniform linear motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-1/"}},{id:"post-depth-4-flat-directions-in-weight-space-are-high-frequency-modes-in-function-space",title:"Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-4/"}},{id:"post-depth-3-fun-facts-about-loss-hessian-eigenvalues",title:"Depth 3 -- Fun facts about loss hessian eigenvalues",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-3/"}},{id:"post-diffusion-2-visualizing-flow-matching-temporal-dynamics",title:"Diffusion 2 -- Visualizing flow matching, temporal dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-2/"}},{id:"post-sparse-attention-7-stack-of-causal-attention-creates-implicit-positional-embedding-and-explaning-quot-loss-in-the-middle-quot",title:"Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning &quot;Loss in the middle&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-7/"}},{id:"post-sparse-attention-6-in-context-associative-recall",title:"Sparse attention 6 -- In-context Associative recall",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-6/"}},{id:"post-mlp-2-effective-linearity-generalized-silu",title:"MLP 2 -- Effective linearity, Generalized SiLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-2/"}},{id:"post-mlp-1-gating-is-good-for-polynomials",title:"MLP 1 -- Gating is good for polynomials",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-1/"}},{id:"post-optimization-4-loss-spikes",title:"Optimization 4 -- Loss Spikes",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-4/"}},{id:"post-optimization-3-depth-2-adding-bias-after-relu",title:"Optimization 3 / Depth 2 -- Adding Bias After ReLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-3/"}},{id:"post-optimization-2-elementwise-scale-reparametrization",title:"Optimization 2 -- Elementwise Scale Reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-2/"}},{id:"post-optimization-1-norm-reparametrization",title:"Optimization 1 -- Norm reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-1/"}},{id:"post-sparse-attention-5-attention-sink",title:"Sparse attention 5 -- Attention sink",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-5/"}},{id:"post-bigram-4-on-the-difficulty-of-spatial-map-emergence",title:"Bigram 4 -- On the difficulty of spatial map emergence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-4/"}},{id:"post-depth-1-understanding-pre-ln-and-post-ln",title:"Depth 1 -- Understanding Pre-LN and Post-LN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-1/"}},{id:"post-bigram-3-low-rank-structure",title:"Bigram 3 -- Low Rank Structure",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-3/"}},{id:"post-bigram-2-emergence-of-hyperbolic-spaces",title:"Bigram 2 -- Emergence of Hyperbolic Spaces",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-2/"}},{id:"post-bigram-1-walk-on-a-circle",title:"Bigram 1 -- Walk on a Circle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-1/"}},{id:"post-diffusion-1-sparse-and-dense-neurons",title:"Diffusion 1 -- Sparse and Dense Neurons",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-1/"}},{id:"post-sparse-attention-4-previous-token-head",title:"Sparse attention 4 -- previous token head",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-4/"}},{id:"post-sparse-attention-3-inefficiency-of-extracting-similar-content",title:"Sparse attention 3 -- inefficiency of extracting similar content",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-3/"}},{id:"post-emergence-of-induction-head-depends-on-learning-rate-schedule",title:"Emergence of Induction Head Depends on Learning Rate Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/induction-head-lr-schedule/"}},{id:"post-sparse-attention-2-unattention-head-branching-dynamics",title:"Sparse attention 2 -- Unattention head, branching dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-2/"}},{id:"post-sparse-attention-1-sticky-plateau-and-rank-collapse",title:"Sparse attention 1 -- sticky plateau and rank collapse",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-1/"}},{id:"post-unigram-toy-model-is-surprisingly-rich-representation-collapse-scaling-laws-learning-rate-schedule",title:"Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/unigram-toy-1/"}},{id:"post-fine-tuning-with-sparse-updates-a-toy-teacher-student-setup",title:"Fine-tuning with sparse updates? A toy teacher-student Setup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/fine-tuning-sparsity/"}},{id:"post-multi-head-cross-entropy-loss",title:"Multi-Head Cross Entropy Loss",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/multi-head-cross-entropy/"}},{id:"post-what-39-s-the-difference-physics-of-ai-physics-math-and-interpretability",title:"What&#39;s the difference -- (physics of) AI, physics, math and interpretability",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/ai-physics-interpretability/"}},{id:"post-representation-anisotropy-from-nonlinear-functions",title:"Representation anisotropy from nonlinear functions",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/activation-anisotropy/"}},{id:"post-training-dynamics-of-a-single-relu-neuron",title:"Training dynamics of A Single ReLU Neuron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/single-relu-neuron/"}},{id:"post-physics-of-ai-how-to-begin",title:"Physics of AI \u2013 How to Begin",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-of-ai-recipe/"}},{id:"post-physics-of-feature-learning-1-a-perspective-from-nonlinearity",title:"Physics of Feature Learning 1 \u2013 A Perspective from Nonlinearity",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/feature-learning-1/"}},{id:"post-physics-of-ai-requires-mindset-shifts",title:"Physics of AI Requires Mindset Shifts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/physics-of-ai/"}},{id:"post-achieving-agi-intelligently-structure-not-scale",title:"Achieving AGI Intelligently \u2013 Structure, Not Scale",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/structuralism-ai/"}},{id:"post-philosophical-thoughts-on-kolmogorov-arnold-networks",title:"Philosophical thoughts on Kolmogorov-Arnold Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/kolmogorov-arnold-networks/"}},{id:"post-symbolic-regreesion-structure-regression",title:"Symbolic Regreesion? Structure Regression!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/structure-regression/"}},{id:"post-a-good-ml-theory-is-like-physics-a-physicist-39-s-analysis-of-grokking",title:"A Good ML Theory is Like Physics -- A Physicist&#39;s Analysis of Grokking",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/physics-ml-theory/"}},{id:"project-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"project-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"project-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"project-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"project-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"project-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/talk_1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%6D%6C%69%75@%6D%69%74.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=QeXHxlIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kindxiaoming","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/zimingliu11","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>