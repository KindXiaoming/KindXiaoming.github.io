<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> When should I use physics of AI? | Ziming Liu </title> <meta name="author" content="Ziming Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kindxiaoming.github.io/blog/2026/when-physics-of-ai/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script>window.MathJax={tex:{tags:"ams"}};</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziming</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media coverage </a> </li> <li class="nav-item "> <a class="nav-link" href="/talk/">talk </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">When should I use physics of AI?</h1> <p class="post-meta"> Created in February 15, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/physics-of-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Physics-of-AI</a>   <a href="/blog/tag/methodology"> <i class="fa-solid fa-hashtag fa-sm"></i> Methodology</a>     ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr> <h2 id="motivation">Motivation</h2> <p>A student recently told me, “I’m not as convinced as you are about Physics of AI. I think it may apply to some problems, but not to others.”</p> <p>I paused for a moment—because I completely agree with that assessment. In fact, it is almost too obvious to require explanation. But after further discussion with the student, and some reflection on my own part, I realized something important: perhaps I had overemphasized the <em>methodology</em> of Physics of AI, unintentionally making it seem as if one must adopt this framework to earn my approval.</p> <p>If a particular methodology becomes the only acceptable way to solve problems, then it ceases to be a methodology—it becomes a doctrine, even a religion.</p> <p>So I had a long conversation with the student to clarify my perspective. Some parts of that discussion may be helpful for students who are just starting their research journey, so I decided to write down a few core ideas.</p> <hr> <h2 id="research--problems--solutions">Research = Problems + Solutions</h2> <p>A research project consists of two elements: <strong>problems</strong> and <strong>solutions</strong>. Problems are primary; solutions are secondary.</p> <p>Karl Popper once said:</p> <blockquote> <p><em>We are not students of some subject matter, but students of problems. And problems may cut right across the borders of any subject matter or discipline.</em></p> </blockquote> <p>I find that many students implicitly treat <em>methods/solutions</em> as primary. This is understandable—it is how knowledge is often transmitted. To maximize efficiency, we present polished answers in structured textbooks. But in doing so, we often omit the story of how earlier researchers <em>formulated</em> the problems in the first place.</p> <p>What makes it into textbooks are mature areas of knowledge—areas that may no longer be fertile ground for major discoveries.</p> <p>Even experienced researchers sometimes believe that methods matter more than problems. This is partly due to publication culture: only a small number of breakthrough papers define new problems, while the vast majority solve existing ones. This imbalance exists in almost every field. Beginners may mistakenly conclude that “what most people do” must be “what is most important.”</p> <p>The best research, however, begins with a good problem and then designs a good solution around it. This is similar to product development: without a real user need, a product has no foundation.</p> <p>But as I say this, I realize I risk slipping into dogmatism again. In reality, both research and product design are iterative processes. You rarely start with a perfect problem and a perfect solution. You may begin with a solution (“I have an intuition to add a certain layer to the network”), then ask: <em>What problem does this solve?</em> (“It accelerates training.”) Once you identify a valuable problem, you can revisit your solution and refine it. Iterate the problem; iterate the solution; repeat.</p> <h3 id="how-to-find-problems">How to find problems?</h3> <p>Curiosity is essential. Work on problems you genuinely find interesting. When you have too many ideas, introduce a second criterion: <strong>impact</strong>. (We will discuss how to identify impactful problems later.)</p> <h3 id="how-to-find-solutions">How to find solutions?</h3> <p>Learn from existing approaches (“existing information”), and generate new ideas through thinking and experimentation (“new information”). Research is fundamentally an information acquisition and processing game. The more high-quality information you can obtain per unit time, the stronger your advantage.</p> <p>Physics of AI is simply one methodology for acquiring and processing information. It encourages you not to stay at the surface level (e.g., final model performance), but to dig deeper—measuring more statistics and understanding mechanisms.</p> <p>To be clear, “digging deeper” does <em>not</em> mean that surface-level information is invalid. If tuning hyperparameters alone solves the problem, that’s great. But increasingly, that is no longer sufficient. The era when blind hyperparameter tuning could yield major breakthroughs is largely over. We must either dig deeper (understand the science of AI) or explore sideways (open up new problems).</p> <hr> <h2 id="digging-deeper-the-role-of-physics-of-ai">Digging Deeper: The Role of Physics of AI</h2> <p>Physics of AI is a methodology for digging deeper. The ultimate goal aims to address practical questions (I will define five “levels” of AI research):</p> <ul> <li>Short term（Level 1 or Level 4）：Given an exerperimental setting, how should we tune hyper-parameters?</li> <li>Long term（Level 5）：How do we design new models (creating new experimental settings)?</li> </ul> <p>We can model AI research as follows: given a <strong>configuration</strong> (architecture, hyperparameters, etc.), training produces a network with measurable <strong>metrics</strong>. The forward model of AI research is thus a mapping from configuration space to metric space. As we move up levels, we gain increasingly rich knowledge about this mapping.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels-480.webp 480w,/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels-800.webp 800w,/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels-1400.webp 1400w,/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels-1920.webp 1920w,/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/when-physics-of-ai/physics-of-ai-five-levels.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Level 1:</strong> Measure only performance metrics; tune hyperparameters blindly. (Very small metric space.)</li> <li> <strong>Level 2:</strong> Measure additional metrics (e.g., weight or representation statistics), but without understanding structure. (Expanded metric space, but only isolated experiments.)</li> <li> <strong>Level 3:</strong> Conduct controlled experiments or toy models to understand how configuration affects metrics. (Understanding the structure of the mapping.)</li> <li> <strong>Level 4:</strong> Given an optimization objective, infer the optimal configuration. (Understanding the inverse mapping.)</li> <li> <strong>Level 5:</strong> Design new models and introduce new variables. (Expanding configuration space.)</li> </ul> <p>Historically, Levels 1 and 2 dominated AI research because they were effective. But as the “easy gold” has been mined, we must dig deeper.</p> <p>Level 3 can feel intimidating—it may appear to produce no immediate “practical” output. But I believe that if we endure Level 3 and build a scientific understanding of AI, then Levels 4 and 5 will reveal vast new opportunities.</p> <hr> <h2 id="the-life-cycle-of-research-fields">The Life Cycle of Research Fields</h2> <p>A successful research field typically follows this trajectory:</p> <ol> <li>No one cares.</li> <li>A few people care.</li> <li>Exponential growth.</li> <li>Peak.</li> <li>Gradual decline (or rapid collapse due to a black swan).</li> <li>Enters textbooks.</li> </ol> <p>The optimal time to enter is Stage 2—before consensus forms, but as it begins to emerge. Stage 3 is still good. Stage 1 is often too early; Stage 4 or later may be too late.</p> <p>If a field you are interested in is already at Stage 4, consider deviating from the mainstream and identifying a niche. That niche may still be in Stage 2 or 3.</p> <p>Some students say: “But I’m interested in the mainstream field.” Then ask yourself: is your interest strong enough to endure failure and competition? Entering at Stage 4 means you may struggle to compete, especially against large tech companies.</p> <p>Two additional thoughts about “interest”:</p> <ol> <li>Interest is closely tied to competence. If you discover you are not good at something, your interest may fade.</li> <li>Interest is also tied to familarity (sense of control). Narrow interest (“I must work on this topic”) may reflect limited exposure. Often we lack interest because we lack familarity. If you are familar with a topic but still don’t like it, then the uninterest is real.</li> </ol> <p>My hot take: <strong>research fields do not truly exist.</strong> No two people are interested in exactly the same thing. As <em>Sapiens</em> teaches us, many concepts—including “research fields”—are shared stories. Stories help coordinate collaboration, but internally, do not let them constrain you. Mainstream stories will eventually fade. Learn from previous stories, but tell your own.</p> <hr> <h2 id="final-words-my-research-philosophy">Final Words: My Research Philosophy</h2> <p>Physics of AI is not a doctrine. It is my attempt to synthesize lessons from failures—mine and my collaborators’—into a promising path under current AI development trends. I fully acknowledge that other paths may exist, and that circumstances may change in a few years.</p> <p>Taking action is the ultimate test of truth. Any principle detached from context becomes dogma.</p> <p>My attitude is: understand as many perspectives as possible, <strong>but don’t fall in love with any of them</strong>. Use whatever works in practice. And what “works” is highly context-dependent.</p> <p>Although some classify my style as theoretical, my mindset is fundamentally pragmatic. I value experimentation, observation and induction. In AI research, my philosophy is simple:</p> <blockquote> <p><em>All models are wrong, but some are useful.</em></p> </blockquote> <p>So, returning to the original question: when should we use the Physics of AI methodology?</p> <p>When it works.</p> <p>No single tool solves every problem. But adding another tool to your toolbox increases your probability of solving the problem.</p> <hr> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026when-physics-of-ai</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{When should I use physics of AI?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/when-physics-of-ai/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/physics-ml-theory/">A Good ML Theory is Like Physics -- A Physicist's Analysis of Grokking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/toy/">When I say "toy models", what do I mean?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/ai-physics-interpretability/">What's the difference -- (physics of) AI, physics, math and interpretability</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/physics-of-ai/">Physics of AI Requires Mindset Shifts</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/structure-regression/">Symbolic Regreesion? Structure Regression!</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ziming Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" crossorigin="anonymous"></script> <script>window.addEventListener("load",function(){console.log("Page loaded. MathJax available:",void 0!==window.MathJax),void 0!==window.MathJax?console.log("MathJax version:",window.MathJax.version):console.error("MathJax failed to load!")});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-media-coverage",title:"media coverage",description:"",section:"Navigation",handler:()=>{window.location.href="/media/"}},{id:"nav-talk",title:"talk",description:"",section:"Navigation",handler:()=>{window.location.href="/talk/"}},{id:"post-when-should-i-use-physics-of-ai",title:"When should I use physics of AI?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/when-physics-of-ai/"}},{id:"post-memory-1-how-much-do-linear-layers-memorize",title:"Memory 1 -- How much do linear layers memorize?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/memory-1/"}},{id:"post-transformers-don-39-t-learn-newton-39-s-laws-they-learn-kepler-39-s-laws",title:"Transformers don&#39;t learn Newton&#39;s laws? They learn Kepler&#39;s laws!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/kepler-newton/"}},{id:"post-when-i-say-quot-toy-models-quot-what-do-i-mean",title:"When I say &quot;toy models&quot;, what do I mean?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/toy/"}},{id:"post-on-the-physical-interpretation-of-drifting-generative-models",title:"On the physical interpretation of drifting generative models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-3/"}},{id:"post-physics-2-transformers-fail-to-maintain-physical-cosistency-for-circular-motion",title:"Physics 2 -- Transformers fail to maintain physical cosistency for circular motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-2/"}},{id:"post-physics-1-attention-can-39-t-exactly-simulate-uniform-linear-motion",title:"Physics 1 -- Attention can&#39;t exactly simulate uniform linear motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-1/"}},{id:"post-depth-4-flat-directions-in-weight-space-are-high-frequency-modes-in-function-space",title:"Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-4/"}},{id:"post-depth-3-fun-facts-about-loss-hessian-eigenvalues",title:"Depth 3 -- Fun facts about loss hessian eigenvalues",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-3/"}},{id:"post-diffusion-2-visualizing-flow-matching-temporal-dynamics",title:"Diffusion 2 -- Visualizing flow matching, temporal dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-2/"}},{id:"post-sparse-attention-7-stack-of-causal-attention-creates-implicit-positional-embedding-and-explaning-quot-loss-in-the-middle-quot",title:"Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning &quot;Loss in the middle&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-7/"}},{id:"post-sparse-attention-6-in-context-associative-recall",title:"Sparse attention 6 -- In-context Associative recall",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-6/"}},{id:"post-mlp-2-effective-linearity-generalized-silu",title:"MLP 2 -- Effective linearity, Generalized SiLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-2/"}},{id:"post-mlp-1-gating-is-good-for-polynomials",title:"MLP 1 -- Gating is good for polynomials",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-1/"}},{id:"post-optimization-4-loss-spikes",title:"Optimization 4 -- Loss Spikes",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-4/"}},{id:"post-optimization-3-depth-2-adding-bias-after-relu",title:"Optimization 3 / Depth 2 -- Adding Bias After ReLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-3/"}},{id:"post-optimization-2-elementwise-scale-reparametrization",title:"Optimization 2 -- Elementwise Scale Reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-2/"}},{id:"post-optimization-1-norm-reparametrization",title:"Optimization 1 -- Norm reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-1/"}},{id:"post-sparse-attention-5-attention-sink",title:"Sparse attention 5 -- Attention sink",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-5/"}},{id:"post-bigram-4-on-the-difficulty-of-spatial-map-emergence",title:"Bigram 4 -- On the difficulty of spatial map emergence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-4/"}},{id:"post-depth-1-understanding-pre-ln-and-post-ln",title:"Depth 1 -- Understanding Pre-LN and Post-LN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-1/"}},{id:"post-bigram-3-low-rank-structure",title:"Bigram 3 -- Low Rank Structure",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-3/"}},{id:"post-bigram-2-emergence-of-hyperbolic-spaces",title:"Bigram 2 -- Emergence of Hyperbolic Spaces",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-2/"}},{id:"post-bigram-1-walk-on-a-circle",title:"Bigram 1 -- Walk on a Circle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-1/"}},{id:"post-diffusion-1-sparse-and-dense-neurons",title:"Diffusion 1 -- Sparse and Dense Neurons",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-1/"}},{id:"post-sparse-attention-4-previous-token-head",title:"Sparse attention 4 -- previous token head",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-4/"}},{id:"post-sparse-attention-3-inefficiency-of-extracting-similar-content",title:"Sparse attention 3 -- inefficiency of extracting similar content",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-3/"}},{id:"post-emergence-of-induction-head-depends-on-learning-rate-schedule",title:"Emergence of Induction Head Depends on Learning Rate Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/induction-head-lr-schedule/"}},{id:"post-sparse-attention-2-unattention-head-branching-dynamics",title:"Sparse attention 2 -- Unattention head, branching dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-2/"}},{id:"post-sparse-attention-1-sticky-plateau-and-rank-collapse",title:"Sparse attention 1 -- sticky plateau and rank collapse",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-1/"}},{id:"post-unigram-toy-model-is-surprisingly-rich-representation-collapse-scaling-laws-learning-rate-schedule",title:"Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/unigram-toy-1/"}},{id:"post-fine-tuning-with-sparse-updates-a-toy-teacher-student-setup",title:"Fine-tuning with sparse updates? A toy teacher-student Setup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/fine-tuning-sparsity/"}},{id:"post-multi-head-cross-entropy-loss",title:"Multi-Head Cross Entropy Loss",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/multi-head-cross-entropy/"}},{id:"post-what-39-s-the-difference-physics-of-ai-physics-math-and-interpretability",title:"What&#39;s the difference -- (physics of) AI, physics, math and interpretability",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/ai-physics-interpretability/"}},{id:"post-representation-anisotropy-from-nonlinear-functions",title:"Representation anisotropy from nonlinear functions",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/activation-anisotropy/"}},{id:"post-training-dynamics-of-a-single-relu-neuron",title:"Training dynamics of A Single ReLU Neuron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/single-relu-neuron/"}},{id:"post-physics-of-ai-how-to-begin",title:"Physics of AI \u2013 How to Begin",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-of-ai-recipe/"}},{id:"post-physics-of-feature-learning-1-a-perspective-from-nonlinearity",title:"Physics of Feature Learning 1 \u2013 A Perspective from Nonlinearity",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/feature-learning-1/"}},{id:"post-physics-of-ai-requires-mindset-shifts",title:"Physics of AI Requires Mindset Shifts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/physics-of-ai/"}},{id:"post-achieving-agi-intelligently-structure-not-scale",title:"Achieving AGI Intelligently \u2013 Structure, Not Scale",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/structuralism-ai/"}},{id:"post-philosophical-thoughts-on-kolmogorov-arnold-networks",title:"Philosophical thoughts on Kolmogorov-Arnold Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/kolmogorov-arnold-networks/"}},{id:"post-symbolic-regreesion-structure-regression",title:"Symbolic Regreesion? Structure Regression!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/structure-regression/"}},{id:"post-a-good-ml-theory-is-like-physics-a-physicist-39-s-analysis-of-grokking",title:"A Good ML Theory is Like Physics -- A Physicist&#39;s Analysis of Grokking",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/physics-ml-theory/"}},{id:"project-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"project-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"project-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"project-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"project-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"project-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/talk_1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%6D%6C%69%75@%6D%69%74.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=QeXHxlIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kindxiaoming","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/zimingliu11","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>