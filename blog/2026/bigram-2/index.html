<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bigram 2 -- Emergence of Hyperbolic Spaces | Ziming Liu </title> <meta name="author" content="Ziming Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kindxiaoming.github.io/blog/2026/bigram-2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script>window.MathJax={tex:{tags:"ams"}};</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziming</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media coverage </a> </li> <li class="nav-item "> <a class="nav-link" href="/talk/">talk </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bigram 2 -- Emergence of Hyperbolic Spaces</h1> <p class="post-meta"> Created in January 16, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/physics-of-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Physics-of-AI</a>     ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/bigram-1/">yesterday’s blog post</a>, we studied a simple Markov chain—a random walk on a circle. We found that when the embedding dimension is 2, the model fails to perform the task (even though a circle can, in principle, be perfectly embedded in 2D). However, when the embedding dimension is increased to 4, the model can solve the task perfectly. At the time, we did not understand <em>what</em> the 4D solution was actually doing. This article is an attempt to understand the mechanism behind this 4D algorithm.</p> <hr> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br> We use the same dataset as in the previous blog. Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>The model consists only of an Embedding layer, an Unembedding layer (with tied weights), and a linear layer \(A\) in between. This is equivalent to an Attention layer with context length 1, where \(A = OV\).<br> When there is no linear layer \(A\) (or equivalently \(A = I\)), the model completely fails to perform this task. In that case, the model most likely repeats the current token, rather than predicting the token to its left or right (we will discuss this more carefully at the end of the article, but for now we take this as a given fact). Therefore, the linear layer \(A\) is necessary.</p> <hr> <h2 id="observation-1-a-is-symmetric-and-has-negative-eigenvalues-hyperbolic-directions">Observation 1: \(A\) is symmetric, and has negative eigenvalues (hyperbolic directions)</h2> <p>When \(n_{\rm embd} = 4\), the perplexity can go down to 2 (the best achievable perplexity). By directly inspecting the embeddings or their PCA projections, we fail to observe any obvious pattern.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/embd-480.webp 480w,/assets/img/blogs/bigram-2/embd-800.webp 800w,/assets/img/blogs/bigram-2/embd-1400.webp 1400w,/assets/img/blogs/bigram-2/embd-1920.webp 1920w,/assets/img/blogs/bigram-2/embd-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>However, we notice that the linear matrix \(A\) is close to a symmetric matrix:</p> \[A = \begin{pmatrix} -3.3250 &amp; -2.0423 &amp; 2.0838 &amp; -3.5954 \\ -2.0604 &amp; -3.2431 &amp; -2.8658 &amp; 2.8647 \\ 2.0648 &amp; -2.8236 &amp; -2.5492 &amp; -2.8768 \\ -3.5372 &amp; 2.7750 &amp; -2.8383 &amp; -1.5314 \\ \end{pmatrix}\] <p>A symmetric matrix can be diagonalized over the real numbers. The four eigenvalues consist of two positive and two negative values: \([-5.88, -5.41, 5.42, 6.40],\) all of comparable magnitude. It is therefore more natural to project the embeddings onto the eigen-directions:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/eigen-480.webp 480w,/assets/img/blogs/bigram-2/eigen-800.webp 800w,/assets/img/blogs/bigram-2/eigen-1400.webp 1400w,/assets/img/blogs/bigram-2/eigen-1920.webp 1920w,/assets/img/blogs/bigram-2/eigen-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/eigen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We find that the two negative-eigenvalue directions are highly oscillatory (adjacent tokens have opposite signs), whereas the two positive-eigenvalue directions are much smoother (adjacent tokens have similar values).</p> <p>Consider an input token \(m\) with embedding \(E_m\). Its logit for token \(n\) is given by \(E_n^T A E_m\), which is a quadratic form. Along positive eigen-directions, the logit is <em>lower</em> if two embeddings have opposite signs. In contrast, along negative eigen-directions, the logit is <em>higher</em> if two embeddings have opposite signs. The coexistence of positive and negative eigenvalues effectively turns the embedding space into a hyperbolic space, which can strongly conflict with our Euclidean geometric intuition.</p> <p>More formally, for two vectors \(x, y\) in this space, their similarity can be written as \(L(i,j) \sim -x_1 y_1 - x_2 y_2 + x_3 y_3 + x_4 y_4.\)</p> <p>For nearby tokens, we want the similarity to be large. This can be achieved by having opposite signs along the negative eigen-directions, and the same signs (similar values) along the positive eigen-directions. This mixed strategy makes interpretation difficult: although tokens \(i\) and \(i+1\) correspond to nearby points on the circle, they are not necessarily close in the embedding space, because the negative eigen-directions actively push them as far apart as possible.</p> <p>Two remarks are in order:</p> <p><strong>\(A\) is not necessarily symmetric.</strong><br> In our case, \(A\) is symmetric because the Markov process we study is reversible. In general, \(A\) need not be symmetric, and it is unclear how to deal with a non-symmetric \(A\), where the left and right eigenspaces are no longer aligned.</p> <p><strong>More to understand.</strong><br> So far, we have established that the embedding space is hyperbolic, which is already a somewhat surprising result with potentially significant implications for interpretability. However, many details are still missing, in particular how the tokens are arranged <em>quantitatively</em> within this hyperbolic space.</p> <hr> <h2 id="observation-2-geometry-matters">Observation 2: Geometry matters</h2> <p>We find that \(n_{\rm embd} = 4\) works (i.e., the perplexity converges to 2) only for lucky random seeds. For unlucky random seeds, the perplexity instead converges to around 2.13. What distinguishes these cases? We observe that the geometry—specifically, the number of negative directions—is highly correlated with performance.</p> <p>When there are 2 or 3 negative directions, the optimal perplexity is achievable. With only 1 negative direction, optimization appears to get stuck in a local minimum.</p> <table class="table table-bordered"> <thead> <tr> <th>Random Seed</th> <th>Perplexity</th> <th>Eigenvalues</th> <th>Negative Number</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>2.13</td> <td>-7.6, 5.6, 5.7, 6.7</td> <td>1</td> </tr> <tr> <td>1</td> <td>2.13</td> <td>-7.7, 5.1, 6.1, 6.2</td> <td>1</td> </tr> <tr> <td>2</td> <td>2.13</td> <td>-7.2, 5.7, 6.4, 6.5</td> <td>1</td> </tr> <tr> <td>3</td> <td>2.00</td> <td>-6.1, -5.8, -4.4, 5.6</td> <td>3</td> </tr> <tr> <td>4</td> <td>2.00</td> <td>-5.9, -5.4, 5.4, 6.4</td> <td>2</td> </tr> <tr> <td>5</td> <td>2.00</td> <td>-6.7, -6.0, -5.7, 6.5</td> <td>3</td> </tr> <tr> <td>6</td> <td>2.10</td> <td>-7.0, 5.3, 5.7, 6.2</td> <td>1</td> </tr> <tr> <td>7</td> <td>2.00</td> <td>-5.7, -5.2, -4.9, 5.4</td> <td>3</td> </tr> <tr> <td>8</td> <td>2.00</td> <td>-5.8, -4.8, 4.6, 5.9</td> <td>2</td> </tr> <tr> <td>9</td> <td>2.10</td> <td>-7.0, 5.1, 6.1, 7.1</td> <td>1</td> </tr> <tr> <td>42</td> <td>2.10</td> <td>-7.1, 6.0, 6.3, 6.8</td> <td>1</td> </tr> <tr> <td>2026</td> <td>2.00</td> <td>-5.1, -4.8, 4.7, 5.8</td> <td>2</td> </tr> </tbody> </table> <hr> <h2 id="sanity-check-why-do-we-need-a">Sanity check: why do we need \(A\)?</h2> <p>When we remove \(A\) or set \(A = I\), the perplexity remains far above 2, regardless of how large the embedding dimension is. When \(A = I\) (so the embedding space is purely Euclidean), an input token has no mechanism to “un-attend” to itself. In contrast, a negative eigen-direction can serve precisely this un-attention role, a mechanism also explored in <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>. As a result, \(A\) is essential for performing the random-walk task.</p> <p>That said, when \(A = I\) (i.e., attention is completely removed) and \(n_{\rm embd} = 2\), the embeddings can evolve into extremely wild (and aesthetically pleasing) patterns. Even though these patterns may not shed much light on what is really happening in language models, it is pure pleasure to watch the resulting animations.</p> <p>Seed = 0</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_0-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_0-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_0-1400.webp 1400w,/assets/img/blogs/bigram-2/gif_seed_0-1920.webp 1920w,/assets/img/blogs/bigram-2/gif_seed_0-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/gif_seed_0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Seed = 1</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_1-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_1-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_1-1400.webp 1400w,/assets/img/blogs/bigram-2/gif_seed_1-1920.webp 1920w,/assets/img/blogs/bigram-2/gif_seed_1-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/gif_seed_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Seed = 4</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_4-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_4-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_4-1400.webp 1400w,/assets/img/blogs/bigram-2/gif_seed_4-1920.webp 1920w,/assets/img/blogs/bigram-2/gif_seed_4-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/gif_seed_4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Seed = 6</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_6-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_6-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_6-1400.webp 1400w,/assets/img/blogs/bigram-2/gif_seed_6-1920.webp 1920w,/assets/img/blogs/bigram-2/gif_seed_6-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/gif_seed_6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Seed = 9</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_9-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_9-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_9-1400.webp 1400w,/assets/img/blogs/bigram-2/gif_seed_9-1920.webp 1920w,/assets/img/blogs/bigram-2/gif_seed_9-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bigram-2/gif_seed_9.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NmfqzshitvHwZZ3RNobwlJNH2Ebrlqj0?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>.</p> <hr> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 2 -- Emergence of Hyperbolic Spaces}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-2/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/bigram-1/">Bigram 1 -- Walk on a Circle</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/unigram-toy-1/">Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/bigram-4/">Bigram 4 -- On the difficulty of spatial map emergence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/sparse-attention-4/">Sparse attention 4 -- previous token head</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/sparse-attention-2/">Sparse attention 2 -- Unattention head, branching dynamics</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ziming Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" crossorigin="anonymous"></script> <script>window.addEventListener("load",function(){console.log("Page loaded. MathJax available:",void 0!==window.MathJax),void 0!==window.MathJax?console.log("MathJax version:",window.MathJax.version):console.error("MathJax failed to load!")});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-media-coverage",title:"media coverage",description:"",section:"Navigation",handler:()=>{window.location.href="/media/"}},{id:"nav-talk",title:"talk",description:"",section:"Navigation",handler:()=>{window.location.href="/talk/"}},{id:"post-when-i-say-quot-toy-models-quot-what-do-i-mean",title:"When I say &quot;toy models&quot;, what do I mean?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/toy/"}},{id:"post-on-the-physical-interpretation-of-drifting-generative-models",title:"On the physical interpretation of drifting generative models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-3/"}},{id:"post-physics-2-transformers-fail-to-maintain-physical-cosistency-for-circular-motion",title:"Physics 2 -- Transformers fail to maintain physical cosistency for circular motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-2/"}},{id:"post-physics-1-attention-can-39-t-exactly-simulate-uniform-linear-motion",title:"Physics 1 -- Attention can&#39;t exactly simulate uniform linear motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-1/"}},{id:"post-depth-4-flat-directions-in-weight-space-are-high-frequency-modes-in-function-space",title:"Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-4/"}},{id:"post-depth-3-fun-facts-about-loss-hessian-eigenvalues",title:"Depth 3 -- Fun facts about loss hessian eigenvalues",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-3/"}},{id:"post-diffusion-2-visualizing-flow-matching-temporal-dynamics",title:"Diffusion 2 -- Visualizing flow matching, temporal dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-2/"}},{id:"post-sparse-attention-7-stack-of-causal-attention-creates-implicit-positional-embedding-and-explaning-quot-loss-in-the-middle-quot",title:"Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning &quot;Loss in the middle&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-7/"}},{id:"post-sparse-attention-6-in-context-associative-recall",title:"Sparse attention 6 -- In-context Associative recall",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-6/"}},{id:"post-mlp-2-effective-linearity-generalized-silu",title:"MLP 2 -- Effective linearity, Generalized SiLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-2/"}},{id:"post-mlp-1-gating-is-good-for-polynomials",title:"MLP 1 -- Gating is good for polynomials",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-1/"}},{id:"post-optimization-4-loss-spikes",title:"Optimization 4 -- Loss Spikes",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-4/"}},{id:"post-optimization-3-depth-2-adding-bias-after-relu",title:"Optimization 3 / Depth 2 -- Adding Bias After ReLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-3/"}},{id:"post-optimization-2-elementwise-scale-reparametrization",title:"Optimization 2 -- Elementwise Scale Reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-2/"}},{id:"post-optimization-1-norm-reparametrization",title:"Optimization 1 -- Norm reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-1/"}},{id:"post-sparse-attention-5-attention-sink",title:"Sparse attention 5 -- Attention sink",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-5/"}},{id:"post-bigram-4-on-the-difficulty-of-spatial-map-emergence",title:"Bigram 4 -- On the difficulty of spatial map emergence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-4/"}},{id:"post-depth-1-understanding-pre-ln-and-post-ln",title:"Depth 1 -- Understanding Pre-LN and Post-LN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-1/"}},{id:"post-bigram-3-low-rank-structure",title:"Bigram 3 -- Low Rank Structure",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-3/"}},{id:"post-bigram-2-emergence-of-hyperbolic-spaces",title:"Bigram 2 -- Emergence of Hyperbolic Spaces",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-2/"}},{id:"post-bigram-1-walk-on-a-circle",title:"Bigram 1 -- Walk on a Circle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-1/"}},{id:"post-diffusion-1-sparse-and-dense-neurons",title:"Diffusion 1 -- Sparse and Dense Neurons",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-1/"}},{id:"post-sparse-attention-4-previous-token-head",title:"Sparse attention 4 -- previous token head",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-4/"}},{id:"post-sparse-attention-3-inefficiency-of-extracting-similar-content",title:"Sparse attention 3 -- inefficiency of extracting similar content",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-3/"}},{id:"post-emergence-of-induction-head-depends-on-learning-rate-schedule",title:"Emergence of Induction Head Depends on Learning Rate Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/induction-head-lr-schedule/"}},{id:"post-sparse-attention-2-unattention-head-branching-dynamics",title:"Sparse attention 2 -- Unattention head, branching dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-2/"}},{id:"post-sparse-attention-1-sticky-plateau-and-rank-collapse",title:"Sparse attention 1 -- sticky plateau and rank collapse",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-1/"}},{id:"post-unigram-toy-model-is-surprisingly-rich-representation-collapse-scaling-laws-learning-rate-schedule",title:"Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/unigram-toy-1/"}},{id:"post-fine-tuning-with-sparse-updates-a-toy-teacher-student-setup",title:"Fine-tuning with sparse updates? A toy teacher-student Setup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/fine-tuning-sparsity/"}},{id:"post-multi-head-cross-entropy-loss",title:"Multi-Head Cross Entropy Loss",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/multi-head-cross-entropy/"}},{id:"post-what-39-s-the-difference-physics-of-ai-physics-math-and-interpretability",title:"What&#39;s the difference -- (physics of) AI, physics, math and interpretability",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/ai-physics-interpretability/"}},{id:"post-representation-anisotropy-from-nonlinear-functions",title:"Representation anisotropy from nonlinear functions",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/activation-anisotropy/"}},{id:"post-training-dynamics-of-a-single-relu-neuron",title:"Training dynamics of A Single ReLU Neuron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/single-relu-neuron/"}},{id:"post-physics-of-ai-how-to-begin",title:"Physics of AI \u2013 How to Begin",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-of-ai-recipe/"}},{id:"post-physics-of-feature-learning-1-a-perspective-from-nonlinearity",title:"Physics of Feature Learning 1 \u2013 A Perspective from Nonlinearity",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/feature-learning-1/"}},{id:"post-physics-of-ai-requires-mindset-shifts",title:"Physics of AI Requires Mindset Shifts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/physics-of-ai/"}},{id:"post-achieving-agi-intelligently-structure-not-scale",title:"Achieving AGI Intelligently \u2013 Structure, Not Scale",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/structuralism-ai/"}},{id:"post-philosophical-thoughts-on-kolmogorov-arnold-networks",title:"Philosophical thoughts on Kolmogorov-Arnold Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/kolmogorov-arnold-networks/"}},{id:"post-symbolic-regreesion-structure-regression",title:"Symbolic Regreesion? Structure Regression!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/structure-regression/"}},{id:"post-a-good-ml-theory-is-like-physics-a-physicist-39-s-analysis-of-grokking",title:"A Good ML Theory is Like Physics -- A Physicist&#39;s Analysis of Grokking",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/physics-ml-theory/"}},{id:"project-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"project-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"project-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"project-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"project-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"project-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/talk_1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%6D%6C%69%75@%6D%69%74.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=QeXHxlIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kindxiaoming","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/zimingliu11","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>