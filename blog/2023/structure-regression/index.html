<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Symbolic Regreesion? Structure Regression! | Ziming Liu </title> <meta name="author" content="Ziming Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kindxiaoming.github.io/blog/2023/structure-regression/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script>window.MathJax={tex:{tags:"ams"}};</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziming</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media coverage </a> </li> <li class="nav-item "> <a class="nav-link" href="/talk/">talk </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Symbolic Regreesion? Structure Regression!</h1> <p class="post-meta"> Created in July 08, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> ¬† ¬∑ ¬† <a href="/blog/tag/modularity"> <i class="fa-solid fa-hashtag fa-sm"></i> Modularity</a> ¬† <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural-Networks</a> ¬† <a href="/blog/tag/neuroscience-for-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Neuroscience-for-AI</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/interpretability"> <i class="fa-solid fa-tag fa-sm"></i> Interpretability</a> ¬† </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Many scientific problems can be formulated as regression: given independent variables \((x_1, x_2, \cdots, x_d)\) and dependent variable \(y\), we want to find a function such that \(y = f(x_1,x_2,\cdots, x_d)\). Scientists, especially physicists, have put great effort and labor into solving tasks of this kind. For example, Kepler spent eight years staring at astronomical data, before he figured out his eponymous three laws. By contrast, many scientists are less crazy about symbolic formulas. They are content with empirical laws. To be specific, they set \(f\) to be a specific functional form, allowing some tunable empirical parameters, which may not have very clear physical meanings.</p> <p>Two goals mentioned above, symbolic regression (SR) and empirical regression (ER), have their own limitations: SR is powerful but brittle, while ER is robust but constrained. Is it possible to have something in the middle, which is both powerful and robust? The answer is structure regression (StR)! This blog is organized as such: Firstly, I discuss what is structure regression, arguing why structure regression is probably a better goal to pursue than symbolic regression. Secondly, I describe our method BIMT to do structure regression. Finally, I bet on scientific fields that structure regression is promising for, and most importantly, call for collaboration!</p> <h2 id="symbol-or-structure">Symbol or Structure?</h2> <p>Although symbols play huge roles in mathematics and physics, I am not a big believer for ‚Äúeverything is symbolic‚Äù. For example, only very few unary functions are labeled as ‚Äúsymbolic‚Äù or ‚Äúanalytic‚Äù, such as \(f(x)=x^2, {\rm sin}(x), {\rm exp}(x)\). If one randomly draw a 1D curve on a piece of paper, only probability one the function is symbolic. ‚ÄúSymbolic functions‚Äù defined by us are too limited. The definition can also strongly depend on contexts: hypergeometric functions may be viewed as symbolic in mathematical physics, but may be unacceptably too complicated in engineering.</p> <p>In contrast to symbols, structures are probably more universal. An example of structure is independence. Independence is what makes physics possible at all: our universe has infinite degrees of freedom, but physical systems we care about only depend on a finite number of them. A similar structural property is modularity, which allows us to decompose a huge object into small pieces which are much more manageable. Other examples of structural properties are hierarchy, compositionality, reusability, sparsity etc.</p> <p>So, what does structure regression mean? Given independent variables \((x_1, x_2, \cdots, x_d)\) and dependent variable y, we want to find a structure (or structures) such that \(y = S(x_1,x_2,\cdots, x_d)\). Suppose the structure is additive modularity, then \(S(x_1,x_2,\cdots, x_d)=\sum_{i=1}^d f_i(x_i)\). Note that figuring out symbolic forms of \(f_i (i=1,2,\cdots,d)\) is not necessary for structure regression. As long as the additive property is discovered, a victory is claimed.</p> <p>One may say that structure regression is a weaker or less ambitious version of symbolic regression. This is true, but for good reasons. Firstly, as I argued above, there are cases where symbolic regression is impossible. If so, structure regression is probably the best thing one can hope for! Secondly, for cases where symbolic regression are indeed possible, structure regression is a nice intermediate milestone to target for, because it greatly simplifies the symbolic regression problem (e.g., AI Feynman).</p> <p>One may feel that my critiques for symbolic regression can directly apply to structure regression: ‚Äúthe search space for symbolic regression is (discrete) symbolic formulas, the search space for structure regression is (discrete) structures. In both cases, you need to specify your symbols/structures (which can be limited and context-dependent), and combinatorial searches are needed.‚Äù This is not true. There are key differences between symbolic regression and structure regression. Let‚Äôs say we take a neural network. Structure regression only cares about the graph of how neurons connect to each other. In addition to that, symbolic regression also cares about how each neuron processes signals. Structural regression is more robust than symbolic regression: Remember how in condensed matter physics or in many emergent phenomenon, robust/universal macroscopic behavior is usually only dependent on the relations of microscopic units, but the details of each unit are not so relevant. Moreover, structure regression can be made differentiable and can be easily visualized. This is not obvious at all, but in the following I will describe a machine learning method for structure regression that mets these desirable properties.</p> <h2 id="structure-regression-with-bimt-brain-inspired-modular-training">Structure regression with BIMT (Brain-inspired Modular Training)</h2> <p>Before introducing our method, let‚Äôs see what our method can achieve. For symbolic formulas with structural properties, our trained fully-connected neural networks display interpretable modules. You can understand the structures immediately after seeing the connectivity graph for the network!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bimt_example-480.webp 480w,/assets/img/blogs/bimt_example-800.webp 800w,/assets/img/blogs/bimt_example-1400.webp 1400w,/assets/img/blogs/bimt_example-1920.webp 1920w,/assets/img/blogs/bimt_example-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bimt_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We present three examples here. The first example is independence. Two outputs depend on non-overlapping sets of input variables. Our method is able to split the network into two independent parts. The second example is feature sharing. We‚Äôd expect that \(x_i^2 (i=1,2,3)\) are important intermediate features. Our method is able to recover this intuition. The third example is compositionality. We‚Äôd expect to compute sum of squares first, and then take the square root. Our method automatically discovers this structure too. Note that all input and output variables are numeric (nothing symbolic). With our training method, neural networks self-reveal their structures, i.e., structure regression is achieved.</p> <p>Now it‚Äôs time to describe our method BIMT (Brain-inspired modular training). For technical details please refer to the paper, or the podcast. Here we do a quick walk through the basic idea.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bimt-480.webp 480w,/assets/img/blogs/bimt-800.webp 800w,/assets/img/blogs/bimt-1400.webp 1400w,/assets/img/blogs/bimt-1920.webp 1920w,/assets/img/blogs/bimt-2560.webp 2560w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/bimt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Our goal is to encourage modules to emerge when training (non-modular) neural networks. Remember that human brains are remarkably modular. Is there a lesson we can learn from our brains? Our brains are modular because modular brains require less energy and react faster than non-modular brains, hence have survival advantages in evolution. Modular brains have more local connections than non-local connections. To introduce the notion of ‚Äúlocality‚Äù into artificial neural networks, we embed NNs into geometric spaces (e.g., 2D Euclidean space), assigning each neuron a spatial coordinate and defining lengths for neuron connections. We penalize longer connections more than shorter connections in training. Specifically, our penalty term is a simple modification to \(L_1\) regularization: \(L_1\) penalty is \(\lambda \lvert w\rvert\), while our penalty is \(\lambda \ell \lvert w\rvert\) where \(w\), \(\ell\) are the weight and the length of the neuron connection, respectively. Besides this differentiable penalty term added to the training objective to encourage locality, we also allow swaps of neurons, to avoid topological issues.</p> <h2 id="structure-regression-for-science">Structure regression for science</h2> <p>There are two main paradigms for science: model-driven and data-driven. Model-driven approaches start from (symbolic) models, predicting what will happen via deduction. Data-driven approaches start from data, predicting what will happen via induction. Each paradigm has its pros and cons: Model-driven methods are interpretable, but require creativity of researchers. Data-driven methods are less intellectually challenging, but may not be interpretable. Usually these two paradigms are separately applied. Recently there have been efforts to integrate models into data-driven approches (e.g., PINNs), but not the other way around, i.e., use data-driven methods to inspire models development. Structure regression can exactly do that.</p> <p>Putting interpretability aside, encouraging structures in neural networks can improve generalization. This is very important for cases where only very few data are accessible, when controlled experiments are either impossible or too expensive.</p> <p>I‚Äôm very excited to apply structure regression to scientific problems! BIMT might be a reasonable starting point, but very likely not the ultimate answer, so there will be a lot of fun and fruitful things to do along the journey. If you have any applications in your fields, please don‚Äôt hesitate to email me! If you ask me which fields structure regression is most promising for, my prior would be a uniform distribution over all scientific domains! Said that, if you care about interpretability or scientific insights, maybe more well-defined fields (e.g., mathematics, physics) are better. If you want to do something more practical or useful, fields with some extent of messiness (e.g., chemistry, biology, social science, engineering) might be better. To be concrete, I think structure regression is quite promising for the following fields:</p> <ul> <li>Fluid mechanics</li> <li>Biophysics</li> <li>Astro &amp; Plasma physics</li> <li>Quantum chemistry (DFT)</li> <li>Molecular dynamics</li> <li>Atmospheric science</li> <li>Biology (Protein folding)</li> <li>Economy</li> <li>‚Ä¶</li> </ul> <p>The list can go on and on. Again, shoot me an email if you are working on a scientific problem and are interested in applying structure regression to it! I‚Äôm open to any form of collaboration. üôÇ</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/physics-ml-theory/">A Good ML Theory is Like Physics -- A Physicist's Analysis of Grokking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/structuralism-ai/">Achieving AGI Intelligently ‚Äì Structure, Not Scale</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/ai-physics-interpretability/">What's the difference -- (physics of) AI, physics, math and interpretability</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/unigram-toy-1/">Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/kolmogorov-arnold-networks/">Philosophical thoughts on Kolmogorov-Arnold Networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Ziming Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" crossorigin="anonymous"></script> <script>window.addEventListener("load",function(){console.log("Page loaded. MathJax available:",void 0!==window.MathJax),void 0!==window.MathJax?console.log("MathJax version:",window.MathJax.version):console.error("MathJax failed to load!")});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-media-coverage",title:"media coverage",description:"",section:"Navigation",handler:()=>{window.location.href="/media/"}},{id:"nav-talk",title:"talk",description:"",section:"Navigation",handler:()=>{window.location.href="/talk/"}},{id:"post-physics-1-attention-can-39-t-exactly-simulate-uniform-linear-motion",title:"Physics 1 -- Attention can&#39;t exactly simulate uniform linear motion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-1/"}},{id:"post-depth-4-flat-directions-in-weight-space-are-high-frequency-modes-in-function-space",title:"Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-4/"}},{id:"post-depth-3-fun-facts-about-loss-hessian-eigenvalues",title:"Depth 3 -- Fun facts about loss hessian eigenvalues",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-3/"}},{id:"post-diffusion-2-visualizing-flow-matching-temporal-dynamics",title:"Diffusion 2 -- Visualizing flow matching, temporal dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-2/"}},{id:"post-sparse-attention-7-stack-of-causal-attention-creates-implicit-positional-embedding-and-explaning-quot-loss-in-the-middle-quot",title:"Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning &quot;Loss in the middle&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-7/"}},{id:"post-sparse-attention-6-in-context-associative-recall",title:"Sparse attention 6 -- In-context Associative recall",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-6/"}},{id:"post-mlp-2-effective-linearity-generalized-silu",title:"MLP 2 -- Effective linearity, Generalized SiLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-2/"}},{id:"post-mlp-1-gating-is-good-for-polynomials",title:"MLP 1 -- Gating is good for polynomials",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/mlp-1/"}},{id:"post-optimization-4-loss-spikes",title:"Optimization 4 -- Loss Spikes",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-4/"}},{id:"post-optimization-3-depth-2-adding-bias-after-relu",title:"Optimization 3 / Depth 2 -- Adding Bias After ReLU",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-3/"}},{id:"post-optimization-2-elementwise-scale-reparametrization",title:"Optimization 2 -- Elementwise Scale Reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-2/"}},{id:"post-optimization-1-norm-reparametrization",title:"Optimization 1 -- Norm reparametrization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/optimization-1/"}},{id:"post-sparse-attention-5-attention-sink",title:"Sparse attention 5 -- Attention sink",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-5/"}},{id:"post-bigram-4-on-the-difficulty-of-spatial-map-emergence",title:"Bigram 4 -- On the difficulty of spatial map emergence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-4/"}},{id:"post-depth-1-understanding-pre-ln-and-post-ln",title:"Depth 1 -- Understanding Pre-LN and Post-LN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/depth-1/"}},{id:"post-bigram-3-low-rank-structure",title:"Bigram 3 -- Low Rank Structure",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-3/"}},{id:"post-bigram-2-emergence-of-hyperbolic-spaces",title:"Bigram 2 -- Emergence of Hyperbolic Spaces",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-2/"}},{id:"post-bigram-1-walk-on-a-circle",title:"Bigram 1 -- Walk on a Circle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/bigram-1/"}},{id:"post-diffusion-1-sparse-and-dense-neurons",title:"Diffusion 1 -- Sparse and Dense Neurons",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/diffusion-1/"}},{id:"post-sparse-attention-4-previous-token-head",title:"Sparse attention 4 -- previous token head",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-4/"}},{id:"post-sparse-attention-3-inefficiency-of-extracting-similar-content",title:"Sparse attention 3 -- inefficiency of extracting similar content",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-3/"}},{id:"post-emergence-of-induction-head-depends-on-learning-rate-schedule",title:"Emergence of Induction Head Depends on Learning Rate Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/induction-head-lr-schedule/"}},{id:"post-sparse-attention-2-unattention-head-branching-dynamics",title:"Sparse attention 2 -- Unattention head, branching dynamics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-2/"}},{id:"post-sparse-attention-1-sticky-plateau-and-rank-collapse",title:"Sparse attention 1 -- sticky plateau and rank collapse",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/sparse-attention-1/"}},{id:"post-unigram-toy-model-is-surprisingly-rich-representation-collapse-scaling-laws-learning-rate-schedule",title:"Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/unigram-toy-1/"}},{id:"post-fine-tuning-with-sparse-updates-a-toy-teacher-student-setup",title:"Fine-tuning with sparse updates? A toy teacher-student Setup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/fine-tuning-sparsity/"}},{id:"post-multi-head-cross-entropy-loss",title:"Multi-Head Cross Entropy Loss",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/multi-head-cross-entropy/"}},{id:"post-what-39-s-the-difference-physics-of-ai-physics-math-and-interpretability",title:"What&#39;s the difference -- (physics of) AI, physics, math and interpretability",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/ai-physics-interpretability/"}},{id:"post-representation-anisotropy-from-nonlinear-functions",title:"Representation anisotropy from nonlinear functions",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/activation-anisotropy/"}},{id:"post-training-dynamics-of-a-single-relu-neuron",title:"Training dynamics of A Single ReLU Neuron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/single-relu-neuron/"}},{id:"post-physics-of-ai-how-to-begin",title:"Physics of AI \u2013 How to Begin",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/physics-of-ai-recipe/"}},{id:"post-physics-of-feature-learning-1-a-perspective-from-nonlinearity",title:"Physics of Feature Learning 1 \u2013 A Perspective from Nonlinearity",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/feature-learning-1/"}},{id:"post-physics-of-ai-requires-mindset-shifts",title:"Physics of AI Requires Mindset Shifts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/physics-of-ai/"}},{id:"post-achieving-agi-intelligently-structure-not-scale",title:"Achieving AGI Intelligently \u2013 Structure, Not Scale",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/structuralism-ai/"}},{id:"post-philosophical-thoughts-on-kolmogorov-arnold-networks",title:"Philosophical thoughts on Kolmogorov-Arnold Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/kolmogorov-arnold-networks/"}},{id:"post-symbolic-regreesion-structure-regression",title:"Symbolic Regreesion? Structure Regression!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/structure-regression/"}},{id:"post-a-good-ml-theory-is-like-physics-a-physicist-39-s-analysis-of-grokking",title:"A Good ML Theory is Like Physics -- A Physicist&#39;s Analysis of Grokking",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/physics-ml-theory/"}},{id:"project-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"project-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"project-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"project-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"project-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"project-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/talk_1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%6D%6C%69%75@%6D%69%74.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=QeXHxlIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kindxiaoming","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/zimingliu11","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>