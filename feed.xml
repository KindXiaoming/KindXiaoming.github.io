<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-03T18:40:49+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Training dynamics of A Single ReLU Neuron</title><link href="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/" rel="alternate" type="text/html" title="Training dynamics of A Single ReLU Neuron"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/single-relu-neuron</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a previous <a href="/blog/2026/feature-learning-1/">blog post</a>, we studied feature learning in shallow, wide MLPs. In this article, we consider an even simpler setting: an MLP with only <strong>one hidden layer</strong>, <strong>a single ReLU neuron</strong>, and a <strong>self-generated target function</strong> (a teacher network).</p> <p>In this setting, we know that there exists a set of weights for which the loss is exactly zero. But will gradient-based optimization run into difficulties? For example, if the neuron is initialized to be overly <strong>active</strong> (positive pre-activation for all inputs), or overly <strong>inactive</strong> (negative pre-activation for all inputs), will optimization fail? Even if the initialization is reasonable, can the training dynamics drive the neuron into a bad state (too active or too inactive)?</p> <p>For convenience, we define three states of a neuron:</p> <ul> <li><strong>Hyperactive</strong>: activated for all inputs (pre-activation always positive)</li> <li><strong>Inactive / Dead</strong>: never activated (pre-activation always negative)</li> <li><strong>Balanced</strong>: activated for some inputs and inactive for others</li> </ul> <p>Although this setup is extremely simple, our ultimate goal is to gain insights relevant to LLM training. One intuition is that many tricks used in LLM training—such as LR warmup, LR decay, weight decay, and MoE balancing—may implicitly control neuron activity levels, thereby influencing feature learning.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>An MLP with one hidden layer and one neuron can be written as (for simplicity, the input is also 1D):</p> \[f(x; w_1, b_1, w_2, b_2) = w_2\sigma(w_1x+b_1)+b_2, \quad \sigma(x) ={\rm ReLU}(x) \equiv {\rm max}(0,x)\] <p>For the teacher network, we set \(w_1^T = w_2^T = 1, \quad b_1^T = b_2^T = 0\), and so \(f(x)\equiv {\rm ReLU}(x)\). We take the input domain to be \(x \in [-1,1].\)</p> <p>For the student network, we initialize \(w_1^S = w_2^S = 1,\) and focus on varying the initializations of \(b_1^S\) and \(b_2^S\). Training uses MSE loss and the Adam optimizer (default LR = 0.01). Below, we only discuss the student’s weights, so we omit the superscript \(S\).</p> <hr/> <h2 id="observation-0-large-b_1-leads-to-local-minima">Observation 0: Large \(|b_1|\) Leads to Local Minima</h2> <p>We fix the initialization \(b_2 = 0\).</p> <ul> <li>When \(b_1 &gt; 1\), the neuron is initialized in the <strong>hyperactive</strong> state, and the loss gets stuck around 0.02 (corresponding to approximating ReLU with linear regression).</li> <li>When \(b_1 &lt; -1\), the neuron is initialized in the <strong>dead</strong> state, and the loss gets stuck around 0.1 (corresponding to approximating ReLU with a constant function).</li> </ul> <p>This may help explain why some initialization schemes (e.g., Kaiming initialization) set the bias to zero.</p> <hr/> <h2 id="observation-1-large-b_2-can-kill-the-neuron">Observation 1: Large \(|b_2|\) Can Kill the Neuron</h2> <p>We fix \(b_1 = 0\), so the neuron is <strong>balanced</strong> at initialization. Is everything fine then? Not quite. We find that when \(b_2 = -1.1\), the loss can go to zero, but when \(b_2 = -1.2\), the loss gets stuck around 0.02 (again corresponding to approximating ReLU with linear regression).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/loss-480.webp 480w,/assets/img/blogs/single-relu-neuron/loss-800.webp 800w,/assets/img/blogs/single-relu-neuron/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By closely examining the training dynamics, we find that the turning point \(x_t \equiv - b_1 / w_1\) moves left during training (starting from 0). When it moves past -1, the neuron transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>The local minimum for \(b_2 = -1.2\) corresponds to the linear regression solution (the neuron is in the hyperactive state):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/large_b2_init-480.webp 480w,/assets/img/blogs/single-relu-neuron/large_b2_init-800.webp 800w,/assets/img/blogs/single-relu-neuron/large_b2_init-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/large_b2_init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Conversely, if we increase the bias of the target function, the neuron also transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>This observation connects to two empirical practices:</p> <ul> <li><strong>Model bias</strong>: In some LLM training setups, bias terms are disabled. The observation above suggests that bias dynamics can drive neurons away from the balanced state.</li> <li><strong>Data bias</strong>: Whitening inputs and normalizing intermediate representations are common practices.</li> </ul> <hr/> <h2 id="observation-2-too-large-a-learning-rate-can-also-kill-the-neuron">Observation 2: Too Large a Learning Rate Can Also Kill the Neuron</h2> <p>We fix \(b_1 = 0\) and \(b_2 = -1\).</p> <ul> <li>When LR = 0.2, the model’s loss can be optimized to zero.</li> <li>When LR = 0.4, the loss only reaches 0.1 (again corresponding to fitting ReLU with a constant function), because the neuron becomes <strong>dead</strong>.</li> </ul> <p>This may be related to LR warmup in LLM training: if the initial learning rate is too large, the loss may decrease quickly, but at the cost of killing some neurons. Once dead, these neurons are hard (or impossible) to bring back to a balanced state.</p> <hr/> <h2 id="observation-3-silu-eventually-recovers-but-gets-stuck-at-a-saddle-point-for-a-long-time">Observation 3: SiLU Eventually Recovers, but Gets Stuck at a Saddle Point for a Long Time</h2> <p>For SiLU, we observe that the loss can eventually reach zero (up to machine precision), but training gets stuck at saddle points for very long time, reducing training efficiency.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/silu-480.webp 480w,/assets/img/blogs/single-relu-neuron/silu-800.webp 800w,/assets/img/blogs/single-relu-neuron/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="takeaway">Takeaway</h2> <p><strong>Irreversibility</strong>: once a neuron leaves the balanced state, it is very hard to return. Bias plays a crucial role. This might be why we have various ugly tricks for LLM (learning rate schedule, weight decay, etc).</p> <hr/> <h2 id="questionsideas">Questions/Ideas</h2> <p><strong>Question 1: How can we better control bias?</strong></p> <ul> <li>Can we analytically compute the bias instead of learning it via gradient descent?</li> <li>Can we use a different parameterization to better control neuron activation? For example, \(wx + b \;\to\; w(x + b'),\) where \(b'\) directly controls neuron activation (assuming the input distribution is known). This may allow us to more directly control neuron activity, e.g., by applying weight decay to \(b'\) or explicitly enforcing balancing.</li> </ul> <p><strong>Question 2: Which neurons have better learning dynamics?</strong></p> <ul> <li>SiLU is better than ReLU. Is there something better, gating?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1gSrKOVfEtVNTa7ZCUOpqOI1uTXm0SL9s?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026single-relu-neuron</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Training dynamics of A Single ReLU Neuron}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/single-relu-neuron/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics of AI – How to Begin</title><link href="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/" rel="alternate" type="text/html" title="Physics of AI – How to Begin"/><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/"><![CDATA[<div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-480.webp 480w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-800.webp 800w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In a recent <a href="/blog/2026/physics-of-ai/">blog post</a>, we mentioned that <em>Physics of AI</em> requires a shift in mindset.<br/> This post will explain, from a slightly more technical perspective, <strong>how</strong> to do <em>Physics of AI</em>—I will present a <strong>modular framework</strong>.</p> <p>On the one hand, we need a framework. With a basic framework, the community can share a common language for communication. On the other hand, we must not be constrained by the framework itself. <em>Physics of AI</em> genuinely requires novel, out-of-the-box ideas.</p> <hr/> <h2 id="what-is-physics-of-ai">What Is Physics of AI?</h2> <p><em>Physics of AI</em>, as the name suggests, means studying AI in the same way we study physics.<br/> It aims to answer the following questions (so simple that they may sound trivial):</p> <blockquote> <p><strong>What models, on what data, exhibit what phenomena?</strong><br/> <em>(Bonus: why?)</em></p> </blockquote> <p>There are three core elements here: <strong>models, data, and phenomena</strong>.<br/> To emphasize data and phenomena (which are often overlooked by the community), I deliberately fold many details—such as optimizers and loss functions—into the notion of the <em>model</em>.</p> <p>Below, I elaborate on these three elements.</p> <hr/> <h2 id="models">Models</h2> <ul> <li> <p><strong>Architecture</strong><br/> Examples include MLPs, RNNs, Transformers, DINO, Mamba, KANs, etc.<br/> Architectures consist of various layers such as MLP layers, attention layers, convolutions, etc.</p> </li> <li> <p><strong>Training</strong></p> <ul> <li><strong>Paradigms</strong><br/> Supervised learning, unsupervised learning, self-supervised learning, representation learning, etc. In the LLM era, we further distinguish <em>pre-training</em>, <em>mid-training</em>, and <em>post-training</em>.</li> <li><strong>Optimizers</strong><br/> SGD, Adam, Muon, etc.</li> <li><strong>Loss functions / Rewards</strong><br/> MSE, cross-entropy, accuracy, diffusion loss, etc.<br/> These are often coupled with the training paradigm—for example, contrastive loss in representation learning.</li> <li><strong>Regularization</strong><br/> L2 weight decay, dropout, KL divergence, and so on.</li> </ul> </li> </ul> <hr/> <h2 id="data">Data</h2> <ul> <li><strong>Synthetic (toy) data</strong><br/> We know the data generation process, and have full control over it.</li> <li><strong>Real data</strong><br/> We do not fully know the data generation process.</li> </ul> <hr/> <h2 id="phenomena--observables">Phenomena / Observables</h2> <p>Beyond commonly tracked quantities such as loss curves and accuracy, we also care about:</p> <ul> <li> <p><strong>Biology-like phenomena (often data-dependent)</strong><br/> For example, understanding whether (and how) representations, computations, algorithms, or structures emerge during training.<br/> A well-known example is the <em>induction head</em>.<br/> The field of <em>mechanistic interpretability</em> is closely related here.</p> </li> <li> <p><strong>Physics-like phenomena (often data-independent)</strong><br/> For example, weight matrix spectra, activation subspaces, attention patterns, and so on.</p> </li> </ul> <hr/> <h2 id="what-should-we-measure">What Should We Measure?</h2> <p>A central and difficult question in AI phenomenology is: <strong>what exactly should we measure?</strong><br/> Of course, this depends on the phenomena we want to observe. But in order to observe phenomena, we must first know <em>which quantities to measure</em>—that is, <em>which observables correspond to which phenomena</em>. This creates a classic <strong>chicken-and-egg problem</strong>.</p> <p>We must start somewhere. Based on my experience, common starting points include:</p> <ul> <li> <p><strong>Making abstract ideas concrete</strong>, which leads to observables.<br/> For example, I may care about <em>feature learning</em>, but features are high-dimensional. I therefore define observables that capture certain aspects of feature learning. In this <a href="/blog/2026/feature-learning-1/">blog</a>, I use the <em>number of non-linear neurons</em> as one such observable.</p> </li> <li> <p><strong>Starting from a known phenomenon and discovering new ones.</strong><br/> For instance, some people study <em>grokking</em> and, during reproduction, discover new phenomena such as <em>loss spikes</em>, which then become a subject of study themselves. This leads to <a href="https://arxiv.org/abs/2206.04817">Apple’s slingshot paper</a>.</p> </li> <li> <p><strong>Starting from real data</strong>, and logging common observables during training.<br/> What counts as “common” requires accumulation of experience:<br/> (i) seeing what others in the field measure, and<br/> (ii) learning which observables were useful in your own past experiments.</p> </li> <li> <p><strong>Starting from toy data</strong>, imagining how the model <em>should</em> behave (prompting yourself: <em>“If you were an AI model, what would you do?”</em>), and then designing observables to test whether the model actually behaves that way.</p> </li> </ul> <p>In this way, the original chicken-and-egg problem becomes a <strong>spiral of ascent</strong>:<br/> more phenomena inspire new observables, and new observables lead to the discovery of more phenomena.</p> <hr/> <h2 id="why-does-this-phenomenon-occur">Why Does This Phenomenon Occur?</h2> <p>My personal thinking habit is to first ask:<br/> <strong>Is this phenomenon caused by the data, or by the model?</strong><br/> Here, “model” also includes random seeds: <em>how sensitive is the phenomenon to the random seed?</em></p> <p>The fastest way to get answers is through experiments—changing seeds, changing models, changing data, and checking whether the phenomenon persists. Once these basic experimental results are in hand, we can begin to form <strong>hypotheses</strong>. Hypotheses predict new observables and phenomena, which are then tested by further experiments.</p> <p>This forms a discovery loop:</p> <blockquote> <p><strong>Run experiments → observe phenomena → form hypotheses → design the next experiments</strong></p> </blockquote> <hr/> <h2 id="what-does-it-mean-to-understand-something">What Does It Mean to “Understand” Something?</h2> <p>There are many layers to asking “why” or claiming understanding. How much understanding is <em>enough</em>?</p> <p>My personal criterion is: <strong>when my predictions are broadly consistent with new experimental results, I consider that a success</strong>.<br/> The meaning of “broadly” is subtle. When I am lazy or tolerant, matching trends may be enough. When I am diligent or obsessive, I may want quantitative agreement as well.</p> <p>For example, the discovery of the \(t\sim \gamma^{-1}\) (\(t\): grokking time, \(\gamma\):weight decay) in <a href="https://arxiv.org/abs/2210.01117">our Omnigrok paper</a> came from pushing this obsession further. How far one pushes before stopping largely depends on the researcher’s personality, beliefs, and scientific taste. If we post our findings publicly, the whole community can push the frontier together (we’re all “blind men touching elephants”; different perspectives are always helpful).</p> <hr/> <h2 id="what-makes-a-model-good">What Makes a Model “Good”?</h2> <p>A common debate in the field is: <em>Which model is better?</em><br/> Is this judgment objective or subjective?</p> <p>The <strong>No Free Lunch theorem</strong> already gives the answer: no model is universally better than another across all data.<br/> The real philosophical question is therefore:</p> <blockquote> <p><strong>What kind of data is the world closest to?</strong></p> </blockquote> <p><em>Physics of AI</em> can objectively answer:<br/> <strong>what data, under what models, exhibits what phenomena</strong>—this part is fully objective and uncontested.</p> <p>Once we have a notion of <strong>which phenomena are desirable</strong> (possibly subjective, possibly objective), we can answer:<br/> <strong>what data requires what models</strong>.</p> <p>Going one step further, once we have a notion of <strong>what kind of data the world is closest to</strong> (again, possibly subjective or objective), we can answer:<br/> <strong>which models are good</strong>.</p> <p>The current state of AI development often skips the purely objective stage and jumps directly to subjective claims. As a result, discussions often devolve into “you say yours, I say mine,” lacking the <strong>shared knowledge</strong> required for meaningful communication.</p> <p>The goal of <em>Physics of AI</em> is precisely to construct this shared knowledge.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of AI – How to Begin}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-of-ai-recipe/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Physics of Feature Learning 1 – A Perspective from Nonlinearity</title><link href="https://kindxiaoming.github.io/blog/2026/feature-learning-1/" rel="alternate" type="text/html" title="Physics of Feature Learning 1 – A Perspective from Nonlinearity"/><published>2026-01-01T00:00:00+00:00</published><updated>2026-01-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/feature-learning-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/feature-learning-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Please Read our latest post <a href="/blog/2026/physics-of-ai/">Physics of AI Requires Mindset Shifts</a> for general philosophy.</p> <p>While reading <a href="https://arxiv.org/abs/2509.21519">Yuandong Tian’s work on explaining grokking through feature learning</a>, I found the proposed <em>three-stage dynamics of feature learning</em> particularly intriguing. However, grokking itself has some special characteristics (e.g., the modular addition dataset), which led me to wonder whether feature learning could be studied in a more <em>standard</em> setting. This motivated the low-dimensional regression example explored below.</p> <p>Rather than starting from mathematical derivations, I decided to go straight to experiments. Beyond plotting the loss, we also wanted to define observables that characterize <em>features</em>. One of the most basic observables is the <strong>number of nonlinear neurons</strong>. This measure does not care about the specific form of the features, only whether they are nonlinear.</p> <p>To define neuron activation cleanly, we use <strong>ReLU</strong> activations. A neuron is considered <em>nonlinear</em> if it is activated for some inputs and inactive for others.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>We consider the following regression function:</p> \[y = \sum_{i=1}^{d} \sin^2(f x_i), \quad d = 10,\; f = 10.\] <p>We use an MLP with 10-dimensional input, 100 neurons in the first hidden layer, 100 neurons in the second hidden layer, and a single output. We denote the architecture as ([10, 100, 100, 1]). Inputs are sampled from a standard Gaussian distribution, with a total of 500 samples. The training objective is MSE loss, optimized using <strong>Adam</strong> (learning rate \(10^{-3}\), full batch).</p> <hr/> <h2 id="observation-four-phases-during-training">Observation: Four Phases During Training</h2> <p>We find that all 100 neurons in the first hidden layer remain nonlinear throughout training. In contrast, the number of nonlinear neurons in the second hidden layer exhibits <strong>non-trivial dynamics</strong>. Therefore, all plots below focus on the evolution of nonlinear neurons in the second hidden layer.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss-480.webp 480w,/assets/img/blogs/feature-learning-1/loss-800.webp 800w,/assets/img/blogs/feature-learning-1/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Empirically, the training dynamics appear to consist of four phases:</p> <ul> <li> <p><strong>Phase I (first ~50 steps)</strong><br/> The loss decreases rapidly, while the number of active neurons drops sharply.<br/> <em>Hypothesis</em>: the model is removing irrelevant features.</p> </li> <li> <p><strong>Phase II (50–500 steps)</strong><br/> The loss plateaus around 1, and the number of active neurons remains roughly constant (around 14).<br/> <em>Hypothesis</em>: the model is stuck near a saddle point, leading to slow learning.</p> </li> <li> <p><strong>Phase III (500–1500 steps)</strong><br/> The loss decreases slowly, while the number of active neurons increases.<br/> <em>Hypothesis</em>: this corresponds to genuine feature learning.</p> </li> <li> <p><strong>Phase IV (after ~1500 steps)</strong><br/> The loss decreases rapidly (exponential convergence), while the number of active neurons stays constant.<br/> <em>Hypothesis</em>: the model is fine-tuning features, or fine-tuning the final linear layer.</p> </li> </ul> <p>There are many interesting questions one could study here.</p> <hr/> <h2 id="curiosity-driven-questions">Curiosity-Driven Questions</h2> <ul> <li> <p>Why does Phase I remove so many features?<br/> <em>Speculation</em>: is this related to optimization tricks in LLMs? For example, do weight decay and learning-rate warmup mainly serve to prevent excessive feature removal in this phase?</p> </li> <li> <p>Phase II appears “sticky.” How should we better understand the meaning of this solution?<br/> <em>Speculation</em>: is the loss plateau simply the result of linear regression?</p> </li> <li> <p>How do nonlinear features emerge in Phase III?</p> </li> <li> <p>What determines the exponential convergence rate in Phase IV?<br/> <em>Speculation</em>: once features are learned, does the problem effectively reduce to linear regression, which we know converges exponentially?</p> </li> </ul> <hr/> <h2 id="a-linear-regression-perspective">A Linear Regression Perspective</h2> <p>To better understand the dynamics, we analyze the model from a linear regression viewpoint. We compute three linear regression baselines:</p> \[X \to Y,\quad f_1 \to Y,\quad f_2 \to Y,\] <p>where \(f_1\) and \(f_2\) denote the features from the first and second hidden layers, respectively, which evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss_linear_regression-480.webp 480w,/assets/img/blogs/feature-learning-1/loss_linear_regression-800.webp 800w,/assets/img/blogs/feature-learning-1/loss_linear_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss_linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The loss plateau around 1 corresponds closely to the solution of linear regression.</li> <li>In the late stage of training, the blue and orange curves nearly coincide, indicating that the final-layer weights are essentially optimal at every moment, while the features are still being fine-tuned. As a result, the loss continues to decrease.</li> </ul> <p>This is not standard linear regression (where features are fixed and weights evolve), but rather a <strong>dual version</strong>: features evolve while weights remain near their instantaneous optimum. Interestingly, this still leads to exponential decay. A natural next step is to understand the convergence rate quantitatively.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/17gyczGrRuUAuWYuD_NFB7ZlCAA37p1cK?usp=sharing">here</a>.</p> <p>Observing interesting phenomena and asking good questions is already half the battle. We are still working toward explaining more of these effects. If you have any idea regarding how to understand some of the phenomena, please email me at lzmsldmjxm@gmail.com</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026feature-learning-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of Feature Learning 1 -- A Perspective from Nonlinearity}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/feature-learning-1/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu (January 2026). Physics of Feature Learning 1 – A Perspective from Nonlinearity. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2026/feature-learning-1/</p>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）]]></summary></entry><entry><title type="html">Physics of AI Requires Mindset Shifts</title><link href="https://kindxiaoming.github.io/blog/2025/physics-of-ai/" rel="alternate" type="text/html" title="Physics of AI Requires Mindset Shifts"/><published>2025-12-31T00:00:00+00:00</published><updated>2025-12-31T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2025/physics-of-ai</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2025/physics-of-ai/"><![CDATA[<p><strong>The “physics of AI” is still far from arriving.</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-of-ai-480.webp 480w,/assets/img/blogs/physics-of-ai-800.webp 800w,/assets/img/blogs/physics-of-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-of-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we use the Tycho–Kepler–Newton analogy, today’s AI development largely remains at the <strong>Tycho stage</strong>—experimentation and observation. Yet even at the level of observation, what we currently have is extremely primitive: most people focus on tuning for a handful of performance-based metrics. This stems from a fundamental difference in goals between physics and AI. Physics aims to <em>transform the world by understanding it</em>, where <em>understanding</em> itself occupies a central position. As a result, the field is very tolerant of work that provides insight even if it is (temporarily) useless. In contrast, AI aims directly to <em>transform the world</em>. The scaling laws of recent years have allowed the field to skip the “understanding” step and jump straight into transforming AI itself. In my view, this constitutes a form of <strong>cognitive debt</strong>—one that will inevitably have to be repaid, sooner or later, if not already.</p> <p>For this reason, it is premature to talk about AI’s “Newtonian mechanics.” Even at the level of basic phenomenology, we are still at a very early stage. Phenomenology can be relatively macroscopic—connecting different models, such as <strong>emergence</strong> and <strong>scaling laws</strong>—or more microscopic—focused on training dynamics, such as <strong>grokking</strong>, <strong>double descent</strong>, or the <strong>edge of stability</strong>. We first need to discover more phenomena; only then will we be motivated to model them and develop theories to study them.</p> <hr/> <p><strong>Why Is AI Phenomenology Hard to Develop?</strong></p> <p>Why is AI phenomenology so difficult to develop? I believe publication culture plays a major role. What is publishable is either work with strong performance gains (in which case phenomenology seems unnecessary), or a compelling story.</p> <p>There are two kinds of “good stories”:</p> <ul> <li><strong>Universality</strong>: the phenomenon must be demonstrated across many settings. <em>Edge of stability</em> is an example. This imposes a very high bar for publications.</li> <li><strong>Surprise</strong>: the phenomenon is striking and unexpected. This is rare and highly unpredictable—<em>grokking</em> being a representative case.</li> </ul> <p>This explains why the list of commonly cited AI phenomenology examples is so short. Given the historical stage of the “physics of AI,” we seem to hold phenomenology to excessively high expectations, which in fact hinders its development.</p> <p><a href="https://physics.allen-zhu.com/">Allen Zeyuan Zhu’s <em>Physics of LLMs</em></a> is excellent work, but from my conversations with friends, the common reaction is that it is interesting yet hard to know where/how to begin if they want to dive into the field. The same applies to our own work <a href="https://openreview.net/forum?id=knPz7gtjPW"><em>Superposition Leads to Robust Neural Scaling</em> (NeurIPS 2025 Best Paper Runner-up)</a>: people are curious about how such a story was conceived. I cannot speak for other researchers in the physics-of-AI space, but from my own experience, I spend an inordinate amount of time packaging a story—”wasting” my own time and increasing the distance between myself and readers.</p> <p>Moreover, phenomena that can be packaged into a story are exceedingly rare. Many phenomena that I personally find fascinating, but cannot turn into a paper, end up casually discarded.</p> <hr/> <p><strong>Toward a More Accessible Phenomenology</strong></p> <p>Therefore, I advocate for a more <strong>accessible and inclusive form of phenomenological research</strong>. This approach would be more tolerant than current AI phenomenology and closer in spirit to phenomenology in physics. It would:</p> <ul> <li>not be oriented toward immediate usefulness;</li> <li>not require being packaged into a complete story;</li> <li>place no restrictions on analytical tools, as long as they are effective in description/prediction.</li> </ul> <p>At the same time, it would emphasize:</p> <ol> <li> <p><strong>Controllability</strong><br/> Use toy models to simplify and abstract real-world settings, such that results can be reproduced with minimal resources (ideally a single notebook plus a CPU is sufficient).</p> </li> <li> <p><strong>Multi-perspective characterization</strong><br/> Describe the object of study from as many angles and metrics as possible—like blind men feeling an elephant.</p> </li> <li> <p><strong>Curiosity / hypothesis-driven exploration</strong><br/> Phenomena should yield new insights—qualitative is sufficient, quantitative is even better.</p> </li> </ol> <p>This kind of accessible phenomenology may not be easy to publish at mainstream AI conferences, but it is extremely valuable for <strong>community building</strong>. Perhaps researcher A discovers a phenomenon (the key is making it public), B connects it to another phenomenon they previously observed, C unifies the two, D develops some theoretical analysis, and E turns the insights into algorithmic improvements. The five of them can then write a paper together.</p> <p>Traditionally, A might only collaborate within a small circle, but my understanding of the physics-of-AI community is that it is still highly fragmented, often divided by application domains. For example, vision researchers tend to collaborate with other vision researchers, and their intuitions are shaped primarily by vision tasks.</p> <hr/> <p><strong>So what can we do?</strong></p> <p><strong>On my end: Starting Blogposts</strong></p> <p>I will start sharing my (our) own “AI phenomenology” research in the form of blog posts. The right expectation for readers is that a colleague is sharing partial results: the work may be incomplete, but the raw data and thought process are presented transparently.</p> <p>The goals are threefold:</p> <ol> <li> <p><strong>To force myself to record observations</strong><br/> As mentioned earlier, phenomena that cannot be turned into papers are often thrown away. This effort is partially inspired by <a href="https://kexue.fm/">Jianlin Su’s blog</a>—his focuses on mathematical principles, whereas mine will emphasize experimental observations (phenomenology), “physical” intuition, and, when necessary, some (semi-)quantitative analysis, providing problems and intuition for future mathematical work.</p> </li> <li> <p><strong>To attract researchers and students who share similar interests</strong><br/> If you are interested in exploring these ideas together, feel free to reach out.</p> </li> <li> <p><strong>Course preparation</strong><br/> I plan to offer a <em>Physics of AI</em> course at Tsinghua University. These blog posts (along with accompanying code) may eventually become course materials.</p> </li> </ol> <p><strong>On your end: How to Get Started</strong></p> <ol> <li> <p><strong>Find questions you care about</strong><br/> For example, studying parameterizations of diffusion-model losses, or reproducing known phenomena such as grokking.</p> </li> <li> <p><strong>Define a simple toy model</strong><br/> For instance, <a href="https://arxiv.org/pdf/2511.13720">Tianhong Li and Kaiming He’s JIT paper</a> uses a 2D spiral dataset to study loss parameterization. The best way to understand grokking is simply to train a modular addition task yourself.</p> </li> <li> <p><strong>Commit to fully understanding the toy model</strong><br/> This is the hardest step. We are often too eager to move from toy models to realistic ones (again, due to publishing culture). Once a toy model produces the desired positive result, we move on. This is a <em>supervised</em> use of toy models. I believe toy models reveal their greatest power when used <em>unsupervised</em>. As the name suggests, it is a toy—approach it with childlike curiosity, play with it, and understand it from every possible angle (like blind men touching elephants).</p> </li> </ol> <p>I cannot guarantee that the insights gained will immediately translate into performance improvements, but I believe that if we, as a field, continue to accumulate such insights, a <strong>percolation-like phase transition</strong> will eventually occur.</p> <hr/> <p>The first phenomenology blogpost is <a href="/blog/2026/feature-learning-1/">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of AI Requires Mindset Shifts}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{December}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2025/physics-of-ai/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[The “physics of AI” is still far from arriving.]]></summary></entry><entry><title type="html">Achieving AGI Intelligently – Structure, Not Scale</title><link href="https://kindxiaoming.github.io/blog/2025/structuralism-ai/" rel="alternate" type="text/html" title="Achieving AGI Intelligently – Structure, Not Scale"/><published>2025-12-25T00:00:00+00:00</published><updated>2025-12-25T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2025/structuralism-ai</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2025/structuralism-ai/"><![CDATA[<p><strong>TL;DR</strong>: Structuralism AI is the inevitable path beyond scaling — <em>not because scaling is wrong, but because it will eventually hit the energy/data wall.</em></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/structuralist-ai-480.webp 480w,/assets/img/blogs/structuralist-ai-800.webp 800w,/assets/img/blogs/structuralist-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/structuralist-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p>Scaling laws have been the most important guiding principle in AI over the past few years. That much is undeniable. They have driven unprecedented performance gains and largely unified both industry and academia around a single direction. Yet the logic behind scaling laws is surprisingly simple: because AI struggles with Out-of-Distribution (OOD) tasks, the most straightforward solution is to collect more data and train larger models, until everything becomes In-distribution.</p> <p>Scaling laws therefore offer a future that is <strong>robust, but inefficient</strong>.</p> <p>Let me be very clear about my position. If we completely ignore constraints on energy and data, I do not doubt that <strong>pure scaling alone can eventually reach AGI</strong>. I have never doubted this. If compute were infinite and data unlimited, large models could, in principle, cover everything. The problem is precisely that the real world is not like this.</p> <p>So the question becomes:<br/> is there a <strong>more intelligent way</strong> to achieve AGI?</p> <p>I believe there is.<br/> And the answer is not more scale, but <strong>more structure</strong>.</p> <p>It is important that I deliberately use the word <em>structure</em>, not <em>symbol</em>. This distinction is intentional, and I will explain it later.</p> <hr/> <p>Why do we need structure? Because structure enables <strong>compression</strong>. And compression lies at the core of intelligence. As Ilya once put it: <em>Compression is intelligence.</em></p> <p>Consider a simple example. If fractal structure is allowed, the intrinsic complexity of a snowflake is extremely low—it is highly compressible. If structure is disallowed and one must describe it point by point, the apparent complexity of a snowflake is effectively infinite. Today’s scaling laws resemble the latter approach: using ever more parameters and computation to fit massive apparent complexity.</p> <p>A deeper example comes from celestial mechanics. The most direct way to model planetary motion is to store the positions of planets at every moment in time—a lookup table with enormous cost. Kepler achieved the first true compression by realizing that planetary orbits are ellipses, a global structure in time that dramatically reduced complexity. Newton achieved the second compression by discovering local dynamical laws that explained even more with fewer parameters.</p> <p>And where does modern AI stand? Work by Vafa and collaborators shows that Transformers do not naturally learn Newtonian world models. This means that <strong>correct physical structure does not reliably emerge from scale alone</strong>.</p> <p>Our current expectation that “structure will eventually emerge” often resembles primitive humans praying to gods. The only difference is that our sacrifices—data and compute—actually work (to some extent). And precisely because they work, we lack sufficient motivation to search for a more scientific, more intelligent path forward.</p> <hr/> <p>Structure is explicit and everywhere in the natural sciences. In fact, without structure, there would be no natural science at all.</p> <p>If we draw an analogy with the Tycho–Kepler–Newton trajectory, today’s AI still largely lives in the Tycho era: experiment-driven, data-driven, with only the beginnings of a Keplerian phase—empirical laws such as scaling laws. But unlike the history of celestial mechanics, we have turned these empirical laws into articles of faith, aggressively scaling experiments and engineering systems around them, rather than treating them as clues toward a deeper theory—a “Newtonian mechanics of AI.”</p> <p>From an intellectual perspective, this is not progress. It is regression.</p> <hr/> <p>At this point, you might say: “This is just another piece criticizing scaling and foundation models.”<br/> It is not.</p> <p>My stance is clear and neutral. According to the No Free Lunch theorem, every model has its domain of applicability and its limitations. Or, more bluntly: <em>All models are wrong, but some are useful.</em></p> <p>The real issue is not whether to use foundation models, but whether we understand that <strong>different tasks possess fundamentally different structures and compressibility</strong>. From a compression perspective, and by analogy with the natural sciences, tasks fall naturally into categories. Some are <strong>“physics-like”</strong>: highly compressible, with symbolic formulas emerging from continuous data. Some are <strong>“chemistry-like”</strong>: substantially compressible, with clear structure but incomplete or approximate symbols. Others are <strong>“biology-like”</strong>: only weakly compressible, dominated by empirical regularities and statistical induction. Pure noise exists as well, but no model can handle it, so we can safely ignore it.</p> <p>An ideal intelligent system should be able to recognize which kind of task it is facing and apply the appropriate degree of compression.</p> <p>Symbolic models excel at physics-like tasks but fail on chemistry- and biology-like ones. Connectionist models, due to their generality, can in principle handle all types—but precisely because they lack structure, they are extremely inefficient on physics- and chemistry-like problems.</p> <table class="table table-bordered"> <thead> <tr> <th style="border-right: 2px solid #dee2e6;"> <div style="display: flex; justify-content: space-between;"> </div> </th> <th>"Physics-like" tasks</th> <th>"Chemistry-like" tasks</th> <th>"Biology-like" tasks</th> </tr> </thead> <tbody> <tr> <td>**Symbolism AI**</td> <td>Excellent</td> <td>Poor</td> <td>Poor</td> </tr> <tr> <td>**Connectionism AI**</td> <td>Inefficient</td> <td>Inefficient</td> <td>Good</td> </tr> <tr> <td>**Structuralism AI**</td> <td>Good</td> <td>Good</td> <td>Good</td> </tr> </tbody> </table> <hr/> <p>This is why I argue for <strong>Structuralism</strong>.</p> <p>Symbolism starts from physics-like tasks. Connectionism starts from biology-like tasks. A natural question follows: can we build AI starting from chemistry-like tasks? Structuralism, by design, aims to capture this intermediate regime. We want symbols—stricter, more discrete structures—to emerge from structure, and we want empirical regularities—looser structures—to be learned by relaxing structure from data.</p> <p>In supervised learning, this distinction is already quite concrete. Linear regression is symbolic. Multi-layer perceptrons are connectionist. EQL represents a neural–symbolic hybrid. Kolmogorov–Arnold Networks (KANs), by contrast, are structuralist. The representation theory underlying KANs compactly captures the compositional structure of multivariate functions. As a result, KANs are neither structureless like MLPs, nor overconstrained like linear models, nor plagued by instability from neural–symbolic mismatch.</p> <p>Structuralism is not a compromise. It is a unification.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/four-ai-philosophy-480.webp 480w,/assets/img/blogs/four-ai-philosophy-800.webp 800w,/assets/img/blogs/four-ai-philosophy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/four-ai-philosophy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p>But the real world goes far beyond supervised learning. We do not merely learn structure from data—we compare structures, reuse structures, and build “structures of structures” (category theory!). This is <strong>abstraction</strong>.</p> <p>I want to argue explicitly: <strong>abstraction is one of the central bottlenecks to AGI</strong>. This aligns closely with Rich Sutton’s emphasis on abstraction in OaK architecture. Continual learning is fundamentally about preserving abstract invariances across tasks. Adaptivity and fluidity—for example in ARC-AGI—is about in-context abstraction. And many ARC-AGI tasks are, at their core, simplified forms of intuitive physics, an essential element of world models.</p> <p>So how do we enable abstraction? I will be honest: I do not yet have a complete solution.</p> <p>One insight is that abstraction arises from comparing and reusing structures. Attention is also a mechanism for comparison, but it implicitly assumes that structure can be embedded into vector spaces and that similarity can be measured by dot products. In reality, most structures are not isomorphic to vector spaces. We do this largely because it fits GPU computation, not because it is cognitively or scientifically correct.</p> <hr/> <p>I believe current AI development is <strong>secretly structuralist</strong>, but mostly in an extrinsic sense. Reasoning is structured. Agent frameworks are structured. Yet the underlying models remain connectionist. This is extrinsic structuralism, and it relies heavily on Chain-of-Thought-like data to explicitly supervise structure.</p> <p>I am willing to bet that the next wave of progress will come from <strong>intrinsic structuralism</strong>: injecting general-purpose structure into models, or enabling structure to emerge internally, without relying on explicit CoT supervision.</p> <p>From an application perspective, the AGI we actually need must be efficient, adaptive, generalizable, and physically grounded. Structure is essential to all four. The physical world itself is highly structured and highly compressible—compositionality, sparsity, temporal locality. Without these structures appearing in our models, world models will remain out of reach.</p> <hr/> <p><strong>In summary, Structuralism AI represents a path fundamentally different from scaling. It may be harder, but it is also more interesting, richer in opportunity, and far more promising in the long run.</strong></p> <p><strong>In 2026, it’s time to bet/work on something different.</strong></p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025structuralism-ai</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Achieving AGI Intelligently -- Structure, Not Scale}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{December}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2025/structuralism-ai/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming (December 2025). Achieving AGI Intelligently – Structure, Not Scale. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2025/structuralism-ai/</p>]]></content><author><name></name></author><category term="AI"/><category term="Structuralism-AI"/><category term="AGI"/><summary type="html"><![CDATA[TL;DR: Structuralism AI is the inevitable path beyond scaling — not because scaling is wrong, but because it will eventually hit the energy/data wall.]]></summary></entry><entry><title type="html">Philosophical thoughts on Kolmogorov-Arnold Networks</title><link href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/" rel="alternate" type="text/html" title="Philosophical thoughts on Kolmogorov-Arnold Networks"/><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/"><![CDATA[<p>Recently, collaborators and I proposed a new type of neural networks called the Kolmogorov-Arnold Networks (KANs), which are somewhat similar to but mostly different from Multi-Layer Perceptrons (MLPs).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/kan-480.webp 480w,/assets/img/blogs/kan-800.webp 800w,/assets/img/blogs/kan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/kan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The technical differences between MLPs and KANs can be found in our paper and many discussions over the internet. This blogpost does not delve into technicalities, but want to lay out quick philosophical thoughts, open to discussion. I will attempt to answer the follwoing questions:</p> <ul> <li>Q1: Are KANs and MLPs the same?</li> <li>Q2: What’s the philosophical difference between KANs and MLPs?</li> <li>Q3: Which is more aligned with science, KANs or MLPs?</li> </ul> <h2 id="q1-are-kans-and-mlps-the-same">Q1: Are KANs and MLPs the same?</h2> <p>The argument that “KANs and MLPs are the same because they have the same expressive power” is similar to the argument “A human being and a cup are the same because they are both made up of atoms.” It is well accepted in physics that each (energy) level has a physical theory, so even if two systems share the same theory in the most microscopic level, they are not necessarily the same on higher levels because of emeregent phenomenon. Back to the MLPs vs KANs case, even though MLPs and KANs have the same expressive power (which is a foundational aspect), other aspects emerging from it might be quite different. These emerging properties include optimization, generalization, interpretability, etc.</p> <h2 id="q2-whats-the-philosophical-difference-between-kans-and-mlps">Q2: What’s the philosophical difference between KANs and MLPs?</h2> <p><strong>Reductionism vs. Holism</strong> While MLPs are more aligned with holism, KANs are more aligned with reductionism. The design principle of MLPs is “more is different”. In an MLP, each neuron is simple because it has fixed activation functions. However, what matters is the complicated connection patterns among neurons. The magical power of MLPs performing a task is an emergent behavior which is attributed to collective contribution from all neurons. By contrast, in a KAN, each activation function is complicated because it has learnable functions. By sparsification and pruning, we hope the computation graph to be simple. In summary, MLPs have simple ‘atoms’ but complicated ways to combine these atoms; KANs have complicated (diverse) ‘atoms’ but simple ways to combine these atoms. In this sense, MLPs’ expressive power comes from the complicated connection patterns (fully-connected structure), which give rise to emergent bahavior (holism). In contrast, KANs’ expressive power comes from the complexity of fundamental units (learnable activation functions), but the way to decompose whole network to units is simple (reductionsim).</p> <h2 id="q3-which-is-more-aligned-with-science-kans-or-mlps">Q3: Which is more aligned with science, KANs or MLPs?</h2> <p>We ask a crazy question: if our science is written by a neural network, is it more likely to be generated with KANs or with MLPs? I tend to say KANs are more aligned with our science. Note that the way we write science, is mostly based on reductionism. Throughout the history of science, reducntionism has been the standard way of thinking. It was not until very recently did scientists start to realize the importance of holism (“more is different”), however, the study of complex systems is extremely hard and many tools are still based on reductionism. Because our science is mostly reductionism, and KANs are more aligned with redunctionsim than MLPs, KANs are more promising tools to describe science. If you think the above discussion is too abstract, let us just consider a concrete task of compiling a symbolic formula into neural networks. Given the formula of Hooke’s law \(F=kx\), it is unclear how to compile this formula into MLPs with (say) ReLU activations, but it is quite straight-forward to compile it into KANs by leveraging flexiable activation functions. For example \(kx=((k+x)^2-k^2-x^2)/2\) can be represeted as a \([2,2,1]\) KAN; when both \(k, x\) are postive, we even just need a \([2,1,1]\) KAN by leveraging \(kx={\rm exp}({\rm log}k+{\rm log}x)\).</p> <h2 id="closing-remarks">Closing Remarks</h2> <p>A model performs well on a task, when the inductive bias of the model meets the inductive bias of the task. So there is no free lunch – because KANs are good at science, there must be something KANs are not good at. MLPs are not good at science, people have shown their effectivenss on many non-scientific tasks including vision and languages. Trained as a physicist, I tend to think in redunctionsim. I constantly think about how a dataset can be generated from simple primitives (a process my advisor Max calls “braining”, i.e., training with my own brains), because it can inspire the design of better models. However, real world can be too complicated for my tiny brain to fully imagine and understand. Sometimes we just have to get our hands wet before burning out our brains. Combing back to the KAN vs MLP case, although I would love to understand their strengths and limitations with pure reasoning (philosophical thinking is one way), empirical experiments (guided by some reasoning) are probably more effective.</p>]]></content><author><name></name></author><category term="AI"/><category term="Kolmogorov-arnold-networks"/><category term="interpretability"/><category term="AI-for-Science"/><summary type="html"><![CDATA[Recently, collaborators and I proposed a new type of neural networks called the Kolmogorov-Arnold Networks (KANs), which are somewhat similar to but mostly different from Multi-Layer Perceptrons (MLPs).]]></summary></entry><entry><title type="html">Symbolic Regreesion? Structure Regression!</title><link href="https://kindxiaoming.github.io/blog/2023/structure-regression/" rel="alternate" type="text/html" title="Symbolic Regreesion? Structure Regression!"/><published>2023-07-08T00:00:00+00:00</published><updated>2023-07-08T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2023/structure-regression</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2023/structure-regression/"><![CDATA[<p>Many scientific problems can be formulated as regression: given independent variables \((x_1, x_2, \cdots, x_d)\) and dependent variable \(y\), we want to find a function such that \(y = f(x_1,x_2,\cdots, x_d)\). Scientists, especially physicists, have put great effort and labor into solving tasks of this kind. For example, Kepler spent eight years staring at astronomical data, before he figured out his eponymous three laws. By contrast, many scientists are less crazy about symbolic formulas. They are content with empirical laws. To be specific, they set \(f\) to be a specific functional form, allowing some tunable empirical parameters, which may not have very clear physical meanings.</p> <p>Two goals mentioned above, symbolic regression (SR) and empirical regression (ER), have their own limitations: SR is powerful but brittle, while ER is robust but constrained. Is it possible to have something in the middle, which is both powerful and robust? The answer is structure regression (StR)! This blog is organized as such: Firstly, I discuss what is structure regression, arguing why structure regression is probably a better goal to pursue than symbolic regression. Secondly, I describe our method BIMT to do structure regression. Finally, I bet on scientific fields that structure regression is promising for, and most importantly, call for collaboration!</p> <h2 id="symbol-or-structure">Symbol or Structure?</h2> <p>Although symbols play huge roles in mathematics and physics, I am not a big believer for “everything is symbolic”. For example, only very few unary functions are labeled as “symbolic” or “analytic”, such as \(f(x)=x^2, {\rm sin}(x), {\rm exp}(x)\). If one randomly draw a 1D curve on a piece of paper, only probability one the function is symbolic. “Symbolic functions” defined by us are too limited. The definition can also strongly depend on contexts: hypergeometric functions may be viewed as symbolic in mathematical physics, but may be unacceptably too complicated in engineering.</p> <p>In contrast to symbols, structures are probably more universal. An example of structure is independence. Independence is what makes physics possible at all: our universe has infinite degrees of freedom, but physical systems we care about only depend on a finite number of them. A similar structural property is modularity, which allows us to decompose a huge object into small pieces which are much more manageable. Other examples of structural properties are hierarchy, compositionality, reusability, sparsity etc.</p> <p>So, what does structure regression mean? Given independent variables \((x_1, x_2, \cdots, x_d)\) and dependent variable y, we want to find a structure (or structures) such that \(y = S(x_1,x_2,\cdots, x_d)\). Suppose the structure is additive modularity, then \(S(x_1,x_2,\cdots, x_d)=\sum_{i=1}^d f_i(x_i)\). Note that figuring out symbolic forms of \(f_i (i=1,2,\cdots,d)\) is not necessary for structure regression. As long as the additive property is discovered, a victory is claimed.</p> <p>One may say that structure regression is a weaker or less ambitious version of symbolic regression. This is true, but for good reasons. Firstly, as I argued above, there are cases where symbolic regression is impossible. If so, structure regression is probably the best thing one can hope for! Secondly, for cases where symbolic regression are indeed possible, structure regression is a nice intermediate milestone to target for, because it greatly simplifies the symbolic regression problem (e.g., AI Feynman).</p> <p>One may feel that my critiques for symbolic regression can directly apply to structure regression: “the search space for symbolic regression is (discrete) symbolic formulas, the search space for structure regression is (discrete) structures. In both cases, you need to specify your symbols/structures (which can be limited and context-dependent), and combinatorial searches are needed.” This is not true. There are key differences between symbolic regression and structure regression. Let’s say we take a neural network. Structure regression only cares about the graph of how neurons connect to each other. In addition to that, symbolic regression also cares about how each neuron processes signals. Structural regression is more robust than symbolic regression: Remember how in condensed matter physics or in many emergent phenomenon, robust/universal macroscopic behavior is usually only dependent on the relations of microscopic units, but the details of each unit are not so relevant. Moreover, structure regression can be made differentiable and can be easily visualized. This is not obvious at all, but in the following I will describe a machine learning method for structure regression that mets these desirable properties.</p> <h2 id="structure-regression-with-bimt-brain-inspired-modular-training">Structure regression with BIMT (Brain-inspired Modular Training)</h2> <p>Before introducing our method, let’s see what our method can achieve. For symbolic formulas with structural properties, our trained fully-connected neural networks display interpretable modules. You can understand the structures immediately after seeing the connectivity graph for the network!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bimt_example-480.webp 480w,/assets/img/blogs/bimt_example-800.webp 800w,/assets/img/blogs/bimt_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bimt_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We present three examples here. The first example is independence. Two outputs depend on non-overlapping sets of input variables. Our method is able to split the network into two independent parts. The second example is feature sharing. We’d expect that \(x_i^2 (i=1,2,3)\) are important intermediate features. Our method is able to recover this intuition. The third example is compositionality. We’d expect to compute sum of squares first, and then take the square root. Our method automatically discovers this structure too. Note that all input and output variables are numeric (nothing symbolic). With our training method, neural networks self-reveal their structures, i.e., structure regression is achieved.</p> <p>Now it’s time to describe our method BIMT (Brain-inspired modular training). For technical details please refer to the paper, or the podcast. Here we do a quick walk through the basic idea.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bimt-480.webp 480w,/assets/img/blogs/bimt-800.webp 800w,/assets/img/blogs/bimt-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bimt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Our goal is to encourage modules to emerge when training (non-modular) neural networks. Remember that human brains are remarkably modular. Is there a lesson we can learn from our brains? Our brains are modular because modular brains require less energy and react faster than non-modular brains, hence have survival advantages in evolution. Modular brains have more local connections than non-local connections. To introduce the notion of “locality” into artificial neural networks, we embed NNs into geometric spaces (e.g., 2D Euclidean space), assigning each neuron a spatial coordinate and defining lengths for neuron connections. We penalize longer connections more than shorter connections in training. Specifically, our penalty term is a simple modification to \(L_1\) regularization: \(L_1\) penalty is \(\lambda \lvert w\rvert\), while our penalty is \(\lambda \ell \lvert w\rvert\) where \(w\), \(\ell\) are the weight and the length of the neuron connection, respectively. Besides this differentiable penalty term added to the training objective to encourage locality, we also allow swaps of neurons, to avoid topological issues.</p> <h2 id="structure-regression-for-science">Structure regression for science</h2> <p>There are two main paradigms for science: model-driven and data-driven. Model-driven approaches start from (symbolic) models, predicting what will happen via deduction. Data-driven approaches start from data, predicting what will happen via induction. Each paradigm has its pros and cons: Model-driven methods are interpretable, but require creativity of researchers. Data-driven methods are less intellectually challenging, but may not be interpretable. Usually these two paradigms are separately applied. Recently there have been efforts to integrate models into data-driven approches (e.g., PINNs), but not the other way around, i.e., use data-driven methods to inspire models development. Structure regression can exactly do that.</p> <p>Putting interpretability aside, encouraging structures in neural networks can improve generalization. This is very important for cases where only very few data are accessible, when controlled experiments are either impossible or too expensive.</p> <p>I’m very excited to apply structure regression to scientific problems! BIMT might be a reasonable starting point, but very likely not the ultimate answer, so there will be a lot of fun and fruitful things to do along the journey. If you have any applications in your fields, please don’t hesitate to email me! If you ask me which fields structure regression is most promising for, my prior would be a uniform distribution over all scientific domains! Said that, if you care about interpretability or scientific insights, maybe more well-defined fields (e.g., mathematics, physics) are better. If you want to do something more practical or useful, fields with some extent of messiness (e.g., chemistry, biology, social science, engineering) might be better. To be concrete, I think structure regression is quite promising for the following fields:</p> <ul> <li>Fluid mechanics</li> <li>Biophysics</li> <li>Astro &amp; Plasma physics</li> <li>Quantum chemistry (DFT)</li> <li>Molecular dynamics</li> <li>Atmospheric science</li> <li>Biology (Protein folding)</li> <li>Economy</li> <li>…</li> </ul> <p>The list can go on and on. Again, shoot me an email if you are working on a scientific problem and are interested in applying structure regression to it! I’m open to any form of collaboration. 🙂</p>]]></content><author><name></name></author><category term="Interpretability"/><category term="Modularity"/><category term="Neural-Networks"/><category term="Neuroscience-for-AI"/><summary type="html"><![CDATA[Many scientific problems can be formulated as regression: given independent variables \((x_1, x_2, \cdots, x_d)\) and dependent variable \(y\), we want to find a function such that \(y = f(x_1,x_2,\cdots, x_d)\). Scientists, especially physicists, have put great effort and labor into solving tasks of this kind. For example, Kepler spent eight years staring at astronomical data, before he figured out his eponymous three laws. By contrast, many scientists are less crazy about symbolic formulas. They are content with empirical laws. To be specific, they set \(f\) to be a specific functional form, allowing some tunable empirical parameters, which may not have very clear physical meanings.]]></summary></entry><entry><title type="html">A Good ML Theory is Like Physics – A Physicist’s Analysis of Grokking</title><link href="https://kindxiaoming.github.io/blog/2023/physics-ml-theory/" rel="alternate" type="text/html" title="A Good ML Theory is Like Physics – A Physicist’s Analysis of Grokking"/><published>2023-06-16T00:00:00+00:00</published><updated>2023-06-16T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2023/physics-ml-theory</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2023/physics-ml-theory/"><![CDATA[<blockquote> <p>Only calulate after you know the answer.</p> </blockquote> <p>Machine Learning (ML) has demonstrated impressive empirical performance on a wide range of applications, from vision, language and even science. Sadly, as much as theorists refuse to admit, recent developments in ML are mainly attributed to experimentalists and engineers.</p> <p>This calls for the questions:</p> <ul> <li>(Q1) Do we really need theories of ML?</li> <li>(Q2) If so, what do good ML theories look like?</li> </ul> <p>Trained as a physicist, I am always fascinated by the way physicists approach the reality. Usually a mental picture is formed, and a lot of useful predictions can be made even without the need to write down any math equations. Physicists are good at identifying important factors in problems, while neglecting irrelevant factors — To me, the “Spherical chicken in the vacuum” joke sounds more like a compliment rather than teasing. As a physicist, it is always satisfying to have a theory for certain phenomenon, but the theory is not necessarily a pile of enigmatic mathematical symbols or equations. What physicists really care about is how well our theory can describe reality (the goal), not how (tools used to reach the goal). My personal and perhaps biased feeling for (most) current ML theories is that: these theories focus too much on the tools to notice they are actually deviating from the original goal.</p> <p>Consequently, my personal take to the questions posed above are:</p> <ul> <li>(A1) Yes, we do need ML theories.</li> <li>(A2) Good ML theories should be “physics-like”. In short, they should place emphasis on reality, admit intuitive mental pictures, have predictive power, and finally guide future experiments.</li> </ul> <p>It is worth mentioning that “physics-like” ML theory does not only mean certain tools in physics, or certain physical phenomenon, but also (more importantly!) the mindset that physicists adopt to approach our physical reality (will be made precise). This blog consists of three parts:</p> <ul> <li>(a) Why do we need theory? I will review the philosophy of theory in science.</li> <li>(b) Physics-like ML theory. I will define “physics-like” ML theory.</li> <li>(c) An example: Grokking. I will demonstrate the power of “physics-like” thinking applied to demystify “grokking”, one puzzling phenomenon.</li> </ul> <h2 id="why-do-we-need-theory">Why do we need theory?</h2> <p>You may think the word “theory” is a niche term for science nerds, but we all human beings have a theory carved in our DNA: To survive and win the natural selection game, we must have understood the theory of the physical world around us, without even realizing it. If you go look up the word “theory” in a dictionary, you will get something like this:</p> <p>“A supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.”</p> <p>Two takeaways from this definition: (1) a theory is intended to explain something X. (2) a theory is based on principles independent of X. These two point can be elegantly unified from an information-theoretic perspective. (1) is saying that the theory simplifies X (information compression). (2) is saying that the theory can predict X (information gain).</p> <p>Let me elaborate more. Assume we were in our ancestors’ position and don’t know about the cycle of four seasons, we want to know when to reap and sow. Can we just document temperature and other conditions everyday, without trying to figure out a theory (i.e., every year has four seasons) for it? This is no good at all! It is too redundant, failing the information compression criterion. Also, it cannot provide anything useful about the future, failing the information gain criterion. So, it is clear that a good theory should be able to do information compression to past observations, and can obtain information gain (i.e., have predictive power) for future experiments/observations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/strong_inference-480.webp 480w,/assets/img/blogs/strong_inference-800.webp 800w,/assets/img/blogs/strong_inference-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/strong_inference.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I like this “Strong Inference” paper published in Science in 1964, discussing about what good theories look like. I especially like the quote “A theory which cannot be mortally endangered cannot be alive”. If a theory predicts a thing that nobody believes (”mortally endangered”), but finally it turns out that thing actually happens, this is a strong signal that the theory is undeniably true (“alive”). This is again aligned with the information gain criterion: a lot of surprisal (information) is gained from the theory. The author also quote the “Eightfold Way” example, where particles physicists managed to unify eight mesons under a simple Lie group SU(3) (information compression) and predict the last missing particle (information gain).</p> <h2 id="physics-like-ml-theory">“Physics-like” ML theory</h2> <p>Physics has been playing a huge role in understanding and improving machine learning. Physics can help ML technically, by offering tools and concepts that physicists have developed for hundreds of years. There have been great examples where physicists use their toolkits to tackle problems in ML. One well-known example is to apply field theory analysis to deep neural networks (DNNs). Since DNNs are special kinds of complex systems and dynamical systems that physicists have always been dealing with, maybe it is not that surprising that physicists have off-the-shelf toolkits immediately useful for DNNs.</p> <p>Although physics can help ML technically, I want to argue that, more importantly, physics can help ML philosophically. This is something I found quite fundamental and potentially game-changing, but I don’t yet know many people sharing the same intuition. That’s why I’m writing this blog.</p> <p>So when I say “physics-like”, I don’t necessarily mean technical tools or concepts in physics, but rather a mindset that physicists adopt to approach our physical reality. The physicists’ mindset is:</p> <ul> <li>(1) Reality. We put an emphasis on reality.</li> <li>(2) Simplicity. We view simplicity as beauty.</li> <li>(3) Dynamics. We view the world dynamically rather than statically.</li> <li>(4) Intuition. We appreciate mental pictures more than (vacuous) math rigour.</li> <li>(5) Control. We design well-controlled experiments to test our theory.</li> </ul> <p>Of course the list goes on and on, but to me personally, these strategies really help me in my quest to understanding ML. I’ll now use a concrete example to illustrate how these physics-like thinking help me understand ML.</p> <h2 id="an-example-of-physics-like-thinking-grokking">An example of physics-like thinking: Grokking</h2> <p>About two years ago, when we were doing a journal club, an OpenAI paper called “grokking” immediately caught my eyes. The paper found that, for neural networks trained on algorithmic datasets, generalization happens long after memorization (see the accuracy curves below, from their paper).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_curve-480.webp 480w,/assets/img/blogs/grokking_curve-800.webp 800w,/assets/img/blogs/grokking_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is quite striking to me, since we normally don’t see such huge delays on standard ML datasets, e.g., image classifications. This sounds like: a neural network is a student who is diligent but not so smart, who first tries to memorize everything in the textbook without understanding them, but eventually has a Eureka moment when understanding happens (“grokking”). I was thinking, this weird phenomenon must have a dynamical reason behind it. I was enchanted by this phenomenon – To me, it is as exciting as a new particle being discovered in physics, here a “grokkon”. I decided to understand grokking in my own way, and I strongly feel that a “physics-like” theory is probably most promising.</p> <h3 id="simplicity-identifying-relevant-variables">Simplicity: Identifying relevant variables</h3> <p>When a physicist say “I have a solution but it only works for spherical chickens in a vacuum!”, I appreciate it a lot since it is really saying “What matters most is the size of the chicken”. This is a reasonable and smart simplification, as long as it can answer the target question. It is widely agreed in physics that “Everything should be made as simple as possible, not but simpler”.</p> <p>Coming back to the grokking case, I was wondering: what is the most important factor here? Now recalling the diligent student analogy: At first, the neural network memorizes the whole training datasets by rote learning; then the neural network finds a simpler solution which requires memorizing much fewer samples. So the relevant variable here is the number of memorized samples. Although the number of memorized samples is not observable (at least I don’t know how to estimate it), intuitively it is related to the complexity or capacity of the neural network. Now the question becomes: which complexity measure is relevant here? The number of parameters is of course not appropriate, because the architecture (hence the number of parameters) is fixed in training. This is when the weight norm w (\(L_2\) norm of weight parameters) came to my mind. To verify that the weight norm is relevant to grokking, we keep track of the weight norm (purple curve below). We found that weight norm is highly correlated with overfitting and generalization. When overfitting happens (~50-100 step), the weight norm increases; when generalization happens (~5000-10000 step), the weight norm decreases. This observation is aligned with the diligent student analogy: the student tries to memorize everything (need more capacity, i.e., large weight norm), and then manages to simplify things (need fewer capacity, i.e., small weight norm).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_simplicity-480.webp 480w,/assets/img/blogs/grokking_simplicity-800.webp 800w,/assets/img/blogs/grokking_simplicity-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_simplicity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="dynamics-viewing-the-world-dynamically">Dynamics: Viewing the world dynamically**</h3> <p>After identifying that the memorization circuit has a large weight norm, while the generalization circuit has a small weight norm, our next question is: how does the neural network transit from the large norm state to the small norm state? What is the dynamics? This is a natural question physicists would ask – everything must have a dynamical origin. What is observed now must be originated from something in the past. Our physical universe has a time dimension, so are neural networks (time = training step).</p> <p>After overfitting, the training loss is almost zero, hence the gradients of the prediction loss is almost zero. The only explicit force that drives the model to change is weight decay. The role of weight decay \(\gamma\) is to reduce the weight norm. If we view the weight space as our physical space, then the weight decay plays the role of velocity along weight norm. From elementary school, we know that a traveller needs to spend time \(t=d/v\propto v^{-1}\) to travel along distance d from city A to city B with velocity \(v\) (see Figure below). Analogously, it takes \(t\propto \gamma^{-1}\) to “travel” from circuit A (memorization, large norm) to circuit B (generalization, small norm).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_dynamics-480.webp 480w,/assets/img/blogs/grokking_dynamics-800.webp 800w,/assets/img/blogs/grokking_dynamics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_dynamics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is worth noting that the time to generalize \(t\propto\gamma^{-1}\) is a prediction our theory makes. It is “mortally endangered” in the sense that this simple relation sounds too good to be true. However, this is indeed true! We were able to verify the inverse relation in experiments (see below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_verification-480.webp 480w,/assets/img/blogs/grokking_verification-800.webp 800w,/assets/img/blogs/grokking_verification-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_verification.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="intuition-forming-mental-pictures">Intuition: Forming mental pictures</h3> <p>Although the above theory makes quite accurate predictions, it is a bit oversimplified, and does not provide a global picture for loss landscapes. I hope to have a more detailed mental picture (but still, as simple as possible).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_mental_picture-480.webp 480w,/assets/img/blogs/grokking_mental_picture-800.webp 800w,/assets/img/blogs/grokking_mental_picture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_mental_picture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The mental picture is (figure above, left): in the weight space of model parameters, there exists a hypersphere w=w_c where all generalization solutions lie on (called “Goldilocks zone”, in green). Inside the sphere are underfitting solutions (non-zero training loss). Outside the sphere, there are many overfitting solutions, connected as valleys (in orange). When the neural network is initialized to have a large norm (black square), it will first quickly goes to a nearby overfitting solution, but then slowly drifts along the overfitting valley towards the hypersphere. The velocity of the drift, as we discussed above, is proportional to the weight decay, which is usually set to be small in NN training.</p> <p>We can further simplify the picture (figure above, right) to 1D: since we only care about weight norm, we plot train/test losses against it. The training and test losses would look like “L” and “U”, respectively. Grokking happens when the neural network lands on the large weight norm region (where the shape of “L” and “U” mismatch).</p> <h3 id="control-eliminate-grokking">Control: Eliminate grokking</h3> <p>Since grokking is undesirable, our final goal is to control(i.e., eliminate) it. The mental picture suggests that grokking happens due to too large weight norms. This, on the other hand, suggests that constraining the model to have a small weight norm can help eliminate grokking. We set the initialization to be small, i.e., 0.8 times the standard initialization, and constrain the weight norm to be constant in training. We found this strategy indeed eliminates grokking!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/grokking_eliminate-480.webp 480w,/assets/img/blogs/grokking_eliminate-800.webp 800w,/assets/img/blogs/grokking_eliminate-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/grokking_eliminate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We end up publishing two papers on grokking at NeurIPS 2022 (Oral) and ICLR 2023 (Spotlight), although at first I was very concerned that ML community would not appreciate such “physics-like” papers. This makes me happily realize that ML community, just like the physics community, do appreciate mental pictures and care about reality, although researchers in the ML community have long been unwillingly involved in the arms race of (most times) vacuous mathematical rigour.</p> <h2 id="closing-remarks">Closing Remarks</h2> <p>In this blogpost, I described a mindset that physicists adopt in physics research, and argue that this mindset is also beneficial to ML theory research. To have such physics-like way of thinking, one doesn’t need to be a physicist oneself, but really the point is to focus on the goal (understanding ML) rather than the tool (vacuous math rigour). I do believe that best ML practitioners are physicists at heart: they may have the mental pictures deep in mind without even realizing them! My research goal is to make these mental pictures explicit and accessible to anyone who is willing to understand: a good physicist can even let his/her grandma appreciate the beauty of physics!</p>]]></content><author><name></name></author><category term="AI"/><category term="Grokking"/><category term="Science-of-AI"/><summary type="html"><![CDATA[Only calulate after you know the answer.]]></summary></entry></feed>