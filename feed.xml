<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-10T07:26:52+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Sparse attention 2 – Unattention head, branching dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/" rel="alternate" type="text/html" title="Sparse attention 2 – Unattention head, branching dynamics"/><published>2026-01-10T00:00:00+00:00</published><updated>2026-01-10T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/sparse-attention-1/">previous blog</a>, we found that a single-layer attention model (without positional embeddings) can learn to copy the <strong>current</strong> token. But what about copying <strong>earlier</strong> tokens? Since positional embeddings are missing, it seems impossible to determine where each token is located in the sequence. Under this reasoning, the task should be unsolvable, and I would expect the loss to be \({\rm log}(V)\), or equivalently the perplexity to be \(V\), where \(V\) is the vocabulary size.</p> <p>As we show below, however, the story is more subtle.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to copy the previous token. Taking context length = 4 as an example, this corresponds to<br/> \([A][B][C][D] \rightarrow [C]\).<br/> In fact, due to the absence of positional embeddings, and because attention is equivariant under permutations of previous tokens, copying <em>any</em> previous token defines an equivalently difficult task.</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>. The model consists only of an Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers.</p> <hr/> <h2 id="observation-loss-plateau-embedding-branching-dynamics">Observation: loss plateau, embedding branching dynamics</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 2 (task: \([A][B]\to[A]\)), as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em> (as in the <a href="/blog/2026/sparse-attention-1/">previous blog</a>), where the perplexity is approximately half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/perplexity-embed-480.webp 480w,/assets/img/blogs/sparse-attention-2/perplexity-embed-800.webp 800w,/assets/img/blogs/sparse-attention-2/perplexity-embed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/perplexity-embed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By visualizing the evolution of the embeddings, we find that the dynamics proceed in two stages. In the first stage, there is a dominant direction along which the embeddings expand. Only in the second stage do the embeddings begin to move along the orthogonal direction, eventually forming a circular structure. This second stage also exhibits interesting branching dynamics:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/gif-480.webp 480w,/assets/img/blogs/sparse-attention-2/gif-800.webp 800w,/assets/img/blogs/sparse-attention-2/gif-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/gif.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="whats-happening">What’s happening?</h2> <p>My first reaction was: how could this task possibly be solved? The low final loss clearly indicates that the model must have developed some internal algorithm to accomplish it. The emergence of circular embeddings is particularly suggestive, hinting that the model has learned something nontrivial and elegant.</p> <p>We can inspect the learned \(W_Q/W_K\) matrices (\(W_V\) is unimportant here since we only care about the attention pattern):</p> \[W_Q = \begin{pmatrix} 13.24 &amp; -3.40 \\ -3.28 &amp; -14.7 \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -14.03 &amp; 3.56 \\ 3.19 &amp; 13.99 \\ \end{pmatrix}\] <p>We observe two numerical clusters (ignoring signs): one around 13 (denoted \(a\)), and another around 3 (denoted \(b\)). We therefore approximate them by the following symbolic forms:</p> \[W_Q = \begin{pmatrix} a &amp; -b \\ -b &amp; -a \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -a &amp; b \\ b &amp; a \\ \end{pmatrix},\] <p>Let the two inputs to the attention layer be \(E_1 = (x_1, y_1)^T\) and \(E_2 = (x_2, y_2)^T\). The attention logit between them is</p> \[E_2 W_K^T W_Q E_1 = (a x_1 - b y_1,\,-b x_1 - a y_1) \cdot (-a x_2 + b y_2,\, b x_2 + a y_2) = -(a^2 + b^2)(x_1 x_2 + y_1 y_2).\] <p>Since both \(E_1\) and \(E_2\) lie on a circle, the term \(x_1 x_2 + y_1 y_2\) is simply their inner product. The self-attention logit is</p> \[E_2 W_K^T W_Q E_2 = -(a^2 + b^2)(x_2^2 + y_2^2) &lt; E_2 W_K^T W_Q E_1.\] <p>This means that the model attends more strongly to the previous token than to the current token. Importantly, however, this effect is achieved not by <em>increasing</em> attention to the previous token, but by <em>suppressing</em> attention to the current token. This distinction is subtle for context length = 2, but becomes crucial for longer contexts.</p> <hr/> <h2 id="longer-context-length">Longer context length</h2> <p>The attention layer has discovered a way to <em>unattend</em> the current token, but it still cannot distinguish among earlier tokens due to permutation equivariance in the absence of positional embeddings. As a result, the model can do no better than guessing: for context length \(C\), it effectively selects one token uniformly at random from the first \(C-1\) tokens, with probability \(\frac{1}{C-1}\) of choosing any particular one. The probability of guessing the correct token is therefore \(\frac{C-2}{C-1}\).</p> <p>With vocabulary size \(V\), the best achievable perplexity is thus \(\frac{C-2}{C-1} V\), which is better than pure random guessing \(V\). We verify empirically below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/best_perplexity-480.webp 480w,/assets/img/blogs/sparse-attention-2/best_perplexity-800.webp 800w,/assets/img/blogs/sparse-attention-2/best_perplexity-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/best_perplexity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This suggests that for long sequences, an attention layer without positional embeddings cannot reliably select a specific previous token. Positional embeddings are therefore necessary to implement, for example, a previous-token head. That said, the discovered strategy of <em>unattending the current token</em> is itself an interesting and nontrivial phenomenon.</p> <hr/> <h2 id="acknowledgement">Acknowledgement</h2> <p>Kaiyue Wen suggests that the sticky plateau might be due to the fact that Sub-n-grams are near-stationary Points (<a href="https://arxiv.org/pdf/2508.12837v1">paper</a>).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/135t06Hz2FCrEKiTVErR8P4_MiPhl4pnE?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 2 -- Unattention head, branching dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 1 – sticky plateau and rank collapse</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/" rel="alternate" type="text/html" title="Sparse attention 1 – sticky plateau and rank collapse"/><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>The idea of attention is to select what we want from a collection of items. Token embeddings select based on content itself, while positional embeddings select based on position.</p> <p>In this article, we examine attention’s ability to select based on content. Therefore, for simplicity, we ignore positional embeddings.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is: select the current token. Taking context length = 4 as an example, this is<br/> \([A][B][C][D] \rightarrow [D]\).</p> <p><strong>Model</strong><br/> The model consists of only Embedding, Unembedding, and a single Attention layer, with no MLP layers. The main tunable parameters are the vocabulary size, embedding dimension, and context length.</p> <hr/> <h2 id="observation-sticky-plateau">Observation: sticky plateau</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 4, as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em>, where the perplexity corresponds to half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/sticky-plateau-480.webp 480w,/assets/img/blogs/sparse-attention-1/sticky-plateau-800.webp 800w,/assets/img/blogs/sparse-attention-1/sticky-plateau-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/sticky-plateau.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Explanation</strong><br/> We visualize the evolution of the embeddings (vocab size = 4) and find that there are two stages. In the first stage, the blue and yellow directions coincide, and the green and red directions coincide. As a result, the model cannot distinguish blue from yellow, or green from red, but it can distinguish which group a token belongs to. In the second stage, the elements within each group are finally separated.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/branching-480.webp 480w,/assets/img/blogs/sparse-attention-1/branching-800.webp 800w,/assets/img/blogs/sparse-attention-1/branching-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/branching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="how-general-is-the-sticky-plateau">How general is the sticky plateau?</h2> <p>Although the sticky plateau phenomenon is interesting, how general is it? When the embedding dimension increases, there is enough resolution to distinguish different tokens, and the plateau may disappear. We indeed observe that this is the case. However, if we increase the context length, the sticky plateau comes back again. The picture, therefore, is that packing as many items as the context length into an embedding space of a given dimension leads to a situation where the presence or absence of a sticky plateau seems to correspond to a percolation phase transition.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/generality-480.webp 480w,/assets/img/blogs/sparse-attention-1/generality-800.webp 800w,/assets/img/blogs/sparse-attention-1/generality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/generality.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="rank-collapse">Rank collapse</h2> <p>From the embedding visualizations above, we find that the embeddings initially expand along a single direction, and only later expand along another direction. Therefore, we expect that the effective dimensionality of the embeddings (defined in the same way as in the previous <a href="/blog/2026/unigram-toy-1/">blog post</a>), as well as the effective rank of the Q/K/V matrices, should exhibit corresponding dynamics.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/eff_rank-480.webp 480w,/assets/img/blogs/sparse-attention-1/eff_rank-800.webp 800w,/assets/img/blogs/sparse-attention-1/eff_rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/eff_rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Left: Q/K also appear very sticky during the loss plateau, which may indicate that the dynamics of Q/K are the primary cause of the loss plateau.</p> <p>Right: when all parameters are scaled up, Q/K become less sticky. Another observation that differs from the low-dimensional case is that when the loss decreases, the rank also decreases; when the loss plateaus, the rank increases; and when the loss decreases again, the rank decreases accordingly. The picture is that when the model is very clear about which features are useful, it aggressively exploits (grows) those features while (relatively) suppressing others, leading to a decrease in rank. When the loss reaches a plateau, the model explores which new features should grow, leading to an increase in rank. Once a useful new feature emerges, the model again exploits it, causing the rank to decrease.</p> <hr/> <h2 id="test-on-large-models">Test on large models</h2> <p>The sticky plateau phenomenon predicts a loss plateau at \({\rm Log}(V/2)\), about 0.3 nats below \({\rm Log}(V)\). To the best of my knowledge, this plateau has never been observed in LLMs, maybe due to</p> <ul> <li>learning rate warmup has already nicely handled this problem</li> <li>we did not zoom in enough to see the plateau</li> <li>the sticky plateau is simply less relevant for large models</li> </ul> <p>Either way, no matter if the sticky plateua is relevant for LLMs, we shoud better understand/control representation/rank collapses and the percolation phase transition.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1lXsAuzQ2nw009an8lNt9PYv_HqR1GhoV?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 1 -- sticky plateau and rank collapse}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Unigram toy model is surprisingly rich – representation collapse, scaling laws, learning rate schedule</title><link href="https://kindxiaoming.github.io/blog/2026/unigram-toy-1/" rel="alternate" type="text/html" title="Unigram toy model is surprisingly rich – representation collapse, scaling laws, learning rate schedule"/><published>2026-01-08T00:00:00+00:00</published><updated>2026-01-08T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/unigram-toy-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/unigram-toy-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Many phenomena have been observed during the training of LLMs—such as scaling laws and (early-stage) representation collapse. Empirically, there are also many training tricks—for example, carefully designing the learning rate schedule, including early LR warmup and late-stage LR decay. Are these phenomena and tricks <strong>specific to LLMs</strong>? Can we reproduce them in <strong>much simpler toy models</strong>?</p> <p>The goal of this blog post is to study an extremely simple <strong>Unigram model</strong>, and surprisingly, we find that many of the phenomena observed in LLMs already appear in this toy setting. Thanks to the simplicity of the toy model, we can more systematically investigate the <strong>origins</strong>, <strong>conditions</strong>, and <strong>control mechanisms</strong> of these phenomena.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider the case with context length equal to 1, i.e., predicting the next token \([B]\) from the current token \([A]\).</p> <p><strong>Dataset</strong><br/> We consider a Unigram dataset—\([A]\) and \([B]\) are independent and are independently drawn from a discrete distribution. Let the vocabulary size be \(V\), and the discrete distribution be<br/> \({\{p_i; \, p_i \ge 0, \sum_{i=1}^V p_i = 1\}}.\)</p> <p>Note that this dataset is even simpler than a Bigram dataset, because here the output \([B]\) does <strong>not</strong> depend on the input \([A]\) at all. The model’s task is simply to learn the token frequency distribution. One may ask: why study such a simple dataset, and does it have any relevance to natural language?</p> <ul> <li>First, the Zipf (heavy-tailed) frequency distribution is a characteristic feature of natural language. Although natural language has many other properties, here we focus exclusively on how <strong>single-token frequency distributions</strong> affect training dynamics.</li> <li>Second, in the learning rate warmup section below, we will provide evidence—namely a <em>sticky plateau</em>—suggesting that LLMs indeed exhibit signs of learning Unigram statistics during early training.</li> </ul> <p>For this dataset, the minimum achievable loss is simply the entropy of the frequency distribution: \(L_{\rm min} = S(\{p_i\}) = - \sum_{i=1}^V p_i \log p_i.\) We further assume that the frequencies follow a power law: \(p_i \sim i^{-\alpha}, \quad i = 1, 2, \dots, V,\) where \(\alpha\) controls the degree of heavy-tailedness. The case \(\alpha = 1\) corresponds to the standard Zipf distribution.</p> <p><strong>Model</strong><br/> We consider a very simple model with only an embedding layer and an unembedding layer. For token \(i\), the embedding vector is \(E_i \in \mathbb{R}^D\) and the unembedding vector is \(U_i \in \mathbb{R}^D\). We usually tie the two weights, i.e., \(E_i = U_i.\)</p> <hr/> <h2 id="1d-embedding-is-sufficient">1D embedding is sufficient</h2> <p>We take \(V = 10\), \(\alpha=1\), and embedding dimension \(D = 1\). Training uses batch size 128, while evaluation uses batch size 2,000,000. All data are drawn online. We find that training converges to the entropy (left and middle plots), and the learned embeddings show that tokens with higher frequency have larger values.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/1d_embedding-480.webp 480w,/assets/img/blogs/unigram-toy-1/1d_embedding-800.webp 800w,/assets/img/blogs/unigram-toy-1/1d_embedding-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/1d_embedding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here is a small detail worth pointing out: if we <strong>do not tie</strong> \(U\) and \(E\), the model has more degrees of freedom and converges faster.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/1d_embedding_untie-480.webp 480w,/assets/img/blogs/unigram-toy-1/1d_embedding_untie-800.webp 800w,/assets/img/blogs/unigram-toy-1/1d_embedding_untie-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/1d_embedding_untie.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Mathematical construction</strong><br/> Why emphasize this detail? When \(E\) and \(U\) are not tied, we can easily construct \(E_i = 1, \quad U_i = \log p_i,\) which exactly realizes the target distribution. However, when \(E\) and \(U\) share parameters, we require the logits \(E_i U\) and \(E_j U\) to induce the same probability distribution, which implies \(E_i = E_j\) and hence \(U_i = U_j\). This would lead to a uniform distribution, contradicting the heavy-tailed distribution we want.</p> <p>In practice, the model seems to find a sufficiently good trade-off: embeddings for different tokens have similar magnitudes, yet remain clearly distinguishable. Since NanoGPT uses weight tying, we will also continue to use weight tying in what follows.</p> <hr/> <h2 id="representation-collapse">Representation collapse</h2> <p>Since a 1D embedding is already sufficient, an extreme case with higher-dimensional embeddings is that different embedding dimensions behave identically. In that case, the effective subspace dimension of the embeddings is actually 1. While reality may not be so extreme, this motivates us to measure the <strong>effective dimensionality</strong> of representations, which may be related to representation collapse observed in LLMs.</p> <p>For an embedding matrix, we perform principal component analysis (PCA), treat the explained variance ratios as a probability distribution, compute its entropy \(S\), and define the effective dimension as \(n_{\rm eff} = \exp(S).\)</p> <p>Taking \(V = 100\) and \(D = 100\), we indeed observe representation collapse, which becomes more pronounced for larger learning rates.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/repr_collapse-480.webp 480w,/assets/img/blogs/unigram-toy-1/repr_collapse-800.webp 800w,/assets/img/blogs/unigram-toy-1/repr_collapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/repr_collapse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="loss-scaling-laws">Loss scaling laws</h2> <p>We study temporal scaling of the form \(L - L_{\rm min} \propto t^{-\beta},\) where \(\beta\) measures the speed of loss decay. We vary \(\alpha\) and \(V\) and examine how \(\beta\) depends on them. We fix \(D = 100\) and learning rate \(\text{lr} = 0.001\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/scaling_law-480.webp 480w,/assets/img/blogs/unigram-toy-1/scaling_law-800.webp 800w,/assets/img/blogs/unigram-toy-1/scaling_law-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/scaling_law.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that:</p> <ul> <li>larger \(V\) leads to smaller \(\beta\);</li> <li>larger \(\alpha\) leads to larger \(\beta\).</li> </ul> <p>Both trends are intuitive: larger \(V\) or smaller \(\alpha\) corresponds to a harder task.</p> <hr/> <h2 id="learning-rate-decay">Learning rate decay</h2> <p>We next ask whether late-stage learning rate decay is helpful in this toy setup. We find that learning rate decay indeed helps, and the improvement becomes larger as the system size increases (i.e., larger embedding dimension \(D\)). We train for 2000 steps with \(\text{lr} = 0.01\), then linearly decay the learning rate from 0.01 to 0.001 over the final 1000 steps. We fix \(V = 100\) and \(\alpha = 1\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/lr_decay-480.webp 480w,/assets/img/blogs/unigram-toy-1/lr_decay-800.webp 800w,/assets/img/blogs/unigram-toy-1/lr_decay-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/lr_decay.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>How can we understand this? When the system has many degrees of freedom, and each update is normalized, many degrees of freedom may update in similar directions. Their combined effect can be too large, causing overshoot. In this case, reducing the learning rate is necessary to avoid overshooting.</p> <hr/> <h2 id="learning-rate-warmup">Learning rate warmup</h2> <p>Can our experiments shed light on the necessity of learning rate warmup? In the representation collapse section, we showed that large learning rates lead to more severe representation collapse—this is one possible explanation.</p> <p>Here we provide another observation. In <em>Analyzing &amp; Reducing Learning Rate Warmup</em> (<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/059445c2d5b3ef918079851628fef1d6-Paper-Conference.pdf">paper</a>), the authors found that without warmup, training descends faster initially but becomes slightly <em>sticky</em> around loss ≈ 7.5, after which it becomes slower than training with warmup:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 30%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/sticky_7d5-480.webp 480w,/assets/img/blogs/unigram-toy-1/sticky_7d5-800.webp 800w,/assets/img/blogs/unigram-toy-1/sticky_7d5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/sticky_7d5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>How should we interpret this value 7.5? Using our Unigram model, if we take \(V = 50304\) (NanoGPT) and \(\alpha = 1\) (Zipfian), we can compute the entropy to be 7.57. This may not be a coincidence.</p> <p>With a relatively large initial learning rate, the model may overly exploit Unigram token frequencies to reduce loss quickly, but because the steps are too large, it fails to capture more subtle structures in the data (Bigram, Trigram, or longer-range correlations). As a result, the model is attracted to a Unigram “saddle-point solution,” and escaping from it requires more time. Of course, this is only a hypothesis, and more complex toy models are needed to test it.</p> <p>Finally, from the perspective of neuron activations, there is another possible explanation: excessively large early learning rates may cause neurons to die more quickly. We discussed this possibility in a previous <a href="(/blog/2025/feature-learning-1/)">blog post</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1aVARfgLyf13arPbXw2tODPWacCgWMhBi?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026fine-unigram-toy-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/unigram-toy-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Fine-tuning with sparse updates? A toy teacher-student Setup</title><link href="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/" rel="alternate" type="text/html" title="Fine-tuning with sparse updates? A toy teacher-student Setup"/><published>2026-01-07T00:00:00+00:00</published><updated>2026-01-07T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>When we fine-tune a model, do we really need to update <strong>all</strong> layers?<br/> For language models, my mental picture is that the model can be divided along depth into three parts:</p> <ul> <li>early layers: mapping <strong>tokens → latent space</strong></li> <li>middle layers: performing <strong>reasoning in latent space</strong></li> <li>late layers: mapping <strong>latent space → tokens</strong></li> </ul> <p>As long as a task is still expressed in natural language, the mapping between language space and latent space should be largely fixed. Therefore, during fine-tuning, perhaps it is sufficient to fine-tune only the <strong>middle layers</strong>. How can we test this hypothesis?</p> <p>There are three possible approaches:</p> <ul> <li> <p><strong>Method 1</strong>: Fine-tune only the middle layers and check whether it works.<br/> However, to judge effectiveness, we need baselines (e.g., only fine-tuning early layers, or only fine-tuning late layers). The number of baselines grows exponentially, since each layer can either be frozen or fine-tuned.</p> </li> <li> <p><strong>Method 2</strong>: Train normally and examine how the magnitude of parameter updates varies across layers.<br/> The problem is that update magnitudes are highly influenced by the optimizer. For example, even if some layers have very small gradients (and arguably do not need to change), adaptive optimizers may still update them.</p> </li> <li> <p><strong>Method 3</strong>: Train normally, but impose sparsity on the updates (by adding L1 regularization), and then examine how update magnitudes vary across layers.<br/> This approach may avoid the issue in Method 2.</p> </li> </ul> <p>In this article, we use a <strong>teacher–student model</strong> to explore Methods 2 and 3. We find that Method 2 indeed suffers from the suspected issue, while Method 3 can effectively resolve it.</p> <hr/> <h2 id="teacherstudent-setup">Teacher–student setup</h2> <p>We adopt a teacher–student setup: the <strong>Teacher Network</strong> and the <strong>Student Network</strong> are MLPs with identical architectures.<br/> The teacher network is randomly initialized and generates input–output pairs. The student network is trained in a supervised manner to minimize the MSE loss of output predictions.</p> <p>Some layers of the student network are copied from the teacher network, while the remaining layers are randomly initialized. Motivated by the discussion above, we consider a <strong>three-layer MLP</strong>. At initialization:</p> <ul> <li>the first and third layers of the student network are identical to those of the teacher network,</li> <li>the second layer is randomly initialized (and thus different from the teacher network).</li> </ul> <p>During training, we additionally compute the <strong>L1 distance</strong> between the student network at training time and its initialization, and use this as a regularization term to encourage sparse updates. The strength of this regularization is denoted by \(\lambda\).</p> <p>The student network can be interpreted as a <strong>pretrained model</strong>, while the teacher network serves as a <strong>fine-tuned data generator</strong>. We ask whether the student network can “realize” that it actually does <strong>not</strong> need to update the first and third layers.</p> <p>We define two sets of observables to characterize the training dynamics:</p> <ul> <li>the distance between the student network during training and its initialization,</li> <li>the distance between the student network during training and the teacher network.</li> </ul> <p>Specifically, we compute the L1 distances for \(W_1, W_2, W_3, b_1, b_2, b_3\).</p> <hr/> <h3 id="normal-training-lambda--0">Normal training (\(\lambda = 0\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the update magnitude of \(W_2\) is actually <strong>smaller</strong> than that of \(W_1\) and \(W_3\). This shows that Method 2 is unreliable.</p> <p><strong>Additional observation.</strong> Each spike in the loss curve corresponds to a “step” in the weights. This phenomenon may be related to the <a href="https://arxiv.org/abs/2206.04817">slingshot mechanism</a>.</p> <hr/> <h3 id="sparse-updates-lambda--0001">Sparse updates (\(\lambda = 0.001\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0d001-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0d001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that only \(W_2\) has a significantly non-zero update, while the updates of all other weights eventually approach zero (although they may move substantially at the beginning, which is likely an artifact of Adam). This indicates that <strong>sparse updates can reveal the intrinsic structure of the data</strong>—namely, that only the middle layer needs to be fine-tuned.</p> <p><strong>Additional observation.</strong> The distance between the trained student network and the teacher network for \(W_2\) does not converge to zero (and is in fact quite large). This may be because \(W_2\), acting on sparse inputs, is effectively low-rank, so the optimal \(W_2\) is not unique.</p> <hr/> <h2 id="questions--ideas">Questions / Ideas</h2> <ul> <li> <p>Fine-tuning with sparse updates can serve as an <strong>interpretability tool</strong>—revealing layer-wise similarity between two datasets.<br/> Given a model pretrained on dataset A and fine-tuned on dataset B with sparse updates, it would be interesting to see which layers undergo large updates and which remain nearly unchanged.</p> </li> <li> <p>L1 regularization could be replaced by other metrics (e.g., the <strong>nuclear norm</strong> to encourage low-rank structure, to accompany LORA).</p> </li> <li> <p>Can this method be scaled up, and if so, how?</p> </li> <li> <p>Could this approach help with <strong>continual learning</strong>?<br/> Sparse updates may mitigate catastrophic forgetting.</p> </li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NsviyXmeuCppxr1wuL53Q9PzK_5BQWnB?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026fine-tuning-sparsity</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Fine-tuning with sparse updates? A toy teacher-student Setup}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/fine-tuning-sparsity/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Multi-Head Cross Entropy Loss</title><link href="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/" rel="alternate" type="text/html" title="Multi-Head Cross Entropy Loss"/><published>2026-01-06T00:00:00+00:00</published><updated>2026-01-06T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In LLMs, next-token prediction is performed by passing the final-layer hidden representation through the LM head to produce logits, thereby projecting into the token space. The definition of cross-entropy implies that only the correct token receives a low loss, while <em>any</em> other token incurs a high loss. For example, if the correct token is “red,” predicting either “green” or “rabbit” results in a large loss. However, it is clear that <em>green</em> is much closer to <em>red</em> than <em>rabbit</em> is, since both are adjectives and both denote colors.</p> <p>This motivates the following question: when designing the loss, should we take <em>similarity between tokens</em> into account? If token A and token B are similar in some respects, then if the model mistakenly predicts token B instead of token A, should it really be penalized as heavily as predicting a completely unrelated token C?</p> <p>This idea is still quite abstract, and there are likely many concrete ways to implement it. In this post, we explore one possibility: <strong>multi-head cross-entropy loss</strong>. Inspired by multi-head attention—where different heads attend to different semantic aspects—multi-head cross-entropy aims to capture token similarity from multiple semantic perspectives.</p> <hr/> <h2 id="definition">Definition</h2> <p><strong>Standard Cross-Entropy</strong></p> <p>Let the input representation to the LM head be \(x \in \mathbb{R}^D\) and the LM head weights be \(W \in \mathbb{R}^{V \times D}.\) The LM head output (i.e., the logits) is \(y = W x \in \mathbb{R}^V,\) where \(V\) is the vocabulary size. Standard cross-entropy computes the loss between $$y$ and the ground-truth label.</p> <p><strong>Multi-Head Cross-Entropy</strong></p> <p>We split \(x\) into \(H\) heads: \(x = [x_1; x_2; \cdots; x_H], x_i \in \mathbb{R}^{D/H}\). Similarly, we split \(W\) into \(H\) parts: \(W_i \equiv W[:,i\frac{D}{H}:(i+1)\frac{D}{H}] \in \mathbb{R}^{V\times D/H}.\) The logits for the \(i\)-th head are \(y_i = W_i x_i \in \mathbb{R}^V.\) After applying Softmax, we define a probability distribution \(p_{i,j} = \frac{\exp(y_{i,j})}{\sum_{j=1}^{V} \exp(y_{i,j})},\) which represents the probability of token \(j\) under the \(i\)-th semantic head.</p> <p>How should we aggregate different heads? Here we simply sum the probabilities (without a rigorous justification, and many other choices are possible): \(p_j = \sum_{i=1}^{H} p_{i,j}.\) The corresponding aggregated logit is \(y_j = \log(p_j).\)</p> <hr/> <h2 id="example-toy-language">Example: Toy Language</h2> <p><strong>Dataset</strong></p> <p>We assume each token has two features:</p> <ul> <li><strong>Part of speech</strong>: noun or verb</li> <li><strong>Topic</strong>: mathematics or sports</li> </ul> <p><strong>Grammar:</strong> nouns and verbs alternate: \(\text{noun} \rightarrow \text{verb} \rightarrow \text{noun} \rightarrow \text{verb} \rightarrow \cdots\)</p> <p><strong>Topic:</strong> within a sentence, the topic is consistent—either mathematics or sports.</p> <p>Thus, we have four classes of tokens:</p> <ul> <li><strong>A</strong>: math nouns</li> <li><strong>B</strong>: math verbs</li> <li><strong>C</strong>: sports nouns</li> <li><strong>D</strong>: sports verbs</li> </ul> <p>Each class contains 10 tokens (randomly chosen). Valid sentences look like:</p> \[[A8] \rightarrow [B2] \rightarrow [A5] \rightarrow [B6] \rightarrow [A1] \rightarrow [B7] \rightarrow \cdots\] \[[C2] \rightarrow [D1] \rightarrow [C6] \rightarrow [D2] \rightarrow [C10] \rightarrow [D3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>We consider a <strong>bi-gram model</strong>, which predicts the next token based on the previous token. The model uses an MLP, with weight tying between the embedding layer and the LM head.</p> <hr/> <h2 id="results">Results</h2> <p>We perform PCA on the embedding space and project each token embedding onto PC1/PC2. We find that as the number of heads increases:</p> <ol> <li>Clustering improves.</li> <li>The explained variance increases (i.e., the representation becomes more low-dimensional).</li> </ol> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/multi-head-cross-entropy/toy-language-480.webp 480w,/assets/img/blogs/multi-head-cross-entropy/toy-language-800.webp 800w,/assets/img/blogs/multi-head-cross-entropy/toy-language-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/multi-head-cross-entropy/toy-language.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="grokking">Grokking</h2> <p>I also randomly tried a grokking (modular addition) setup and found that multi-head cross-entropy does not significantly accelerate grokking. This was not particularly surprising—I did not have a strong prior for why it should help. This result is reasonable because, in modular addition, tokens essentially have only a single semantic dimension (numerical value), so there is no meaningful notion of multiple semantics for different heads to exploit.</p> <hr/> <h2 id="questions">Questions</h2> <ul> <li>Can we design a more suitable toy dataset that enables more mechanistic interpretability?</li> <li>Does it make sense to apply multi-head cross-entropy loss to large language models?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1QviExbkM6yCz_-T7UfSmCVL89ZGy7Gj_?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026multi-head-cross-entropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Multi-Head Cross Entropy Loss}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/multi-head-cross-entropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">What’s the difference – (physics of) AI, physics, math and interpretability</title><link href="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/" rel="alternate" type="text/html" title="What’s the difference – (physics of) AI, physics, math and interpretability"/><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/"><![CDATA[<p>In a previous <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>, I discussed how to conduct <em>Physics of AI</em> research. Some friends later asked: <strong>Can AI and physics really be fully analogous?</strong> And <strong>what is the difference between Physics of AI and interpretability?</strong></p> <p>This article will discuss two points:</p> <ol> <li><strong>AI and physics are not fully analogous</strong>, but that does not prevent us from borrowing methodologies from physics. In fact, <em>Physics of AI</em> is technically a <strong>simpler game than physics</strong>. Its main obstacles lie in <strong>publication culture</strong> (as discussed in a previous <a href="/blog/2026/physics-of-ai/">blog post</a>), not in the intrinsic difficulty of the subject.</li> <li><strong>(As I define) Physics of AI is not the same as (what people usually define) interpretability</strong>. The key reason is that I see some genuinely new research perspectives that deserve a new name. What that name is does not really matter. If, in the future, the community expands the definition of interpretability, I would also be happy to simply call it interpretability.</li> </ol> <hr/> <p>First, what’s the difference between physics and (physics of) AI?</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-480.webp 480w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-800.webp 800w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="physics">Physics</h2> <p>If we count from the time of Newton, physics has taken about <strong>400 years</strong> to develop to its current state. Compared with the development of AI, this is extremely slow. This slowness comes from both <strong>practical constraints</strong> and <strong>philosophical constraints</strong>.</p> <p><strong>Practical constraints — physics research has strong directionality</strong></p> <ul> <li><strong>Experiment-driven</strong>: theories emerge from experimental observations.</li> <li><strong>“Human-centered” scales</strong>: starting from scales directly accessible to human perception, then extending in both directions—toward the microscopic (atoms, quarks) and the macroscopic (celestial bodies, the universe).</li> </ul> <p>Both directions are constrained by experimental and observational bottlenecks. It takes a long time to build microscopes and telescopes in the physical world.</p> <p><strong>Philosophical constraints</strong></p> <p>We do not actually know how the “creator” (physical laws) truly runs the universe. We can only infer laws from phenomena.<br/> <em>All models are wrong, but some are useful.</em></p> <p>Even the most committed reductionists are troubled by two questions:</p> <ul> <li><strong>Minimum and maximum scales</strong>: Are fundamental particles truly fundamental, e.g., can quarks or electrons be further divided? What lies beyond the universe?</li> <li><strong>Cross-scale emergence</strong>: How do phenomena at different levels “emerge” from one another?</li> </ul> <hr/> <h2 id="physics-of-ai">(Physics of) AI</h2> <p>For AI, <strong>none of the above constraints really exist</strong>.</p> <p><strong>No practical constraints</strong></p> <p>Apart from computational limits that prevent us from reaching arbitrarily large scales, we have <strong>no observational limitations</strong>. In principle, we can study the evolution of <strong>all weights and all neurons</strong>.</p> <p><strong>No philosophical constraints</strong></p> <p>We fully understand the “fundamental particles” of AI (neurons, weights, gradient descent), and we know the “boundary of the universe” (the entire neural network). <strong>We are the creator.</strong></p> <p>In principle, we can observe phenomena at any level at any time. There is no inherent directionality and no “human-centered” scale. AI systems are also <strong>closed systems</strong>—we know exactly how they evolve because we train them ourselves. Therefore, in principle, it must always be possible to explain large-scale phenomena using small-scale mechanisms, even if such explanations are not always useful.</p> <p>Physics is different: the continual discovery of “new physics” shows that its boundaries are still being broken.</p> <hr/> <h2 id="mathematics">Mathematics</h2> <p>At this point, it is tempting to equate the physics of AI with mathematics, since both have clearly defined “fundamental particles”—called <strong>axioms</strong> in mathematics. But mathematics is clearly more difficult, for several reasons:</p> <ul> <li><strong>Mathematics is also largely directional</strong>: starting from axioms and deriving results is a process from “microscopic” to “macroscopic.” In contrast, AI phenomenology can be studied simultaneously at multiple levels.</li> <li><strong>Symbolic spaces make the definition of scales/levels and observations difficult</strong>: although backward reasoning (induction, abduction) exists in mathematics, it is often hard to define what “intermediate reasoning” even means, because symbolic spaces can be infinite. Neural networks (current AI systems), by contrast, have bounded topologies — no smaller than a neuron and no larger than the entire network.</li> <li><strong>Cultural emphasis on rigor</strong>: mathematical rigor often comes at the cost of faithfulness to reality. AI research, in contrast, emphasizes practicality and is far less obsessed with rigor. Physics lies somewhere in between mathematics and AI in terms of rigor.</li> </ul> <hr/> <h2 id="does-the-physics-of-ai-have-no-difficulties-then">Does the Physics of AI Have No Difficulties, Then?</h2> <p>Of course not.</p> <p>Philosophically, the physics of AI does not suffer from reductionism problems (we know the minimum and maximum scales, and cross-scale explanations are possible in principle). However, it may still face <strong>technical challenges</strong>. Even so, I remain optimistic. Two major technical questions are:</p> <ul> <li><strong>How do we define levels and corresponding observations?</strong> Neurons, weights, and representations are clear, but how should we define circuits or modules? Once levels are defined, what observables should we introduce, and what phenomena should we observe? I gave partial answers in this <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>.</li> <li><strong>How do we characterize the connections (emergence) between levels?</strong> In principle, because AI systems are closed, lower-level phenomena should always be able to explain higher-level ones. In practice, however, finding explanations that are <em>simple</em> and <em>useful</em> is still a non-trivial problem. To be honest, I still have no clue how to answer this question. But more hands-on experiments will give the answer.</li> </ul> <hr/> <h2 id="response-to-critiques-from-the-scaling-camp">Response to Critiques from the Scaling Camp</h2> <p>The issue of emergence is the main line of attack from the <strong>scaling camp</strong> against the <strong>research camp</strong>:<br/> <em>How can phenomena observed in small models transfer to large models?</em></p> <p>Below is my response/attitude/belief:</p> <p><strong>Philosophical level</strong></p> <ul> <li>In natural science, analogies of emergence do not straightforwardly apply here (as discussed above). The ineffectiveness of reductionism in explaining emergence in nature (e.g., <a href="https://arxiv.org/abs/2503.01800">Hilbert’s sixth problem</a> is a hard problem) does not imply the same ineffectiveness in AI. Moreover, I personally do not like the word “emergence” since it seems to imply something mysterious (which might be appropriate for natural science, but not AI).</li> <li>Believe in <strong>shared explanations</strong>. Phenomenon A in small models and phenomenon B in large models may originate from the same cause. The absence of A in large models does not invalidate the value of studying it.</li> </ul> <p><strong>Methodological level</strong></p> <ul> <li><strong>Be pragmatic</strong>. There are many concrete things we can do that have not yet been done. Only by doing them can we know whether they are useful.</li> <li>Be specific. Cross-scale phenomena usually fall into three categories: <ul> <li><strong>Expansion</strong>: phenomenon A in small models becomes more pronounced in large models.</li> <li><strong>Shrinkage</strong>: phenomenon A in small models becomes less visible in large models.</li> <li><strong>Transformation</strong>: phenomenon A in small models turns into phenomenon B in large models.</li> </ul> <p>The scaling critique focuses on <em>shrinkage</em>. We are betting on <em>expansion</em> and <em>transformation</em>. Even if everything turns out to be shrinkage, our efforts are still not meaningless—at that point, I would more firmly side with the scaling camp.</p> </li> <li>We must acknowledge that academia cannot afford to train the largest models. We should call on large-model companies to open-source models—or at least open-source <em>phenomena</em>. Of course, academia must first identify interesting observables in toy and small models, so that industry knows <em>what</em> to observe.</li> </ul> <hr/> <h2 id="interpretability">Interpretability</h2> <p>In a broad sense, interpretability includes everything—one could even say that <em>physics itself is about interpreting the universe</em>. In that sense, the physics of AI certainly belongs to interpretability.</p> <p>However, what people usually mean by interpretability refers narrowly to <strong>interpretability research in AI</strong>, and their understanding is constrained by past work:</p> <ul> <li>Some believe interpretability is just storytelling—pleasant but useless.</li> <li>Others believe interpretability is mathematics—rigorous but useless.</li> <li>The characterizations are often too coarse, making it unclear whether we are studying causality or correlation. This is precisely what <em>mechanistic interpretability</em> seeks to break away from.</li> <li>There is a lack of methodology: <em>at what level does an explanation count as complete?</em></li> </ul> <p>Physics of AI differs from interpretability in all aspects:</p> <ul> <li>Starts with <strong>phenomenology</strong>, emphasizing faithful recording of phenomena and reducing the “storytelling” aspect (see previous <a href="/blog/2025/physics-of-ai/">blog post</a>).</li> <li>Is <strong>not mathematics</strong> (as discussed above). But we can adopt mathematical tools when they are useful.</li> <li>Characterizes phenomena at <strong>multiple levels</strong>, although I personally perfer startting from smallest “toy” models. You might have noticed that my recent technical blogs all study very simple models. The toy models already demonstrate very rich phenomena and I believe that the phenomena in toy models can transform (although maybe not directly transfer) to phenomena in larger models.</li> <li>Gradually builds a <strong>methodology</strong> to describe the connections between phenomena across levels.</li> </ul> <p>Therefore, we deserve a <strong>new name</strong> for what we are doing. <em>Physics of AI</em> is the name I chose.<br/> Again, the name itself is not important. What matters is that it signals <strong>new territory and new treasures</strong>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{What's the difference -- (physics of) AI, physics, math and interpretability}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/ai-physics-interpretability/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[In a previous blog post, I discussed how to conduct Physics of AI research. Some friends later asked: Can AI and physics really be fully analogous? And what is the difference between Physics of AI and interpretability?]]></summary></entry><entry><title type="html">Representation anisotropy from nonlinear functions</title><link href="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/" rel="alternate" type="text/html" title="Representation anisotropy from nonlinear functions"/><published>2026-01-04T00:00:00+00:00</published><updated>2026-01-04T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/activation-anisotropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Neural representations in deep neural networks are usually anisotrophic. This article aims to understand how nonlinear activation functions lead to representation anisotropy.</p> <hr/> <h2 id="toy-model">Toy model</h2> <p>We consider an LNL (linear-nonlinear) model, which is basically a two-layer MLP excluding the down projection layer: \(h_{\rm pre} = Wx, h_{\rm post} = \sigma(h_{\rm pre})\) where \(\sigma(\cdot)\) is the activation function. We set \(x\in\mathbb{R}^d\) to be isotropic – \(N\) samples are drawn from standard Gaussian distribution. \(W\in\mathbb{R}^{d\times D}\) is randomly initialized. We set \(d=100, D=400, N=10000\). We won’t train the model, and will only be interested in characterizing the anisotropy of \(h_{\rm post}\). We can stack \(h_{\rm post}\) of all \(N\) samples into a matrix \(H\in\mathbb{R}^{N\times D}\).</p> <p>To measure anisotropy, we apply singular value decomposition (SVD) to \(H\) to obtain singular values. The singular value distribution characterizes anisotropy of representations. We normalize singular values so that they sum up to 1.</p> <hr/> <h2 id="observation-1-relu-activation-leads-to-massive-sigma_1-while-linear-activation-does-not">Observation 1: ReLU activation leads to massive \(\sigma_1\), while linear activation does not.</h2> <p>ReLU function make \(\sigma_1\) stand out, but linear function does not:</p> <p>ReLU and linear:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/relu-linear-480.webp 480w,/assets/img/blogs/activation-anisotropy/relu-linear-800.webp 800w,/assets/img/blogs/activation-anisotropy/relu-linear-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/relu-linear.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SiLU is qualitatively similar to ReLU:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 45%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/silu-480.webp 480w,/assets/img/blogs/activation-anisotropy/silu-800.webp 800w,/assets/img/blogs/activation-anisotropy/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This begs the question: why? Before answering why, we first do an interpolation experiment – leaky relu interpolates between ReLU (\(p=0\)) and linear (\(p=1\)). We will use the ratio \(\sigma_1/\sigma_2\) to measure how “standing out’’ \(\sigma_1\) is.</p> <hr/> <h2 id="observation-2-first-order-phase-transition-of-leaky-relu">Observation 2: First order phase transition of Leaky ReLU</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/leaky_relu-480.webp 480w,/assets/img/blogs/activation-anisotropy/leaky_relu-800.webp 800w,/assets/img/blogs/activation-anisotropy/leaky_relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/leaky_relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Although I expect that \(\sigma_1/\sigma_2\) would smoothly interpolate between two extremes, there is a first order phase transition around \(p_c\approx 0.8\). For \(p&gt;p_c\), the ratio remains close to 1. For \(p&lt;p_c\), the ratio grows exponentially (the y axis is log-scale) as \(p\) decreases. I don’t understand why.</p> <hr/> <h2 id="explanation-relu-polarizes-activations">Explanation: ReLU polarizes activations</h2> <p>The intuition: because ReLU maps negative values to zero, this creates a notion of polarity. Indeed, we find the first eigenvector to be \([1, 1, 1, \cdots, 1]\).</p> <p>To better see this, we even drop the linear matrix and only keep the nonlinearity. For isotropic pre-activations, the spectrum for the post-activations look like this:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/isotropic-preact-480.webp 480w,/assets/img/blogs/activation-anisotropy/isotropic-preact-800.webp 800w,/assets/img/blogs/activation-anisotropy/isotropic-preact-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/isotropic-preact.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where \(\sigma_1\) stands out, while all rest singular values are small and almost the same.</p> <p>In hindsight, this explantion is almost trivial, but the consequence (large \(\sigma_1\)) is something I don’t think I had appreciated enough.</p> <hr/> <h2 id="observation-3-tanh-doesnt-make-sigma_1-stand-out">Observation 3: Tanh doesn’t make \(\sigma_1\) stand out</h2> <p>Based on the above explantion, non-polarized activation functions (like Tanh) don’t make \(\sigma_1\) stand out, which is indeed the case:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/tanh-480.webp 480w,/assets/img/blogs/activation-anisotropy/tanh-800.webp 800w,/assets/img/blogs/activation-anisotropy/tanh-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/tanh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="question">Question</h2> <ul> <li>Is this a feature or a bug? Right now I tend to think this is a bug. Anisotropy could be the reason for training inefficiency.</li> <li>If this is a bug, how can we avoid this? Can we simply try to find better activation functions, or do we need extra processing?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1k-vibPZIgB9e5--i87_zG5IfLDH_elLQ?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026activation-anisotropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Representation anisotropy from nonlinear functions}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/activation-anisotropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Training dynamics of A Single ReLU Neuron</title><link href="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/" rel="alternate" type="text/html" title="Training dynamics of A Single ReLU Neuron"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/single-relu-neuron</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a previous <a href="/blog/2026/feature-learning-1/">blog post</a>, we studied feature learning in shallow, wide MLPs. In this article, we consider an even simpler setting: an MLP with only <strong>one hidden layer</strong>, <strong>a single ReLU neuron</strong>, and a <strong>self-generated target function</strong> (a teacher network).</p> <p>In this setting, we know that there exists a set of weights for which the loss is exactly zero. But will gradient-based optimization run into difficulties? For example, if the neuron is initialized to be overly <strong>active</strong> (positive pre-activation for all inputs), or overly <strong>inactive</strong> (negative pre-activation for all inputs), will optimization fail? Even if the initialization is reasonable, can the training dynamics drive the neuron into a bad state (too active or too inactive)?</p> <p>For convenience, we define three states of a neuron:</p> <ul> <li><strong>Hyperactive</strong>: activated for all inputs (pre-activation always positive)</li> <li><strong>Inactive / Dead</strong>: never activated (pre-activation always negative)</li> <li><strong>Balanced</strong>: activated for some inputs and inactive for others</li> </ul> <p>Although this setup is extremely simple, our ultimate goal is to gain insights relevant to LLM training. One intuition is that many tricks used in LLM training—such as LR warmup, LR decay, weight decay, and MoE balancing—may implicitly control neuron activity levels, thereby influencing feature learning.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>An MLP with one hidden layer and one neuron can be written as (for simplicity, the input is also 1D):</p> \[f(x; w_1, b_1, w_2, b_2) = w_2\sigma(w_1x+b_1)+b_2, \quad \sigma(x) ={\rm ReLU}(x) \equiv {\rm max}(0,x)\] <p>For the teacher network, we set \(w_1^T = w_2^T = 1, \quad b_1^T = b_2^T = 0\), and so \(f(x)\equiv {\rm ReLU}(x)\). We take the input domain to be \(x \in [-1,1].\)</p> <p>For the student network, we initialize \(w_1^S = w_2^S = 1,\) and focus on varying the initializations of \(b_1^S\) and \(b_2^S\). Training uses MSE loss and the Adam optimizer (default LR = 0.01). Below, we only discuss the student’s weights, so we omit the superscript \(S\).</p> <hr/> <h2 id="observation-0-large-b_1-leads-to-local-minima">Observation 0: Large \(|b_1|\) Leads to Local Minima</h2> <p>We fix the initialization \(b_2 = 0\).</p> <ul> <li>When \(b_1 &gt; 1\), the neuron is initialized in the <strong>hyperactive</strong> state, and the loss gets stuck around 0.02 (corresponding to approximating ReLU with linear regression).</li> <li>When \(b_1 &lt; -1\), the neuron is initialized in the <strong>dead</strong> state, and the loss gets stuck around 0.1 (corresponding to approximating ReLU with a constant function).</li> </ul> <p>This may help explain why some initialization schemes (e.g., Kaiming initialization) set the bias to zero.</p> <hr/> <h2 id="observation-1-large-b_2-can-kill-the-neuron">Observation 1: Large \(|b_2|\) Can Kill the Neuron</h2> <p>We fix \(b_1 = 0\), so the neuron is <strong>balanced</strong> at initialization. Is everything fine then? Not quite. We find that when \(b_2 = -1.1\), the loss can go to zero, but when \(b_2 = -1.2\), the loss gets stuck around 0.02 (again corresponding to approximating ReLU with linear regression).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/loss-480.webp 480w,/assets/img/blogs/single-relu-neuron/loss-800.webp 800w,/assets/img/blogs/single-relu-neuron/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By closely examining the training dynamics, we find that the turning point \(x_t \equiv - b_1 / w_1\) moves left during training (starting from 0). When it moves past -1, the neuron transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>The local minimum for \(b_2 = -1.2\) corresponds to the linear regression solution (the neuron is in the hyperactive state):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/large_b2_init-480.webp 480w,/assets/img/blogs/single-relu-neuron/large_b2_init-800.webp 800w,/assets/img/blogs/single-relu-neuron/large_b2_init-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/large_b2_init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Conversely, if we increase the bias of the target function, the neuron also transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>This observation connects to two empirical practices:</p> <ul> <li><strong>Model bias</strong>: In some LLM training setups, bias terms are disabled. The observation above suggests that bias dynamics can drive neurons away from the balanced state.</li> <li><strong>Data bias</strong>: Whitening inputs and normalizing intermediate representations are common practices.</li> </ul> <hr/> <h2 id="observation-2-too-large-a-learning-rate-can-also-kill-the-neuron">Observation 2: Too Large a Learning Rate Can Also Kill the Neuron</h2> <p>We fix \(b_1 = 0\) and \(b_2 = -1\).</p> <ul> <li>When LR = 0.2, the model’s loss can be optimized to zero.</li> <li>When LR = 0.4, the loss only reaches 0.1 (again corresponding to fitting ReLU with a constant function), because the neuron becomes <strong>dead</strong>.</li> </ul> <p>This may be related to LR warmup in LLM training: if the initial learning rate is too large, the loss may decrease quickly, but at the cost of killing some neurons. Once dead, these neurons are hard (or impossible) to bring back to a balanced state.</p> <hr/> <h2 id="observation-3-silu-eventually-recovers-but-gets-stuck-at-a-saddle-point-for-a-long-time">Observation 3: SiLU Eventually Recovers, but Gets Stuck at a Saddle Point for a Long Time</h2> <p>For SiLU, we observe that the loss can eventually reach zero (up to machine precision), but training gets stuck at saddle points for very long time, reducing training efficiency.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/silu-480.webp 480w,/assets/img/blogs/single-relu-neuron/silu-800.webp 800w,/assets/img/blogs/single-relu-neuron/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="takeaway">Takeaway</h2> <p><strong>Irreversibility</strong>: once a neuron leaves the balanced state, it is very hard to return. Bias plays a crucial role. This might be why we have various ugly tricks for LLM (learning rate schedule, weight decay, etc).</p> <hr/> <h2 id="questionsideas">Questions/Ideas</h2> <p><strong>Question 1: How can we better control bias?</strong></p> <ul> <li>Can we analytically compute the bias instead of learning it via gradient descent?</li> <li>Can we use a different parameterization to better control neuron activation? For example, \(wx + b \;\to\; w(x + b'),\) where \(b'\) directly controls neuron activation (assuming the input distribution is known). This may allow us to more directly control neuron activity, e.g., by applying weight decay to \(b'\) or explicitly enforcing balancing.</li> </ul> <p><strong>Question 2: Which neurons have better learning dynamics?</strong></p> <ul> <li>SiLU is better than ReLU. Is there something better, gating?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1gSrKOVfEtVNTa7ZCUOpqOI1uTXm0SL9s?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026single-relu-neuron</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Training dynamics of A Single ReLU Neuron}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/single-relu-neuron/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics of AI – How to Begin</title><link href="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/" rel="alternate" type="text/html" title="Physics of AI – How to Begin"/><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/"><![CDATA[<div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-480.webp 480w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-800.webp 800w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In a recent <a href="/blog/2026/physics-of-ai/">blog post</a>, we mentioned that <em>Physics of AI</em> requires a shift in mindset.<br/> This post will explain, from a slightly more technical perspective, <strong>how</strong> to do <em>Physics of AI</em>—I will present a <strong>modular framework</strong>.</p> <p>On the one hand, we need a framework. With a basic framework, the community can share a common language for communication. On the other hand, we must not be constrained by the framework itself. <em>Physics of AI</em> genuinely requires novel, out-of-the-box ideas.</p> <hr/> <h2 id="what-is-physics-of-ai">What Is Physics of AI?</h2> <p><em>Physics of AI</em>, as the name suggests, means studying AI in the same way we study physics.<br/> It aims to answer the following questions (so simple that they may sound trivial):</p> <blockquote> <p><strong>What models, on what data, exhibit what phenomena?</strong><br/> <em>(Bonus: why?)</em></p> </blockquote> <p>There are three core elements here: <strong>models, data, and phenomena</strong>.<br/> To emphasize data and phenomena (which are often overlooked by the community), I deliberately fold many details—such as optimizers and loss functions—into the notion of the <em>model</em>.</p> <p>Below, I elaborate on these three elements.</p> <hr/> <h2 id="models">Models</h2> <ul> <li> <p><strong>Architecture</strong><br/> Examples include MLPs, RNNs, Transformers, DINO, Mamba, KANs, etc.<br/> Architectures consist of various layers such as MLP layers, attention layers, convolutions, etc.</p> </li> <li> <p><strong>Training</strong></p> <ul> <li><strong>Paradigms</strong><br/> Supervised learning, unsupervised learning, self-supervised learning, representation learning, etc. In the LLM era, we further distinguish <em>pre-training</em>, <em>mid-training</em>, and <em>post-training</em>.</li> <li><strong>Optimizers</strong><br/> SGD, Adam, Muon, etc.</li> <li><strong>Loss functions / Rewards</strong><br/> MSE, cross-entropy, accuracy, diffusion loss, etc.<br/> These are often coupled with the training paradigm—for example, contrastive loss in representation learning.</li> <li><strong>Regularization</strong><br/> L2 weight decay, dropout, KL divergence, and so on.</li> </ul> </li> </ul> <hr/> <h2 id="data">Data</h2> <ul> <li><strong>Synthetic (toy) data</strong><br/> We know the data generation process, and have full control over it.</li> <li><strong>Real data</strong><br/> We do not fully know the data generation process.</li> </ul> <hr/> <h2 id="phenomena--observables">Phenomena / Observables</h2> <p>Beyond commonly tracked quantities such as loss curves and accuracy, we also care about:</p> <ul> <li> <p><strong>Biology-like phenomena (often data-dependent)</strong><br/> For example, understanding whether (and how) representations, computations, algorithms, or structures emerge during training.<br/> A well-known example is the <em>induction head</em>.<br/> The field of <em>mechanistic interpretability</em> is closely related here.</p> </li> <li> <p><strong>Physics-like phenomena (often data-independent)</strong><br/> For example, weight matrix spectra, activation subspaces, attention patterns, and so on.</p> </li> </ul> <hr/> <h2 id="what-should-we-measure">What Should We Measure?</h2> <p>A central and difficult question in AI phenomenology is: <strong>what exactly should we measure?</strong><br/> Of course, this depends on the phenomena we want to observe. But in order to observe phenomena, we must first know <em>which quantities to measure</em>—that is, <em>which observables correspond to which phenomena</em>. This creates a classic <strong>chicken-and-egg problem</strong>.</p> <p>We must start somewhere. Based on my experience, common starting points include:</p> <ul> <li> <p><strong>Making abstract ideas concrete</strong>, which leads to observables.<br/> For example, I may care about <em>feature learning</em>, but features are high-dimensional. I therefore define observables that capture certain aspects of feature learning. In this <a href="/blog/2026/feature-learning-1/">blog</a>, I use the <em>number of non-linear neurons</em> as one such observable.</p> </li> <li> <p><strong>Starting from a known phenomenon and discovering new ones.</strong><br/> For instance, some people study <em>grokking</em> and, during reproduction, discover new phenomena such as <em>loss spikes</em>, which then become a subject of study themselves. This leads to <a href="https://arxiv.org/abs/2206.04817">Apple’s slingshot paper</a>.</p> </li> <li> <p><strong>Starting from real data</strong>, and logging common observables during training.<br/> What counts as “common” requires accumulation of experience:<br/> (i) seeing what others in the field measure, and<br/> (ii) learning which observables were useful in your own past experiments.</p> </li> <li> <p><strong>Starting from toy data</strong>, imagining how the model <em>should</em> behave (prompting yourself: <em>“If you were an AI model, what would you do?”</em>), and then designing observables to test whether the model actually behaves that way.</p> </li> </ul> <p>In this way, the original chicken-and-egg problem becomes a <strong>spiral of ascent</strong>:<br/> more phenomena inspire new observables, and new observables lead to the discovery of more phenomena.</p> <hr/> <h2 id="why-does-this-phenomenon-occur">Why Does This Phenomenon Occur?</h2> <p>My personal thinking habit is to first ask:<br/> <strong>Is this phenomenon caused by the data, or by the model?</strong><br/> Here, “model” also includes random seeds: <em>how sensitive is the phenomenon to the random seed?</em></p> <p>The fastest way to get answers is through experiments—changing seeds, changing models, changing data, and checking whether the phenomenon persists. Once these basic experimental results are in hand, we can begin to form <strong>hypotheses</strong>. Hypotheses predict new observables and phenomena, which are then tested by further experiments.</p> <p>This forms a discovery loop:</p> <blockquote> <p><strong>Run experiments → observe phenomena → form hypotheses → design the next experiments</strong></p> </blockquote> <hr/> <h2 id="what-does-it-mean-to-understand-something">What Does It Mean to “Understand” Something?</h2> <p>There are many layers to asking “why” or claiming understanding. How much understanding is <em>enough</em>?</p> <p>My personal criterion is: <strong>when my predictions are broadly consistent with new experimental results, I consider that a success</strong>.<br/> The meaning of “broadly” is subtle. When I am lazy or tolerant, matching trends may be enough. When I am diligent or obsessive, I may want quantitative agreement as well.</p> <p>For example, the discovery of the \(t\sim \gamma^{-1}\) (\(t\): grokking time, \(\gamma\):weight decay) in <a href="https://arxiv.org/abs/2210.01117">our Omnigrok paper</a> came from pushing this obsession further. How far one pushes before stopping largely depends on the researcher’s personality, beliefs, and scientific taste. If we post our findings publicly, the whole community can push the frontier together (we’re all “blind men touching elephants”; different perspectives are always helpful).</p> <hr/> <h2 id="what-makes-a-model-good">What Makes a Model “Good”?</h2> <p>A common debate in the field is: <em>Which model is better?</em><br/> Is this judgment objective or subjective?</p> <p>The <strong>No Free Lunch theorem</strong> already gives the answer: no model is universally better than another across all data.<br/> The real philosophical question is therefore:</p> <blockquote> <p><strong>What kind of data is the world closest to?</strong></p> </blockquote> <p><em>Physics of AI</em> can objectively answer:<br/> <strong>what data, under what models, exhibits what phenomena</strong>—this part is fully objective and uncontested.</p> <p>Once we have a notion of <strong>which phenomena are desirable</strong> (possibly subjective, possibly objective), we can answer:<br/> <strong>what data requires what models</strong>.</p> <p>Going one step further, once we have a notion of <strong>what kind of data the world is closest to</strong> (again, possibly subjective or objective), we can answer:<br/> <strong>which models are good</strong>.</p> <p>The current state of AI development often skips the purely objective stage and jumps directly to subjective claims. As a result, discussions often devolve into “you say yours, I say mine,” lacking the <strong>shared knowledge</strong> required for meaningful communication.</p> <p>The goal of <em>Physics of AI</em> is precisely to construct this shared knowledge.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of AI – How to Begin}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-of-ai-recipe/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Physics of Feature Learning 1 – A Perspective from Nonlinearity</title><link href="https://kindxiaoming.github.io/blog/2026/feature-learning-1/" rel="alternate" type="text/html" title="Physics of Feature Learning 1 – A Perspective from Nonlinearity"/><published>2026-01-01T00:00:00+00:00</published><updated>2026-01-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/feature-learning-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/feature-learning-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Please Read our latest post <a href="/blog/2026/physics-of-ai/">Physics of AI Requires Mindset Shifts</a> for general philosophy.</p> <p>While reading <a href="https://arxiv.org/abs/2509.21519">Yuandong Tian’s work on explaining grokking through feature learning</a>, I found the proposed <em>three-stage dynamics of feature learning</em> particularly intriguing. However, grokking itself has some special characteristics (e.g., the modular addition dataset), which led me to wonder whether feature learning could be studied in a more <em>standard</em> setting. This motivated the low-dimensional regression example explored below.</p> <p>Rather than starting from mathematical derivations, I decided to go straight to experiments. Beyond plotting the loss, we also wanted to define observables that characterize <em>features</em>. One of the most basic observables is the <strong>number of nonlinear neurons</strong>. This measure does not care about the specific form of the features, only whether they are nonlinear.</p> <p>To define neuron activation cleanly, we use <strong>ReLU</strong> activations. A neuron is considered <em>nonlinear</em> if it is activated for some inputs and inactive for others.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>We consider the following regression function:</p> \[y = \sum_{i=1}^{d} \sin^2(f x_i), \quad d = 10,\; f = 10.\] <p>We use an MLP with 10-dimensional input, 100 neurons in the first hidden layer, 100 neurons in the second hidden layer, and a single output. We denote the architecture as ([10, 100, 100, 1]). Inputs are sampled from a standard Gaussian distribution, with a total of 500 samples. The training objective is MSE loss, optimized using <strong>Adam</strong> (learning rate \(10^{-3}\), full batch).</p> <hr/> <h2 id="observation-four-phases-during-training">Observation: Four Phases During Training</h2> <p>We find that all 100 neurons in the first hidden layer remain nonlinear throughout training. In contrast, the number of nonlinear neurons in the second hidden layer exhibits <strong>non-trivial dynamics</strong>. Therefore, all plots below focus on the evolution of nonlinear neurons in the second hidden layer.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss-480.webp 480w,/assets/img/blogs/feature-learning-1/loss-800.webp 800w,/assets/img/blogs/feature-learning-1/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Empirically, the training dynamics appear to consist of four phases:</p> <ul> <li> <p><strong>Phase I (first ~50 steps)</strong><br/> The loss decreases rapidly, while the number of active neurons drops sharply.<br/> <em>Hypothesis</em>: the model is removing irrelevant features.</p> </li> <li> <p><strong>Phase II (50–500 steps)</strong><br/> The loss plateaus around 1, and the number of active neurons remains roughly constant (around 14).<br/> <em>Hypothesis</em>: the model is stuck near a saddle point, leading to slow learning.</p> </li> <li> <p><strong>Phase III (500–1500 steps)</strong><br/> The loss decreases slowly, while the number of active neurons increases.<br/> <em>Hypothesis</em>: this corresponds to genuine feature learning.</p> </li> <li> <p><strong>Phase IV (after ~1500 steps)</strong><br/> The loss decreases rapidly (exponential convergence), while the number of active neurons stays constant.<br/> <em>Hypothesis</em>: the model is fine-tuning features, or fine-tuning the final linear layer.</p> </li> </ul> <p>There are many interesting questions one could study here.</p> <hr/> <h2 id="curiosity-driven-questions">Curiosity-Driven Questions</h2> <ul> <li> <p>Why does Phase I remove so many features?<br/> <em>Speculation</em>: is this related to optimization tricks in LLMs? For example, do weight decay and learning-rate warmup mainly serve to prevent excessive feature removal in this phase?</p> </li> <li> <p>Phase II appears “sticky.” How should we better understand the meaning of this solution?<br/> <em>Speculation</em>: is the loss plateau simply the result of linear regression?</p> </li> <li> <p>How do nonlinear features emerge in Phase III?</p> </li> <li> <p>What determines the exponential convergence rate in Phase IV?<br/> <em>Speculation</em>: once features are learned, does the problem effectively reduce to linear regression, which we know converges exponentially?</p> </li> </ul> <hr/> <h2 id="a-linear-regression-perspective">A Linear Regression Perspective</h2> <p>To better understand the dynamics, we analyze the model from a linear regression viewpoint. We compute three linear regression baselines:</p> \[X \to Y,\quad f_1 \to Y,\quad f_2 \to Y,\] <p>where \(f_1\) and \(f_2\) denote the features from the first and second hidden layers, respectively, which evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss_linear_regression-480.webp 480w,/assets/img/blogs/feature-learning-1/loss_linear_regression-800.webp 800w,/assets/img/blogs/feature-learning-1/loss_linear_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss_linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The loss plateau around 1 corresponds closely to the solution of linear regression.</li> <li>In the late stage of training, the blue and orange curves nearly coincide, indicating that the final-layer weights are essentially optimal at every moment, while the features are still being fine-tuned. As a result, the loss continues to decrease.</li> </ul> <p>This is not standard linear regression (where features are fixed and weights evolve), but rather a <strong>dual version</strong>: features evolve while weights remain near their instantaneous optimum. Interestingly, this still leads to exponential decay. A natural next step is to understand the convergence rate quantitatively.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/17gyczGrRuUAuWYuD_NFB7ZlCAA37p1cK?usp=sharing">here</a>.</p> <p>Observing interesting phenomena and asking good questions is already half the battle. We are still working toward explaining more of these effects. If you have any idea regarding how to understand some of the phenomena, please email me at lzmsldmjxm@gmail.com</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026feature-learning-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of Feature Learning 1 -- A Perspective from Nonlinearity}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/feature-learning-1/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu (January 2026). Physics of Feature Learning 1 – A Perspective from Nonlinearity. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2026/feature-learning-1/</p>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）]]></summary></entry></feed>