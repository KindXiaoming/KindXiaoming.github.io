<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-01T10:01:44+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Diffusion 2 – Visualizing flow matching, temporal dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-2/" rel="alternate" type="text/html" title="Diffusion 2 – Visualizing flow matching, temporal dynamics"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reproducing Tianhong and Kaiming’s JIT paper, <a href="https://arxiv.org/pdf/2511.13720">“Back to Basics: Let Denoising Generative Models Denoise”</a>. The first step is to implement the v-prediction baseline on the spiral dataset (Figure 2 in the paper). Reproducing the baseline should be straightforward, but I ended up getting stuck for quite a while. Eventually, I realized that how \(t\) is sampled during training is crucial.</p> <p>Specifically, \(t\) is sampled from a logit-normal distribution: \(t = \frac{1}{1 + e^{-X}}, \quad X \sim \mathcal{N}(\mu, \sigma^2).\) However, the main paper does not clearly specify how \((\mu, \sigma)\) are chosen. I therefore experimented with different values to understand their effects. Note that \(t = 0\) corresponds to pure noise, while \(t = 1\) corresponds to clean data.</p> <hr/> <h2 id="tweaking-mu-and-sigma">Tweaking \(\mu\) and \(\sigma\)</h2> <p>We first fix \(\sigma = 2\) to ensure sufficient coverage of the time range, and sweep over \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/mu-480.webp 480w,/assets/img/blogs/diffusion-2/mu-800.webp 800w,/assets/img/blogs/diffusion-2/mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that \(\mu = 2\) yields the best generation results. We then fix \(\mu = 2\) and sweep over \(\sigma\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma-480.webp 480w,/assets/img/blogs/diffusion-2/sigma-800.webp 800w,/assets/img/blogs/diffusion-2/sigma-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Somewhat surprisingly, \(\sigma = 0\) can still lead to decent generation quality. We therefore fix \(\sigma = 0\) and vary \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma_0_mu-480.webp 480w,/assets/img/blogs/diffusion-2/sigma_0_mu-800.webp 800w,/assets/img/blogs/diffusion-2/sigma_0_mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma_0_mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma_0_mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma_0_mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To better understand why \(\sigma = 0\) can still produce good samples, note that \(\sigma = 0\) means training only ever sees a single time \(t = \frac{1}{1 + e^{-\mu}}.\) Below, instead of training a neural network, we numerically estimate the true velocity field and visualize it directly.</p> <hr/> <h2 id="visualizing-true-velocities">Visualizing true velocities</h2> <p>For different values of \(t\), we visualize the velocity field using two color plots (a full vector-field visualization would be difficult to interpret due to potential multi-scale structure). Top: \(v_x\); bottom: \(v_y\). In each subplot, black dots indicate data samples, blue dots indicate generated samples at that time step, and red dashed lines show the zero-level sets.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/v-vis-480.webp 480w,/assets/img/blogs/diffusion-2/v-vis-800.webp 800w,/assets/img/blogs/diffusion-2/v-vis-1400.webp 1400w,/assets/img/blogs/diffusion-2/v-vis-1920.webp 1920w,/assets/img/blogs/diffusion-2/v-vis-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/v-vis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This makes it clear why training at a single \(t \sim 0.9\) can already yield high-quality samples: the zero-velocity contours align well with the spiral manifold. Under this velocity field, samples naturally converge toward the zero-velocity region.</p> <p>We also notice that that scale of \(v\) diverges as \(1/(1-t)\) when \(t\to 1\), which is another reason why predicting \(v\) is hard, besides JIT paper’s reasoning about manifolds.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1G7JUb5HDh6hHgYirsVF7LAfPyNr0CKa2?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Diffusion 2 -- Visualizing flow matching, temporal dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/" rel="alternate" type="text/html" title="Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”"/><published>2026-01-31T00:00:00+00:00</published><updated>2026-01-31T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-7</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading the paper <a href="https://arxiv.org/pdf/2501.00659">“Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph”</a>, which makes an interesting claim: even without explicit positional embeddings, multi-layer causal attention may implicitly encode positional information, despite the fact that a single-layer causal attention is permutation equivariant. Figure 1 from the paper is shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/fig1-480.webp 480w,/assets/img/blogs/sparse-attention-7/fig1-800.webp 800w,/assets/img/blogs/sparse-attention-7/fig1-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/fig1-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/fig1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The goal of this article is to measure how well multi-layer causal attention can extract previous tokens based purely on position. Given a sequence of random tokens (with context length \(C\)), the target is the \(k^{\rm th}\) token in the sequence (\(0 \leq k \leq C-1\)). For example, when \(C = 5\) and \(k = 1\), example sequences are</p> \[[5]\ [2]\ [3]\ [7]\ [9] \to [2]\] \[[1]\ [9]\ [2]\ [5]\ [8] \to [9]\] <p>We set the vocabulary size to \(V = 10\), context length to \(C = 10\), and the number of attention heads to \(n_{\rm head} = 1\). We are interested in varying the number of layers \(n_{\rm layer}\), the embedding dimension \(n_{\rm embd}\), whether residual connections are used, and whether MLPs are included.</p> <hr/> <h2 id="2l-is-qualitatively-different-from-1l">2L is qualitatively different from 1L</h2> <p>For each \(0 \leq k \leq C-1\), we train the transformer to convergence and record the final loss. We then plot how the final loss depends on \(k\). This roughly measures positional bias: a high loss means that retrieving information from that position is difficult—the loss cannot be driven down even when the model is explicitly trained on the retrieval task.</p> <p>In <a href="/blog/2026/sparse-attention-2/">sparse-attention-2</a>, we showed that a 1-layer causal attention model cannot retrieve any previous token (\(k \neq C-1\)), but can retrieve the current token (\(k = C-1\)). This corresponds to the left plot below (high loss for \(k \neq 9\), and almost zero loss for \(k = 9\)). The right plot shows the result for a 2-layer causal attention model, which exhibits the well-known <a href="https://arxiv.org/pdf/2307.03172">“lost in the middle” phenomenon</a>: early and late tokens are easily retrieved, while tokens in the middle are difficult to extract.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/1L-2L-480.webp 480w,/assets/img/blogs/sparse-attention-7/1L-2L-800.webp 800w,/assets/img/blogs/sparse-attention-7/1L-2L-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/1L-2L-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/1L-2L-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/1L-2L.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="embedding-dimension-and-number-of-layers-do-not-change-the-qualitative-behavior">Embedding dimension and number of layers do not change the qualitative behavior</h2> <p>We experiment with embedding dimensions \(\{2, 10\}\) and number of layers \(\{2, 3\}\). The qualitative trend remains unchanged: only the first and last tokens can be retrieved reliably, while tokens in the middle remain difficult. This manifests as a long plateau in the loss curve across middle positions.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/embd-layer-480.webp 480w,/assets/img/blogs/sparse-attention-7/embd-layer-800.webp 800w,/assets/img/blogs/sparse-attention-7/embd-layer-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/embd-layer-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/embd-layer-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/embd-layer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="residual-connections-and-mlps-destroy-the-long-plateau">Residual connections and MLPs destroy the long plateau</h2> <p>When residual connections and MLPs are added, the long plateau disappears and is replaced by a more spiky structure.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/residual-mlp-480.webp 480w,/assets/img/blogs/sparse-attention-7/residual-mlp-800.webp 800w,/assets/img/blogs/sparse-attention-7/residual-mlp-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/residual-mlp-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/residual-mlp-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/residual-mlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <p>This study suggests a simple mechanism underlying the “lost in the middle” phenomenon: even without explicit positional embeddings, stacking multiple causal-attention layers naturally induces an architectural bias toward losing information in the middle of the sequence.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/14OOPXMz4C40aIqOoWSxBzCH7hp3g3GZ9?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-7</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning "Loss in the middle" }</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-7/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 6 – In-context Associative recall</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-6/" rel="alternate" type="text/html" title="Sparse attention 6 – In-context Associative recall"/><published>2026-01-30T00:00:00+00:00</published><updated>2026-01-30T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-6</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-6/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>This article aims to reproduce the in-context associative recall task from the paper <a href="https://arxiv.org/pdf/2505.17863">“The emergence of sparse attention: impact of data distribution and benefits of repetition”</a>. The task is as follows: given a sequence,</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/task-480.webp 480w,/assets/img/blogs/sparse-attention-6/task-800.webp 800w,/assets/img/blogs/sparse-attention-6/task-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/task-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/task-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/task.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>the model must output <strong>Y</strong>, since the query <strong>Z</strong> matches the key in the <strong>Z–Y</strong> pair appearing earlier in the context.</p> <p>I am particularly interested in Figure 4 (left) of the paper (shown below), which demonstrates that learning is significantly delayed when the context length is large (with the context length being twice the number of pairs). In that regime, it takes about \(10^5\) steps for in-context learning (ICL) to emerge. My intuition is that learning can be substantially accelerated by choosing appropriate hyperparameters. For example, I would expect that a smaller embedding dimension could lead to much faster learning, whereas the paper uses an embedding dimension of 256.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/phase-transition-480.webp 480w,/assets/img/blogs/sparse-attention-6/phase-transition-800.webp 800w,/assets/img/blogs/sparse-attention-6/phase-transition-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/phase-transition-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/phase-transition-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/phase-transition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-embedding-dimension">Dependence on embedding dimension</h2> <p>We use a 2-layer, attention-only transformer model with embedding dimension \(N\) (swept), training data size \(10^4\), vocabulary size 64, and 32 key–value pairs. The model is trained using the Adam optimizer with learning rate \(10^{-2}\). While the original paper uses an embedding dimension of 256, we hypothesize that much smaller embedding dimensions can already succeed.</p> <p>Indeed, we find a Goldilocks zone in which ICL emerges in fewer than 1000 steps. Both too small and too large embedding dimensions lead to failure. An embedding dimension of 60 is particularly interesting: the model initially learns but then becomes unstable, likely due to an excessively large learning rate.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/n_embd-480.webp 480w,/assets/img/blogs/sparse-attention-6/n_embd-800.webp 800w,/assets/img/blogs/sparse-attention-6/n_embd-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/n_embd-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/n_embd-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/n_embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="how-to-make-large-embedding-dimensions-work">How to make large embedding dimensions work</h2> <p>When one is forced to use a large embedding dimension for some reason,</p> <ul> <li>the data size should be increased to mitigate overfitting;</li> <li>the learning rate should be reduced, in line with the common wisdom that wider networks require smaller learning rates. This is why learning is slow for large embedding dimensions.</li> </ul> <p>We sweep over data sizes \(\{10^4, 10^5\}\) and learning rates \(\{10^{-2}, 10^{-3}\}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/80-480.webp 480w,/assets/img/blogs/sparse-attention-6/80-800.webp 800w,/assets/img/blogs/sparse-attention-6/80-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/80-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/80-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>More systematically, it would be interesting to draw phase diagrams capturing how learning curves depend on various hyper-parameters.</p> <hr/> <h2 id="comment-link-to-learning-rate-warmup">Comment: Link to learning rate warmup</h2> <p>For patterns with very strong signals that can be learned very early (induction is arguably a strong pattern due to its commonality), keeping a large learning rate will eventually make them unstable. Therefore, in the early stage, a relatively small learning rate is needed to prevent these strong-signal components from becoming unstable. In the later stage, however, we want to accelerate the learning of weaker signals, which calls for a larger learning rate. This is simply a hypothesis which requires further justification.</p> <p>Induction heads are likely such strong-signal components. This also connects nicely to the earlier discussions on <a href="/blog/2026/induction-head-lr-schedule/">learning-rate schedules</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/10wYqmEVn4CGJfkPdZHdGfb7Z3pdHR7Iu?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-6</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 6 -- In-context Associative recall}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-6/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">MLP 2 – Effective linearity, Generalized SiLU</title><link href="https://kindxiaoming.github.io/blog/2026/mlp-2/" rel="alternate" type="text/html" title="MLP 2 – Effective linearity, Generalized SiLU"/><published>2026-01-29T00:00:00+00:00</published><updated>2026-01-29T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/mlp-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/mlp-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>MLPs are powerful because they include nonlinear activation functions. But how effectively do they actually use this nonlinearity? If we perform a linear regression between the MLP inputs and outputs and obtain \(R^2 \approx 1\), then the MLP is not making effective use of its nonlinearity.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We use the same setup as in <a href="/blog/2026/depth-1/">depth-1</a>. Both the teacher and the student networks are residual networks with MLPs. The teacher network uses the SiLU activation function, while we vary the activation function of the student network. We measure the \(R^2\) values for the MLPs in all layers and track how these quantities evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/r2-480.webp 480w,/assets/img/blogs/mlp-2/r2-800.webp 800w,/assets/img/blogs/mlp-2/r2-1400.webp 1400w,/assets/img/blogs/mlp-2/r2-1920.webp 1920w,/assets/img/blogs/mlp-2/r2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/r2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that lower \(R^2\) values tend to correspond to lower losses, which aligns with the intuition that a lower \(R^2\) indicates stronger nonlinearity. One could attempt to minimize \(R^2\) further—for example, by using high-frequency sine functions—but this would likely harm trainability. There appears to be a trade-off between stability and nonlinearity.</p> <hr/> <h2 id="generalized-silu">Generalized SiLU</h2> <p>Can we define a family of activation functions for which SiLU is a special case, while also allowing for more oscillatory behavior? We define a generalized SiLU function as follows:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/gen_silu_shape-480.webp 480w,/assets/img/blogs/mlp-2/gen_silu_shape-800.webp 800w,/assets/img/blogs/mlp-2/gen_silu_shape-1400.webp 1400w,/assets/img/blogs/mlp-2/gen_silu_shape-1920.webp 1920w,/assets/img/blogs/mlp-2/gen_silu_shape-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/gen_silu_shape.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The definition in Python code is shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/code-480.webp 480w,/assets/img/blogs/mlp-2/code-800.webp 800w,/assets/img/blogs/mlp-2/code-1400.webp 1400w,/assets/img/blogs/mlp-2/code-1920.webp 1920w,/assets/img/blogs/mlp-2/code-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SiLU corresponds to generalized SiLU with parameters (0, 0). We find that generalized SiLU with parameters (0, 1) or (1, 1) can achieve extremely low loss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/gen_silu-480.webp 480w,/assets/img/blogs/mlp-2/gen_silu-800.webp 800w,/assets/img/blogs/mlp-2/gen_silu-1400.webp 1400w,/assets/img/blogs/mlp-2/gen_silu-1920.webp 1920w,/assets/img/blogs/mlp-2/gen_silu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/gen_silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1MH6l2vzH0BOcGV0g9CrKm-sc4IepQAet?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026mlp-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MLP 2 -- Effective linearity, Generalized SiLU}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/mlp-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">MLP 1 – Gating is good for polynomials</title><link href="https://kindxiaoming.github.io/blog/2026/mlp-1/" rel="alternate" type="text/html" title="MLP 1 – Gating is good for polynomials"/><published>2026-01-28T00:00:00+00:00</published><updated>2026-01-28T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/mlp-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/mlp-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Gated MLPs are widely used in modern language models. Given an input \(x\), the hidden representation of a gated MLP is \(\sigma(W_g x) \odot W_v x .\) It is often argued that gating promotes sparsity, multiplicative structure, conditional computation, and more stable training. This is far too large a topic to be fully explored in a single blog post. In this article, we narrow the scope to polynomial fitting. If learning a Taylor expansion is important for neural networks, then learning polynomials is a necessary prerequisite.</p> <hr/> <h2 id="polynomials">Polynomials</h2> <p>For both ReLU and SiLU activations, gated MLPs (green) consistently outperform non-gated MLPs (blue) when fitting \(x^n\) for all \(n \ge 2\). We also compare against non-gated MLP but with the activation function squared (orange), and find that squared activation functions can achieve strong performance—sometimes even better than gated networks.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-1/poly-480.webp 480w,/assets/img/blogs/mlp-1/poly-800.webp 800w,/assets/img/blogs/mlp-1/poly-1400.webp 1400w,/assets/img/blogs/mlp-1/poly-1920.webp 1920w,/assets/img/blogs/mlp-1/poly-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-1/poly.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="a-rough-equivalence">A rough equivalence</h2> <p>We observe that as \(\epsilon \to 0^+\), a Taylor expansion gives \(\sigma((w+\epsilon)x) - \sigma((w-\epsilon)x) \approx 2\epsilon x \sigma'(wx) .\) This suggests a rough “equivalence” between \(\sigma(x)\) and \(x \sigma'(x)\), where \(x \sigma'(x)\) can be interpreted as a gate. For gated ReLU networks, since \(\sigma'(x) = {\rm ReLU}\), this implies \(\sigma(x) = {\rm ReLU}^2\). In other words, a gated ReLU network is roughly equivalent to a non-gated ReLU2 network.</p> <p>So far, this is not meant to be mathematically rigorous—just a hand-wavy but potentially useful intuition. Once we have a useful gate, it can be converted to a useful activation function for free (except for doing integration).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Dqn6dkxW0hP04s30F3zbT3wYZivsXv65?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026mlp-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MLP 1 -- Gating is good for polynomials}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/mlp-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 4 – Loss Spikes</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-4/" rel="alternate" type="text/html" title="Optimization 4 – Loss Spikes"/><published>2026-01-27T00:00:00+00:00</published><updated>2026-01-27T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>During neural network training, we often observe sudden loss spikes. The goal of this article is to construct a minimal model that helps us understand the origin of such loss spikes.</p> <hr/> <h2 id="2d-toy-model">2D toy model</h2> <p>We aim to regress the function \(y(x) = x^2\) using an MLP. I first found that a 2-layer MLP is sufficient to produce loss spikes. Then I found that a width of 2 is already enough. Further, removing biases does not eliminate the spikes. At this point, the model has only four parameters, and clear symmetries emerge: the two weights in the first layer are almost identical up to a sign flip, and the two weights in the second layer are nearly the same.</p> <p>After all these simplifications, we are naturally led to study the following toy model, which has only two parameters \((a, w)\): \(f(x) = a \bigl(\sigma(wx) + \sigma(-wx)\bigr),\ \sigma(x) = {\rm silu}(x) = \frac{x}{1+e^{-x}}\)</p> <p>We train this “model” to fit the squared function for \(x \in [-1, 1]\).</p> <hr/> <h2 id="loss-spikes">Loss spikes</h2> <p>We initialize \(w = 1\) and \(a = 0\), and train using the Adam optimizer (learning rate 0.1) for 5000 steps. The left plot shows that the training loss exhibits spikes. When zoomed in, each spike is asymmetric: a rapid instability followed by a gradual recovery. The right plot shows the training trajectory, which appears to consist of two phases whose dominant movement directions are nearly orthogonal. Loss spikes occur in the second phase, where the parameters seem to navigate a long, narrow valley—known as a <a href="https://arxiv.org/abs/2410.05192">river-valley landscape</a>.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/loss-spike-480.webp 480w,/assets/img/blogs/optimization-4/loss-spike-800.webp 800w,/assets/img/blogs/optimization-4/loss-spike-1400.webp 1400w,/assets/img/blogs/optimization-4/loss-spike-1920.webp 1920w,/assets/img/blogs/optimization-4/loss-spike-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/loss-spike.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="loss-landscape">Loss landscape</h2> <p>We further verify the presence of a river-valley landscape:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/loss-landscape-480.webp 480w,/assets/img/blogs/optimization-4/loss-landscape-800.webp 800w,/assets/img/blogs/optimization-4/loss-landscape-1400.webp 1400w,/assets/img/blogs/optimization-4/loss-landscape-1920.webp 1920w,/assets/img/blogs/optimization-4/loss-landscape-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/loss-landscape.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Loss spikes occur when the effective learning rate becomes too large relative to the edge-of-stability threshold \(2/C\), where \(C\) denotes the curvature (see, e.g., <a href="https://arxiv.org/abs/2103.00065">edge of stability</a> and the <a href="https://arxiv.org/abs/2206.04817">slingshot mechanism</a>). We measure that the curvature at the bottom of the valley is approximately proportional to \(a\) (see the left plot below). As \(a\) increases, \(C\) increases, making spikes more likely.</p> <hr/> <h2 id="periodicity">Periodicity</h2> <p>After the first spike, subsequent spikes appear periodically, roughly every 250 steps. As discussed above, the occurrence of a spike depends on two factors: the effective learning rate and the curvature. For Adam, the effective learning rate depends on the second-moment accumulator, <code class="language-plaintext highlighter-rouge">exp_avg_sq</code>, which is shown in the middle plot below. Aside from the initial growth and the spikes themselves, <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> decays exponentially as \(0.999^t\), where \(0.999\) is Adam’s \(\beta_2\).</p> <p>A plausible explanation for the spike periodicity is therefore the following: each spike amplifies <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> by some constant factor, and the interval between spikes corresponds to the time it takes for this factor to decay back to 1 (or slightly above 1, since the curvature increases slowly over time). The curvature is roughly a linear function of \(a\) (left plot), and Adam statistics are also shown in the right plot.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/adam-480.webp 480w,/assets/img/blogs/optimization-4/adam-800.webp 800w,/assets/img/blogs/optimization-4/adam-1400.webp 1400w,/assets/img/blogs/optimization-4/adam-1920.webp 1920w,/assets/img/blogs/optimization-4/adam-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/adam.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If this explanation is correct, we would expect a larger \(\beta_2\) to result in less frequent spikes, which is indeed what we observe.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/beta2-480.webp 480w,/assets/img/blogs/optimization-4/beta2-800.webp 800w,/assets/img/blogs/optimization-4/beta2-1400.webp 1400w,/assets/img/blogs/optimization-4/beta2-1920.webp 1920w,/assets/img/blogs/optimization-4/beta2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/beta2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="video">Video</h2> <p>A close-up view of how a spike unfolds:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/spike_animation-480.webp 480w,/assets/img/blogs/optimization-4/spike_animation-800.webp 800w,/assets/img/blogs/optimization-4/spike_animation-1400.webp 1400w,/assets/img/blogs/optimization-4/spike_animation-1920.webp 1920w,/assets/img/blogs/optimization-4/spike_animation-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/spike_animation.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It’s interesting to observe that the parameter could temporarily go backwards along the river during the spike, probably due to the <a href="https://arxiv.org/abs/2505.10559">entropic force</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1goCzaQj9JAhvI4jJ-m77gsA0SAHRzwDR?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 4 -- Loss Spikes}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 3 / Depth 2 – Adding Bias After ReLU</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-3/" rel="alternate" type="text/html" title="Optimization 3 / Depth 2 – Adding Bias After ReLU"/><published>2026-01-25T00:00:00+00:00</published><updated>2026-01-25T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I have been playing with the toy model of residual networks defined in <a href="/blog/2026/depth-1/">depth-1</a>. There is no specific goal—just exploratory poking around.</p> <hr/> <h2 id="representation-collapse">Representation collapse</h2> <p>Although the input \(x\) to the deep network is drawn from an isotropic Gaussian distribution, I observe that the output \(y\) appears to have a clearly non-zero mean across the batch dimension.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/print_y-480.webp 480w,/assets/img/blogs/optimization-3/print_y-800.webp 800w,/assets/img/blogs/optimization-3/print_y-1400.webp 1400w,/assets/img/blogs/optimization-3/print_y-1920.webp 1920w,/assets/img/blogs/optimization-3/print_y-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/print_y.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The histogram confirms this observation: while the mean of \(x\) is close to 0, the mean of \(y\) clearly deviates from 0.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/hist_x_y_mean-480.webp 480w,/assets/img/blogs/optimization-3/hist_x_y_mean-800.webp 800w,/assets/img/blogs/optimization-3/hist_x_y_mean-1400.webp 1400w,/assets/img/blogs/optimization-3/hist_x_y_mean-1920.webp 1920w,/assets/img/blogs/optimization-3/hist_x_y_mean-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/hist_x_y_mean.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To quantify this deviation, we define \(\Delta\) as the standard deviation (across width) of the mean (across data points) of the output \(y\) at initialization.</p> <p>We make two comments regarding the fact that \(\Delta &gt; 0\):</p> <ul> <li><strong>Origin:</strong> We hypothesize that this effect arises from ReLU inducing distribution shifts (see <a href="/blog/2026/activation-anisotropy/">activation anisotropy</a>).</li> <li><strong>Judgment:</strong> This behavior is undesirable, and we would like to eliminate it (i.e., make \(\Delta \sim 0\)).</li> </ul> <p>This raises the question: how can we eliminate it, and is it truly a feature or a bug?</p> <hr/> <h2 id="trick-adding-bias-after-relu">Trick: adding bias after ReLU</h2> <p>Since the issue originates from ReLU shifting distributions toward the positive side, we can counteract this effect by adding a negative bias \(b\) after ReLU. Equivalently, the activation function becomes \({\rm ReLU}(\cdot) - b\).</p> <p>We use a teacher–student setup. The teacher network is 10 layers deep, with input dimension 50 and hidden dimension 200. It uses post-norm and sets \(b = 0\). The student network shares the same architecture, but we sweep over different values of \(b\). Student networks are trained using the Adam optimizer with learning rate \(10^{-3}\) for 1000 steps. The results are shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/loss_delta-480.webp 480w,/assets/img/blogs/optimization-3/loss_delta-800.webp 800w,/assets/img/blogs/optimization-3/loss_delta-1400.webp 1400w,/assets/img/blogs/optimization-3/loss_delta-1920.webp 1920w,/assets/img/blogs/optimization-3/loss_delta-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/loss_delta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the middle plot, we see that \(\Delta\) exhibits a U-shaped dependence on \(b\). Values \(b = 0.2, 0.3\) yield low \(\Delta\), which correspond to fast and smooth learning curves in the left plot. In the right plot, we explicitly show how the final loss correlates with \(\Delta\), revealing a clear positive correlation.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1KE2T0VdxA_1GZRDGFqWXVZFAQX-GmhYI?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 3 / Depth 2 -- Adding Bias After ReLU}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 2 – Elementwise Scale Reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-2/" rel="alternate" type="text/html" title="Optimization 2 – Elementwise Scale Reparametrization"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, we showed that a re-parametrization trick can significantly speed up optimization. The key ideas were: (1) decompose the weight as \(W = s \hat{W}\), where \(s\) is a scale factor and \(\hat{W}\) has unit norm and represents the direction; (2) make both \(s\) and \(\hat{W}\) learnable. After each update, we rescale \(\hat{W}\) back to the unit sphere and adjust \(s\) accordingly so that the product \(W = s \hat{W}\) remains unchanged.</p> <p>In this blog, we continue along this direction, but with two additional questions in mind:</p> <ul> <li>The toy example in the previous blog is too simple. We would like to extend it to a slightly more realistic setup (a linear layer).</li> <li>Previously, we used a scalar \(s\) to normalize the vector. Is it possible to perform the normalization element-wise instead?</li> </ul> <hr/> <h2 id="failure-mode-when-normalizing-element-wise">Failure mode when normalizing element-wise</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, the toy example was two-dimensional. This was a deliberate choice, because the one-dimensional case has a failure mode: when \(s\) is not learnable, the weight cannot change sign, since \(\hat{W}\) can only take values \(+1\) or \(-1\), which are not connected. This issue does not arise in dimensions greater than or equal to two, because unit vectors form a continuous manifold and the two poles are connected, hence reachable through learning.</p> <p>In practice, however, we find that as long as \(s\) is learnable, the weight parameter can change its sign even in 1D, because \(s\) itself can change sign (by crossing zero) during training.</p> <p>We numerically test the four modes discussed in the <a href="/blog/2026/optimization-2/">previous blog</a> for the 1D case, with initial weight \(W_0 = -1\) and target \(W^* = 100\). With re-parametrization and a learnable scale, learning is much faster than in the other modes.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/1D_compare_new-480.webp 480w,/assets/img/blogs/optimization-2/1D_compare_new-800.webp 800w,/assets/img/blogs/optimization-2/1D_compare_new-1400.webp 1400w,/assets/img/blogs/optimization-2/1D_compare_new-1920.webp 1920w,/assets/img/blogs/optimization-2/1D_compare_new-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/1D_compare_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="linear-regression">Linear regression</h2> <p>We consider a linear regression setup where \(y = W^* x\), with \(x \in \mathbb{R}^{10}\), \(y \in \mathbb{R}^{10}\), and \(W \in \mathbb{R}^{10 \times 10}\) (for simplicity, we ignore the bias term). We draw \(x \in \mathbb{R}^{10}\) from a standard Gaussian distribution. To generate labels \(y\), we deliberately make \(W^*\) anisotropic by setting \(W^*_{ij} = s_i s_j v_{ij},\) where \(v_{ij}\) is drawn from a standard Gaussian, and \(s_i = 10^{-1 + 2 i / 9}, \quad (i = 0, 1, \dots, 9),\) which spans a wide range in \([0.1, 10]\). The loss function is the MSE loss, and we use the Adam optimizer with learning rate \(\eta = 0.01\).</p> <p>We compare the following eight modes:</p> <ul> <li>8 = [Scalar, Matrix (element-wise)] × [fixed, learnable] × [Re-parametrization, No re-parametrization]</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/linear_regression-480.webp 480w,/assets/img/blogs/optimization-2/linear_regression-800.webp 800w,/assets/img/blogs/optimization-2/linear_regression-1400.webp 1400w,/assets/img/blogs/optimization-2/linear_regression-1920.webp 1920w,/assets/img/blogs/optimization-2/linear_regression-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Scalar normalization is faster than element-wise normalization in the early stage (compare green and pink).</li> <li>Re-parametrization helps during early training, but not as much—and can even slow things down—during later training (e.g., compare green and red; compare pink and gray).</li> <li>A learnable scale is essential for element-wise normalization (compare purple and pink; compare brown and gray).</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/19ykGEIlSxlnftuTAcycGoffVdFQ1j8aW?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 2 -- Elementwise scale reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 1 – Norm reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-1/" rel="alternate" type="text/html" title="Optimization 1 – Norm reparametrization"/><published>2026-01-23T00:00:00+00:00</published><updated>2026-01-23T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Recently, a number of architectures and optimizers have emphasized the role of normalization and/or learnable scales. To name a few:</p> <ul> <li>2026-01-21: Hyperball optimization (<a href="https://psychedelic-sunstone-851.notion.site/Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them-2-1-Hyperball-Optimization-2e924306e6f280e7a5ffee00eb40a0dd">link</a>)</li> <li>2026-01-13: Controlled LLM training on a spectral hypersphere (<a href="https://arxiv.org/pdf/2601.08393">Link</a>)</li> <li>2026-01-08: Learnable Multipliers (<a href="https://arxiv.org/pdf/2601.04890">Link</a>)</li> </ul> <p>Slightly less recent (last year :-) ), there are also the <a href="https://kellerjordan.github.io/posts/muon/">Muon optimizer</a> and <a href="https://arxiv.org/abs/2410.01131v1">NVIDIA’s nGPT</a>. These examples give a sense of how popular this direction has become.</p> <p>On the one hand, the intuition is clear: when the step size is fixed, constraining parameters to lie on a small (but not too small) sphere allows the optimizer to make reasonable <em>angular</em> updates. On the other hand, we typically do not know the optimal scale in advance, and in some cases the optimal scale may be very large—or may even be infinite (e.g., logistic regression on linearly separable data). As a result, we would like an optimizer that can make both effective <em>angular</em> updates and effective <em>radial (scale)</em> updates.</p> <p>To gain insight, we start with a toy example below.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>Assume the optimal (flattened) weight of a model is \(W^*\in\mathbb{R}^N\). The initial weight is \(W_0\in\mathbb{R}^N\), and the loss is the MSE loss between \(W\) and \(W^*\), i.e., \(L = \|W - W^*\|_2^2 .\) An Adam optimizer with learning rate \(\eta\) is used to minimize the MSE loss.</p> <p><strong>When the target \(W^*\) is very far away</strong>, such that \(R \equiv \|W^*\| \gg \|W_0\| ,\) the time to reach the target is roughly \(t = \frac{R}{\sqrt{N}\eta} ,\) assuming (1) Adam behaves like SignGD with step size \(\eta\) so that each update has norm \(\sqrt{N}\eta\), and (2) updates from different steps are approximately parallel. In summary, the time complexity is \(O(R)\), which is not very good when \(R\) is large.</p> <p><strong>Can we do better?</strong><br/> Yes, we can!</p> <ul> <li>By introducing a learnable norm scalar, the time can be reduced to \(O(\sqrt{R})\).</li> <li>By further using a re-parametrization trick, the time can be reduced to \(O(\log R)\).</li> </ul> <hr/> <h2 id="step-1-introducing-learnable-multipliers">Step 1: Introducing learnable multipliers</h2> <p>To control both the angular direction and the norm, it is natural to introduce a learnable multiplier \(s\). The actual weight \(W\) is parameterized as \(W = s \hat{W},\) where \(\hat{W}\) may or may not be normalized.</p> <p>When \(\hat{W}\) is learnable and not normalized, it is likely that \(|s| \to O(\sqrt{R}), \quad \|\hat{W}\| \to O(\sqrt{R}),\) since their roles are symmetric in the multiplication. Hence, the convergence time becomes \(t = O(\sqrt{R}/\eta).\) This is already better than \(O(R)\), but can we do even better?</p> <hr/> <h2 id="step-2-re-parametrization">Step 2: Re-parametrization</h2> <p>At each step, we compute the norm \(a \equiv \|\hat{W}\|_2\) and apply the following re-parametrization: \(\hat{W} \to \hat{W}/a, \quad s \to a s .\) This places \(\hat{W}\) on the unit hypersphere, representing the angular direction, while \(s\) captures the norm. Their product remains invariant under this re-parametrization. Training proceeds by interleaving gradient updates and re-parametrization steps.</p> <p>In each update, the norm of \(\hat{W}\) changes from 1 (due to re-parametrization in the previous step) to \(1 \pm A\eta\), where \(A = A(N)\) depends on \(N\) and the gradient directions, which we do not worry about here. The key point is that this is a <em>multiplicative</em> (rather than additive) change. Therefore, growing the norm from 1 to \(R\) only takes \(O(\log R)\) steps.</p> <hr/> <h2 id="experiment">Experiment</h2> <p>We conduct a 2D toy experiment. We set \(W_0 = (-50, 0), \quad W^* = (100, 100).\) We use Adam with learning rate \(\eta = 0.01\) and train for 3000 steps. We compare four modes:</p> <ul> <li><strong>no_scale</strong>: standard optimization (fixed multiplier \(s = 1\))</li> <li><strong>learnable_scale</strong>: learnable \(s\)</li> <li><strong>reparametrized_learnable_scale</strong>: re-parametrization, fix \(s = 1\) during the update step</li> <li><strong>reparametrized_fixed_scale</strong>: re-parametrization, allow \(s\) to update during the update step</li> </ul> <p>Optimization trajectory:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/trajectory-480.webp 480w,/assets/img/blogs/optimization-1/trajectory-800.webp 800w,/assets/img/blogs/optimization-1/trajectory-1400.webp 1400w,/assets/img/blogs/optimization-1/trajectory-1920.webp 1920w,/assets/img/blogs/optimization-1/trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/trajectory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Loss:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/loss-480.webp 480w,/assets/img/blogs/optimization-1/loss-800.webp 800w,/assets/img/blogs/optimization-1/loss-1400.webp 1400w,/assets/img/blogs/optimization-1/loss-1920.webp 1920w,/assets/img/blogs/optimization-1/loss-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>As expected, re-parametrization &gt; learnable scale &gt; no scale.</li> <li>With re-parametrization, whether the scale \(s\) is learnable or fixed during the update step does not seem to make a noticeable difference.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Bx3ojrssLueShvJdU-ba7DQMWLFuqRVx?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 1 -- Norm reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 5 – Attention sink</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/" rel="alternate" type="text/html" title="Sparse attention 5 – Attention sink"/><published>2026-01-22T00:00:00+00:00</published><updated>2026-01-22T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-5</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>This article studies how an <strong>attention sink</strong> can emerge in a single-layer (1L), attention-only transformer.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> When all previous tokens are useless for predicting the next token, the ideal attention weights would be zero everywhere. However, due to softmax normalization, attention must be allocated somewhere. As a result, if a <code class="language-plaintext highlighter-rouge">[BOS]</code> token is present at the beginning of the sequence, it can serve as a reservoir that absorbs attention.</p> <p>We use the unigram toy dataset (defined in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>), where the next token does not depend on any previous tokens, but is instead drawn independently from a token-frequency distribution.</p> <p><strong>Model</strong><br/> A 1L attention-only transformer. For simplicity, we remove positional embeddings.</p> <hr/> <h2 id="experiment-1-unigram-dataset">Experiment 1: Unigram dataset</h2> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>We choose the embedding dimension to be 2 and the vocabulary size to be 101 (<code class="language-plaintext highlighter-rouge">[BOS]</code> + 100 tokens whose frequencies follow Zipf’s law). This configuration already performs the task perfectly (in fact, \(n_{\rm embd}=1\) suffices, as discussed in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>).</p> <p>The attention maps are shown below. Each row corresponds to a different sample, and each column corresponds to a different training step. Besides the <code class="language-plaintext highlighter-rouge">[BOS]</code> token, frequent tokens are also used to absorb attention.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram_bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram_bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram_bos-1400.webp 1400w,/assets/img/blogs/sparse-attention-5/unigram_bos-1920.webp 1920w,/assets/img/blogs/sparse-attention-5/unigram_bos-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram_bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The token embeddings for four different random seeds are shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/token_embd-480.webp 480w,/assets/img/blogs/sparse-attention-5/token_embd-800.webp 800w,/assets/img/blogs/sparse-attention-5/token_embd-1400.webp 1400w,/assets/img/blogs/sparse-attention-5/token_embd-1920.webp 1920w,/assets/img/blogs/sparse-attention-5/token_embd-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/token_embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Recall that we set token frequencies to follow a Zipfian distribution, so small-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[0]</code>) appear more often than large-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[99]</code>). Interestingly, large-index tokens are closer to <code class="language-plaintext highlighter-rouge">[BOS]</code> than small-index tokens in embedding space. Since <code class="language-plaintext highlighter-rouge">[BOS]</code> is heavily attended to, this implies that large-index (infrequent) tokens receive more attention than small-index (frequent) tokens. This likely occurs because attention to frequent tokens is more strongly suppressed than attention to infrequent tokens.</p> <p>Indeed, it has been observed in large language models that their embedding spaces contain a direction correlated with token frequency. Our toy model offers one plausible explanation: this structure emerges for the same reason as the attention sink.</p> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>When <code class="language-plaintext highlighter-rouge">[BOS]</code> is not present, attention sinks still emerge, and these sinks correspond to frequent tokens (with low indices, e.g., <code class="language-plaintext highlighter-rouge">[0]</code>, <code class="language-plaintext highlighter-rouge">[1]</code>).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-1400.webp 1400w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-1920.webp 1920w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The five sequences are:</p> \[[ 1, 4, 88, 0, 6, 93, 1, 63, 9, 32]\] \[[ 3, 2, 57, 0, 5, 9, 3, 20, 67, 0]\] \[[ 1, 15, 1, 84, 0, 0, 5, 18, 53, 18]\] \[[ 5, 1, 7, 10, 6, 1, 5, 73, 2, 0]\] \[[ 1, 1, 17, 25, 46, 0, 0, 88, 0, 5]\] <hr/> <h2 id="experiment-2-bigram-dataset">Experiment 2: Bigram dataset</h2> <p>We now switch to a bigram toy dataset defined in <a href="/blog/2026/bigram-3/">bigram-3</a>. In this setting, the next token depends on the current token, so the ideal attention pattern would be a diagonal matrix.</p> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is nearly diagonal, except for splitting among identical previous tokens. In addition, some tokens incorrectly attend to the <code class="language-plaintext highlighter-rouge">[BOS]</code> token (e.g., the 3rd token in example 10 and the 3rd token in example 12).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-w-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-1400.webp 1400w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-1920.webp 1920w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-w-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is again almost diagonal, with the main deviation being splitting among identical previous tokens.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-1400.webp 1400w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-1920.webp 1920w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1f64J1efN-p_OCfcVo2oxbwYjeUEOT_bb?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-5</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 5 -- Attention sink}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-5/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>