<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-24T06:38:02+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Optimization 2 – Elementwise Scale Reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-2/" rel="alternate" type="text/html" title="Optimization 2 – Elementwise Scale Reparametrization"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, we showed that a re-parametrization trick can significantly speed up optimization. The key ideas were: (1) decompose the weight as \(W = s \hat{W}\), where \(s\) is a scale factor and \(\hat{W}\) has unit norm and represents the direction; (2) make both \(s\) and \(\hat{W}\) learnable. After each update, we rescale \(\hat{W}\) back to the unit sphere and adjust \(s\) accordingly so that the product \(W = s \hat{W}\) remains unchanged.</p> <p>In this blog, we continue along this direction, but with two additional questions in mind:</p> <ul> <li>The toy example in the previous blog is too simple. We would like to extend it to a slightly more realistic setup (a linear layer).</li> <li>Previously, we used a scalar \(s\) to normalize the vector. Is it possible to perform the normalization element-wise instead?</li> </ul> <hr/> <h2 id="failure-mode-when-normalizing-element-wise">Failure mode when normalizing element-wise</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, the toy example was two-dimensional. This was a deliberate choice, because the one-dimensional case has a failure mode: when \(s\) is not learnable, the weight cannot change sign, since \(\hat{W}\) can only take values \(+1\) or \(-1\), which are not connected. This issue does not arise in dimensions greater than or equal to two, because unit vectors form a continuous manifold and the two poles are connected, hence reachable through learning.</p> <p>In practice, however, we find that as long as \(s\) is learnable, the weight parameter can change its sign even in 1D, because \(s\) itself can change sign (by crossing zero) during training.</p> <p>We numerically test the four modes discussed in the <a href="/blog/2026/optimization-2/">previous blog</a> for the 1D case, with initial weight \(W_0 = -1\) and target \(W^* = 100\). With re-parametrization and a learnable scale, learning is much faster than in the other modes.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/1D_compare-480.webp 480w,/assets/img/blogs/optimization-2/1D_compare-800.webp 800w,/assets/img/blogs/optimization-2/1D_compare-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/1D_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="linear-regression">Linear regression</h2> <p>We consider a linear regression setup where \(y = W^* x\), with \(x \in \mathbb{R}^{10}\), \(y \in \mathbb{R}^{10}\), and \(W \in \mathbb{R}^{10 \times 10}\) (for simplicity, we ignore the bias term). We draw \(x \in \mathbb{R}^{10}\) from a standard Gaussian distribution. To generate labels \(y\), we deliberately make \(W^*\) anisotropic by setting \(W^*_{ij} = s_i s_j v_{ij},\) where \(v_{ij}\) is drawn from a standard Gaussian, and \(s_i = 10^{-1 + 2 i / 9}, \quad (i = 0, 1, \dots, 9),\) which spans a wide range in \([0.1, 10]\). The loss function is the MSE loss, and we use the Adam optimizer with learning rate \(\eta = 0.01\).</p> <p>We compare the following eight modes:</p> <ul> <li>8 = [Scalar, Matrix (element-wise)] × [fixed, learnable] × [Re-parametrization, No re-parametrization]</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/linear_regression-480.webp 480w,/assets/img/blogs/optimization-2/linear_regression-800.webp 800w,/assets/img/blogs/optimization-2/linear_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Scalar normalization is faster than element-wise normalization in the early stage (compare green and pink).</li> <li>Re-parametrization helps during early training, but not as much—and can even slow things down—during later training (e.g., compare green and red; compare pink and gray).</li> <li>A learnable scale is essential for element-wise normalization (compare purple and pink; compare brown and gray).</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/19ykGEIlSxlnftuTAcycGoffVdFQ1j8aW?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-optimization-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 2 -- Elementwise scale reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 1 – Norm reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-1/" rel="alternate" type="text/html" title="Optimization 1 – Norm reparametrization"/><published>2026-01-23T00:00:00+00:00</published><updated>2026-01-23T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Recently, a number of architectures and optimizers have emphasized the role of normalization and/or learnable scales. To name a few:</p> <ul> <li>2026-01-21: Hyperball optimization (<a href="https://psychedelic-sunstone-851.notion.site/Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them-2-1-Hyperball-Optimization-2e924306e6f280e7a5ffee00eb40a0dd">link</a>)</li> <li>2026-01-13: Controlled LLM training on a spectral hypersphere (<a href="https://arxiv.org/pdf/2601.08393">Link</a>)</li> <li>2026-01-08: Learnable Multipliers (<a href="https://arxiv.org/pdf/2601.04890">Link</a>)</li> </ul> <p>Slightly less recent (last year :-) ), there are also the <a href="https://kellerjordan.github.io/posts/muon/">Muon optimizer</a> and <a href="https://arxiv.org/abs/2410.01131v1">NVIDIA’s nGPT</a>. These examples give a sense of how popular this direction has become.</p> <p>On the one hand, the intuition is clear: when the step size is fixed, constraining parameters to lie on a small (but not too small) sphere allows the optimizer to make reasonable <em>angular</em> updates. On the other hand, we typically do not know the optimal scale in advance, and in some cases the optimal scale may be very large—or may even be infinite (e.g., logistic regression on linearly separable data). As a result, we would like an optimizer that can make both effective <em>angular</em> updates and effective <em>radial (scale)</em> updates.</p> <p>To gain insight, we start with a toy example below.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>Assume the optimal (flattened) weight of a model is \(W^*\in\mathbb{R}^N\). The initial weight is \(W_0\in\mathbb{R}^N\), and the loss is the MSE loss between \(W\) and \(W^*\), i.e., \(L = \|W - W^*\|_2^2 .\) An Adam optimizer with learning rate \(\eta\) is used to minimize the MSE loss.</p> <p><strong>When the target \(W^*\) is very far away</strong>, such that \(R \equiv \|W^*\| \gg \|W_0\| ,\) the time to reach the target is roughly \(t = \frac{R}{\sqrt{N}\eta} ,\) assuming (1) Adam behaves like SignGD with step size \(\eta\) so that each update has norm \(\sqrt{N}\eta\), and (2) updates from different steps are approximately parallel. In summary, the time complexity is \(O(R)\), which is not very good when \(R\) is large.</p> <p><strong>Can we do better?</strong><br/> Yes, we can!</p> <ul> <li>By introducing a learnable norm scalar, the time can be reduced to \(O(\sqrt{R})\).</li> <li>By further using a re-parametrization trick, the time can be reduced to \(O(\log R)\).</li> </ul> <hr/> <h2 id="step-1-introducing-learnable-multipliers">Step 1: Introducing learnable multipliers</h2> <p>To control both the angular direction and the norm, it is natural to introduce a learnable multiplier \(s\). The actual weight \(W\) is parameterized as \(W = s \hat{W},\) where \(\hat{W}\) may or may not be normalized.</p> <p>When \(\hat{W}\) is learnable and not normalized, it is likely that \(|s| \to O(\sqrt{R}), \quad \|\hat{W}\| \to O(\sqrt{R}),\) since their roles are symmetric in the multiplication. Hence, the convergence time becomes \(t = O(\sqrt{R}/\eta).\) This is already better than \(O(R)\), but can we do even better?</p> <hr/> <h2 id="step-2-re-parametrization">Step 2: Re-parametrization</h2> <p>At each step, we compute the norm \(a \equiv \|\hat{W}\|_2\) and apply the following re-parametrization: \(\hat{W} \to \hat{W}/a, \quad s \to a s .\) This places \(\hat{W}\) on the unit hypersphere, representing the angular direction, while \(s\) captures the norm. Their product remains invariant under this re-parametrization. Training proceeds by interleaving gradient updates and re-parametrization steps.</p> <p>In each update, the norm of \(\hat{W}\) changes from 1 (due to re-parametrization in the previous step) to \(1 \pm A\eta\), where \(A = A(N)\) depends on \(N\) and the gradient directions, which we do not worry about here. The key point is that this is a <em>multiplicative</em> (rather than additive) change. Therefore, growing the norm from 1 to \(R\) only takes \(O(\log R)\) steps.</p> <hr/> <h2 id="experiment">Experiment</h2> <p>We conduct a 2D toy experiment. We set \(W_0 = (-50, 0), \quad W^* = (100, 100).\) We use Adam with learning rate \(\eta = 0.01\) and train for 3000 steps. We compare four modes:</p> <ul> <li><strong>no_scale</strong>: standard optimization (fixed multiplier \(s = 1\))</li> <li><strong>learnable_scale</strong>: learnable \(s\)</li> <li><strong>reparametrized_learnable_scale</strong>: re-parametrization, fix \(s = 1\) during the update step</li> <li><strong>reparametrized_fixed_scale</strong>: re-parametrization, allow \(s\) to update during the update step</li> </ul> <p>Optimization trajectory:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/trajectory-480.webp 480w,/assets/img/blogs/optimization-1/trajectory-800.webp 800w,/assets/img/blogs/optimization-1/trajectory-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/trajectory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Loss:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/loss-480.webp 480w,/assets/img/blogs/optimization-1/loss-800.webp 800w,/assets/img/blogs/optimization-1/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>As expected, re-parametrization &gt; learnable scale &gt; no scale.</li> <li>With re-parametrization, whether the scale \(s\) is learnable or fixed during the update step does not seem to make a noticeable difference.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Bx3ojrssLueShvJdU-ba7DQMWLFuqRVx?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-optimization-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 1 -- Norm reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 5 – Attention sink</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/" rel="alternate" type="text/html" title="Sparse attention 5 – Attention sink"/><published>2026-01-22T00:00:00+00:00</published><updated>2026-01-22T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-5</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>This article studies how an <strong>attention sink</strong> can emerge in a single-layer (1L), attention-only transformer.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> When all previous tokens are useless for predicting the next token, the ideal attention weights would be zero everywhere. However, due to softmax normalization, attention must be allocated somewhere. As a result, if a <code class="language-plaintext highlighter-rouge">[BOS]</code> token is present at the beginning of the sequence, it can serve as a reservoir that absorbs attention.</p> <p>We use the unigram toy dataset (defined in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>), where the next token does not depend on any previous tokens, but is instead drawn independently from a token-frequency distribution.</p> <p><strong>Model</strong><br/> A 1L attention-only transformer. For simplicity, we remove positional embeddings.</p> <hr/> <h2 id="experiment-1-unigram-dataset">Experiment 1: Unigram dataset</h2> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>We choose the embedding dimension to be 2 and the vocabulary size to be 101 (<code class="language-plaintext highlighter-rouge">[BOS]</code> + 100 tokens whose frequencies follow Zipf’s law). This configuration already performs the task perfectly (in fact, \(n_{\rm embd}=1\) suffices, as discussed in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>).</p> <p>The attention maps are shown below. Each row corresponds to a different sample, and each column corresponds to a different training step. Besides the <code class="language-plaintext highlighter-rouge">[BOS]</code> token, frequent tokens are also used to absorb attention.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram_bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram_bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram_bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram_bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The token embeddings for four different random seeds are shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/token_embd-480.webp 480w,/assets/img/blogs/sparse-attention-5/token_embd-800.webp 800w,/assets/img/blogs/sparse-attention-5/token_embd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/token_embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Recall that we set token frequencies to follow a Zipfian distribution, so small-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[0]</code>) appear more often than large-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[99]</code>). Interestingly, large-index tokens are closer to <code class="language-plaintext highlighter-rouge">[BOS]</code> than small-index tokens in embedding space. Since <code class="language-plaintext highlighter-rouge">[BOS]</code> is heavily attended to, this implies that large-index (infrequent) tokens receive more attention than small-index (frequent) tokens. This likely occurs because attention to frequent tokens is more strongly suppressed than attention to infrequent tokens.</p> <p>Indeed, it has been observed in large language models that their embedding spaces contain a direction correlated with token frequency. Our toy model offers one plausible explanation: this structure emerges for the same reason as the attention sink.</p> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>When <code class="language-plaintext highlighter-rouge">[BOS]</code> is not present, attention sinks still emerge, and these sinks correspond to frequent tokens (with low indices, e.g., <code class="language-plaintext highlighter-rouge">[0]</code>, <code class="language-plaintext highlighter-rouge">[1]</code>).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The five sequences are:</p> \[[ 1, 4, 88, 0, 6, 93, 1, 63, 9, 32]\] \[[ 3, 2, 57, 0, 5, 9, 3, 20, 67, 0]\] \[[ 1, 15, 1, 84, 0, 0, 5, 18, 53, 18]\] \[[ 5, 1, 7, 10, 6, 1, 5, 73, 2, 0]\] \[[ 1, 1, 17, 25, 46, 0, 0, 88, 0, 5]\] <hr/> <h2 id="experiment-2-bigram-dataset">Experiment 2: Bigram dataset</h2> <p>We now switch to a bigram toy dataset defined in <a href="/blog/2026/bigram-3/">bigram-3</a>. In this setting, the next token depends on the current token, so the ideal attention pattern would be a diagonal matrix.</p> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is nearly diagonal, except for splitting among identical previous tokens. In addition, some tokens incorrectly attend to the <code class="language-plaintext highlighter-rouge">[BOS]</code> token (e.g., the 3rd token in example 10 and the 3rd token in example 12).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-w-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-w-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is again almost diagonal, with the main deviation being splitting among identical previous tokens.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1f64J1efN-p_OCfcVo2oxbwYjeUEOT_bb?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-5</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 5 -- Attention sink}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-5/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 4 – On the difficulty of spatial map emergence</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-4/" rel="alternate" type="text/html" title="Bigram 4 – On the difficulty of spatial map emergence"/><published>2026-01-21T00:00:00+00:00</published><updated>2026-01-21T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In two previous posts (<a href="/blog/2026/bigram-1/">bigram-1</a> and <a href="/blog/2026/bigram-2/">bigram-2</a>), we studied a toy task—random walk on a circle. We showed that standard embeddings fail to learn the circular structure underlying the task. This raises a natural question: <strong>why is it hard for a spatial map to emerge?</strong></p> <p>In this article, we deliberately construct a network that can perform the task <em>perfectly</em> when its weights are set appropriately, yet still exhibits failure modes when the weights are randomly initialized and trained via gradient descent. We will analyze these failure modes and propose a simple fix.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We use the same dataset as in the previous blogs. Suppose there are \(p=10\) points on a circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong><br/> We now construct a network that can solve this task perfectly. Suppose there are \(p\) points in total. The \(i^{\rm th}\) point is embedded as<br/> \(E_i = A\bigl(\cos(\tfrac{2\pi i}{p}), \sin(\tfrac{2\pi i}{p})\bigr).\)</p> <p>To walk clockwise to the neighboring point, we need a rotation matrix with angle \(\theta=\frac{2\pi}{p}\): \(R(\theta)= \begin{pmatrix} \cos\theta &amp; \sin\theta \\ -\sin\theta &amp; \cos\theta \end{pmatrix},\) parameterized as \(W_1\).<br/> To walk counter-clockwise to the neighboring point, we need \(R(-\theta)\), parameterized as \(W_2\).</p> <p>To read out each location, the unembedding matrix shares the same weights as the embedding matrix. To create a bimodal distribution, we must <em>average two probability distributions</em> (not add two logits; note that this cannot be efficiently simulated by standard layers).</p> <p>In summary, when embeddings lie on a circle with a large radius \(A\), and when \(W_1\) and \(W_2\) learn the corresponding rotation matrices, the task can be solved perfectly.</p> <hr/> <h2 id="observation-random-initialization-fails-to-learn-a-circle">Observation: random initialization fails to learn a circle</h2> <p>We set \(n_{\rm embd}=2\) and \(p=29\). We show results from six different random seeds; in none of them does a circular structure emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/random_w_trained-480.webp 480w,/assets/img/blogs/bigram-4/random_w_trained-800.webp 800w,/assets/img/blogs/bigram-4/random_w_trained-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/random_w_trained.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From <a href="/blog/2026/bigram-2/">bigram-2</a>, we learned that there may exist <em>negative directions</em> in which points that are close in physical space are actually far apart in embedding space. This motivates the following experiments—<strong>fixing \(W_1\) and \(W_2\)</strong>.</p> <hr/> <h2 id="experiment-1">Experiment 1</h2> <p>We initialize \(W_1\) and \(W_2\) to simple matrices (corresponding to Euclidean, hyperbolic, and anti-Euclidean geometries) and keep them fixed during training.</p> <p><strong>First case</strong> – \(W_1=W_2=\mathrm{diag}(1,1)\):<br/> A circular structure can be learned for most random seeds. The perplexity (4.15) is still high because a token is closest to itself—the model therefore predicts the most probable next point to be the current point. However, the ground truth is either the left or the right neighbor, leading to high loss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/1_1-480.webp 480w,/assets/img/blogs/bigram-4/1_1-800.webp 800w,/assets/img/blogs/bigram-4/1_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/1_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Second case</strong> – \(W_1=W_2=\mathrm{diag}(1,-1)\):<br/> A global circular structure is visible, but it is locally incorrect (pairs of points collapse together), and the numerical order is not clean.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/1_-1-480.webp 480w,/assets/img/blogs/bigram-4/1_-1-800.webp 800w,/assets/img/blogs/bigram-4/1_-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/1_-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Third case</strong> – \(W_1=W_2=\mathrm{diag}(-1,-1)\):</p> <p>At first glance, a perfect circle seems to emerge. However, upon closer inspection, the ordering is incorrect—for example, 9 is opposite to 8/10 rather than being nearby. Interestingly, the perplexity is lower than in the first case. In fact, the perplexity (2.01) is close to the theoretical optimum of 2. This is because this embedding geometry places a token far from itself, whereas in the first case a token is closest to itself (corresponding to staying in place, which is not allowed by the task).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/-1_-1-480.webp 480w,/assets/img/blogs/bigram-4/-1_-1-800.webp 800w,/assets/img/blogs/bigram-4/-1_-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/-1_-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="experiment-2">Experiment 2</h2> <p>We set \(W_1=W_2=\mathrm{diag}(1,1)\) (injecting small noise to break symmetry) and allow them to be trainable. During training, the perplexity first converges to 4.14 (embeddings evolve while \(W_1/W_2\) remain nearly fixed), and then transitions to 2 (embeddings stabilize while \(W_1/W_2\) evolve).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/trainable_W-480.webp 480w,/assets/img/blogs/bigram-4/trainable_W-800.webp 800w,/assets/img/blogs/bigram-4/trainable_W-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/trainable_W.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <ul> <li>When a sequence of tokens is generated by an underlying <em>continuous process</em>, it may be beneficial to initialize projection matrices as the identity, allowing spatial locality to be learned.</li> <li>Does this initialization trick work for natural language? Not necessarily, for two reasons:<br/> (1) Although one may argue that neural activity (which determines speech) is continuous, the time scales differ—neural activity operates on millisecond scales, whereas natural language unfolds over seconds.<br/> (2) Natural language appears more discrete than continuous. The random-walk dataset is therefore not a good abstraction of language. Instead, we need <em>toy language datasets</em> that capture discrete dependency graphs.</li> <li>The toy model we studied in this article is non-standard, because it is specifically designed for the random walk task.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1B1mqAIva9WMDXGJCFdjJaOTeC78FBjMX?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 4 -- On the difficulty of spatial map emergence}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 1 – Understanding Pre-LN and Post-LN</title><link href="https://kindxiaoming.github.io/blog/2026/depth-1/" rel="alternate" type="text/html" title="Depth 1 – Understanding Pre-LN and Post-LN"/><published>2026-01-20T00:00:00+00:00</published><updated>2026-01-20T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>My line of reasoning is as follows.</p> <p>I want to understand DeepSeek’s <a href="https://arxiv.org/pdf/2512.24880">mHC paper</a>. To do that, I first need to understand ByteDance’s earlier <a href="https://arxiv.org/pdf/2409.19606">HC paper</a>.</p> <p>In the Hyper-connection paper, the introduction states:</p> <blockquote> <p>The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing (Bengio et al., 1994; Glorot &amp; Bengio, 2010). However, it can also lead to the issue of collapse in deep representations (Liu et al., 2020), where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers. This approach can alleviate the issue of representation collapse but also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer.</p> </blockquote> <p>In short: <strong>Pre-Norm tends to lead to representation collapse, while Post-Norm tends to lead to vanishing gradients</strong>. I have heard this conclusion many times, but I never really had the chance to internalize it. The goal of this article is therefore to construct a minimal toy model that helps me better understand these claims about Pre-Norm and Post-Norm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> This article focuses only on forward computations, so we do not need to worry about outputs. The inputs are i.i.d. samples drawn from a Gaussian distribution.</p> <p><strong>Model</strong><br/> The model consists of residual blocks, where each block is a simple MLP. Each MLP has two fully connected layers, FC1 and FC2. We explicitly control the scale (\(\alpha\)) of FC2 so that we can tune the relative importance of the residual update versus the input.</p> <p>In practice, during training, \(\alpha\) often increases from 1. Even though we never actually train the model here, we can still vary \(\alpha\) to gain intuition about how a trained model might behave. A large \(\alpha\) means that the update dominates the input, while a small \(\alpha\) means the update is small compared to the input.</p> <p>The architectures of Post-LN and Pre-LN are shown below.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-vs-pre-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN-vs-pre-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When the number of samples is much larger than the input dimension, the explained variances from PCA are roughly uniform, so the effective dimension is close to the input dimension. We are interested in how this effective dimension (and other statistics) evolve across layers.</p> <hr/> <h2 id="observation-1-pre-ln-and-representation-collapse">Observation 1: Pre-LN and representation collapse</h2> <p>We set the depth to \(L = 100\) layers. Let \(h^l\) denote the residual stream after the \(l\)-th block. We focus on the following quantities:</p> <ul> <li>the cosine similarity between \(h^l\) and \(h^{l+1}\)</li> <li>the norm of \(h^l\)</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/pre-norm-repr-collapse-480.webp 480w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-800.webp 800w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/pre-norm-repr-collapse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe two asymptotic behaviors:</p> <ul> <li> <p>\(\lVert h^l \rVert \sim \sqrt{l}\).<br/> Since the update at each block is random (due to random initialization), the evolution along the depth direction is effectively a random walk. Its variance grows linearly with \(l\), so the norm grows like \(\sqrt{l}\). This likely explains why <a href="https://arxiv.org/pdf/2206.03126">this paper</a> uses a \(1/\sqrt{L}\) scaling, in order to control the norm.</p> </li> <li> <p>\(1 - {\rm cos}(h^l, h^{l+1}) \sim 1/l\).<br/> The norm of the update is roughly constant, while the norm of \(h^l\) keeps growing. Therefore, the relative angular update satisfies \(\theta \sim 1/\lVert h^l \rVert \sim 1/\sqrt{l}\), which implies \(1 - {\rm cos}(h^l, h^{l+1}) \sim \theta^2 \sim 1/l\).</p> </li> </ul> <hr/> <h2 id="observation-2-post-ln-and-vanishing-influence-of-early-layers">Observation 2: Post-LN and vanishing influence of early layers</h2> <p>If we say that Pre-LN suffers from the problem that <em>later layers become “useless”</em>, then Post-LN suffers from the opposite problem: <em>early layers become “useless”</em>. Due to Post-LN, \(\lVert h^l \rVert\) remains approximately constant across layers. As a result, signals originating from early layers keep shrinking as they propagate forward, leading to vanishing influence on the output (and correspondingly, vanishing gradients).</p> <p>We verify that Post-LN does not exhibit strong representation collapse, in the sense that \({\rm cos}(h^l, h^{l_1})\) does not asymptotically approach 1 as the layer index increases. However, even when cosine similarity is not close to 1, the influence of early-layer representations can still decay rapidly as they propagate to later layers.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We also compute the effective rank of \(h^l\): we apply PCA to a point cloud of \(h^l\), normalize the singular values into a probability distribution, compute its entropy, and then exponentiate it. The results suggest that Pre-LN maintains representation rank better than Post-LN.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/eff-rank-480.webp 480w,/assets/img/blogs/depth-1/eff-rank-800.webp 800w,/assets/img/blogs/depth-1/eff-rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/eff-rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At first, this result confused me, because I had incorrectly equated “representation collapse” with “low representation rank.” In fact, what people usually mean by representation collapse in this context is <strong>strong alignment between the representations of consecutive layers</strong>, rather than a literal drop in rank.</p> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Gradient analysis:</strong> So far, we have only analyzed forward computations. How do gradients propagate across layers in this setup?</li> <li>Many papers (including the Hyper-connection paper) attempt to find a better trade-off between vanishing gradients and representation collapse. Is it possible to eliminate both? Or is there fundamentally no free lunch?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1KV3VFzaxtYTiDhU-kdltmu9Jx1_gJCDm?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 1 -- Understanding Pre-LN and Post-LN}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 3 – Low Rank Structure</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-3/" rel="alternate" type="text/html" title="Bigram 3 – Low Rank Structure"/><published>2026-01-19T00:00:00+00:00</published><updated>2026-01-19T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In large language models (LLMs), the embedding dimension is typically <strong>much smaller</strong> than the vocabulary size, yet models still perform remarkably well. Does this suggest that language itself possesses some kind of <strong>low-rank structure</strong>?</p> <p>In this article, we study a <strong>Bigram dataset</strong>, where each token depends only on the immediately preceding token. We further assume that the (log) transition probability matrix has a <strong>low-rank structure</strong>. This naturally raises the following question: <strong>Is it sufficient for the embedding dimension to match this rank, rather than scaling all the way up to the vocabulary size?</strong></p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> Let the vocabulary size be \(V\) and the rank be \(R\). We generate two random matrices \(A\in\mathbb{R}^{V\times R}, \quad B\in\mathbb{R}^{R\times V}.\) Their matrix product \(L = AB/\sqrt{R}\) is a low-rank matrix. The factor \(1/\sqrt{R}\) ensures that the scale of \(L\) is independent of \(R\). Applying a row-wise softmax to \(L\) yields the transition matrix \(P = {\rm Softmax}(L, {\rm dim}=1).\)</p> <p>From the transition matrix, we can compute the steady-state distribution \(\pi\), which is interpreted as the token frequency (i.e., the unigram distribution). To generate a batch of data, we proceed in two steps:</p> <ul> <li><strong>Step 1:</strong> sample the input token from the unigram distribution.</li> <li><strong>Step 2:</strong> sample the output token from the transition matrix, conditioned on the input token.</li> </ul> <p>The best achievable loss \(L_0\) is given by the <strong>conditional entropy</strong>, averaged over input tokens.</p> <p>We can introduce additional knobs to control the dataset:</p> <ul> <li>Instead of assuming all \(R\) ranks are equally important, we can assign different weights by inserting a diagonal matrix between \(A\) and \(B\): \(AB \;\to\; A\Lambda B.\)</li> <li>We can also control the overall scale of the logit matrix. When this scale is large, the dataset becomes more deterministic.</li> </ul> <p><strong>Model</strong><br/> Our model consists only of an <strong>Embedding layer</strong> and an <strong>Unembedding layer</strong>, whose weights are <strong>not tied</strong>. The main quantity of interest in this article is the embedding dimension \(N\).</p> <hr/> <h2 id="observation-1-critical-embedding-dimension-n_capprox-r">Observation 1: critical embedding dimension \(N_c\approx R\)</h2> <p>We set \(V=10\) and \(R=3\). We expect that when \(N=N_c=3\), the model can achieve the optimal loss \(L_0\), whereas for \(N&lt;N_c\) the loss should be strictly higher. This is indeed what we observe.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/critical_dim-480.webp 480w,/assets/img/blogs/bigram-3/critical_dim-800.webp 800w,/assets/img/blogs/bigram-3/critical_dim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/critical_dim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="observation-2-scaling-laws-in-the-regime-nr">Observation 2: scaling laws in the regime \(N&lt;R\)</h2> <p>We now set \(V=100\) and \(R=20\), and sweep \(N\) from 1 to 20. Defining the loss gap \(\Delta \equiv L - L_0,\) we find that it closely follows a scaling law \(\Delta \sim N^{-1}.\) This can be viewed as a generalization of the result reported in <a href="https://arxiv.org/abs/2505.10465">this paper</a>.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/scaling_law-480.webp 480w,/assets/img/blogs/bigram-3/scaling_law-800.webp 800w,/assets/img/blogs/bigram-3/scaling_law-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/scaling_law.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Loss analysis:</strong> which token(s) incur the largest loss?</li> <li><strong>Training dynamics:</strong> how do the embeddings evolve during training?</li> <li><strong>Architecture choices:</strong> how do weight sharing, attention, MLPs, and layer normalization affect the results?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/18QrrL4LOwpgQ4ffxe_vz_CO0tEKWtAqg?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 3 -- Low Rank Structure}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 2 – Emergence of Hyperbolic Spaces</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-2/" rel="alternate" type="text/html" title="Bigram 2 – Emergence of Hyperbolic Spaces"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/bigram-1/">yesterday’s blog post</a>, we studied a simple Markov chain—a random walk on a circle. We found that when the embedding dimension is 2, the model fails to perform the task (even though a circle can, in principle, be perfectly embedded in 2D). However, when the embedding dimension is increased to 4, the model can solve the task perfectly. At the time, we did not understand <em>what</em> the 4D solution was actually doing. This article is an attempt to understand the mechanism behind this 4D algorithm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We use the same dataset as in the previous blog. Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>The model consists only of an Embedding layer, an Unembedding layer (with tied weights), and a linear layer \(A\) in between. This is equivalent to an Attention layer with context length 1, where \(A = OV\).<br/> When there is no linear layer \(A\) (or equivalently \(A = I\)), the model completely fails to perform this task. In that case, the model most likely repeats the current token, rather than predicting the token to its left or right (we will discuss this more carefully at the end of the article, but for now we take this as a given fact). Therefore, the linear layer \(A\) is necessary.</p> <hr/> <h2 id="observation-1-a-is-symmetric-and-has-negative-eigenvalues-hyperbolic-directions">Observation 1: \(A\) is symmetric, and has negative eigenvalues (hyperbolic directions)</h2> <p>When \(n_{\rm embd} = 4\), the perplexity can go down to 2 (the best achievable perplexity). By directly inspecting the embeddings or their PCA projections, we fail to observe any obvious pattern.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/embd-480.webp 480w,/assets/img/blogs/bigram-2/embd-800.webp 800w,/assets/img/blogs/bigram-2/embd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, we notice that the linear matrix \(A\) is close to a symmetric matrix:</p> \[A = \begin{pmatrix} -3.3250 &amp; -2.0423 &amp; 2.0838 &amp; -3.5954 \\ -2.0604 &amp; -3.2431 &amp; -2.8658 &amp; 2.8647 \\ 2.0648 &amp; -2.8236 &amp; -2.5492 &amp; -2.8768 \\ -3.5372 &amp; 2.7750 &amp; -2.8383 &amp; -1.5314 \\ \end{pmatrix}\] <p>A symmetric matrix can be diagonalized over the real numbers. The four eigenvalues consist of two positive and two negative values: \([-5.88, -5.41, 5.42, 6.40],\) all of comparable magnitude. It is therefore more natural to project the embeddings onto the eigen-directions:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/eigen-480.webp 480w,/assets/img/blogs/bigram-2/eigen-800.webp 800w,/assets/img/blogs/bigram-2/eigen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/eigen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the two negative-eigenvalue directions are highly oscillatory (adjacent tokens have opposite signs), whereas the two positive-eigenvalue directions are much smoother (adjacent tokens have similar values).</p> <p>Consider an input token \(m\) with embedding \(E_m\). Its logit for token \(n\) is given by \(E_n^T A E_m\), which is a quadratic form. Along positive eigen-directions, the logit is <em>lower</em> if two embeddings have opposite signs. In contrast, along negative eigen-directions, the logit is <em>higher</em> if two embeddings have opposite signs. The coexistence of positive and negative eigenvalues effectively turns the embedding space into a hyperbolic space, which can strongly conflict with our Euclidean geometric intuition.</p> <p>More formally, for two vectors \(x, y\) in this space, their similarity can be written as \(L(i,j) \sim -x_1 y_1 - x_2 y_2 + x_3 y_3 + x_4 y_4.\)</p> <p>For nearby tokens, we want the similarity to be large. This can be achieved by having opposite signs along the negative eigen-directions, and the same signs (similar values) along the positive eigen-directions. This mixed strategy makes interpretation difficult: although tokens \(i\) and \(i+1\) correspond to nearby points on the circle, they are not necessarily close in the embedding space, because the negative eigen-directions actively push them as far apart as possible.</p> <p>Two remarks are in order:</p> <p><strong>\(A\) is not necessarily symmetric.</strong><br/> In our case, \(A\) is symmetric because the Markov process we study is reversible. In general, \(A\) need not be symmetric, and it is unclear how to deal with a non-symmetric \(A\), where the left and right eigenspaces are no longer aligned.</p> <p><strong>More to understand.</strong><br/> So far, we have established that the embedding space is hyperbolic, which is already a somewhat surprising result with potentially significant implications for interpretability. However, many details are still missing, in particular how the tokens are arranged <em>quantitatively</em> within this hyperbolic space.</p> <hr/> <h2 id="observation-2-geometry-matters">Observation 2: Geometry matters</h2> <p>We find that \(n_{\rm embd} = 4\) works (i.e., the perplexity converges to 2) only for lucky random seeds. For unlucky random seeds, the perplexity instead converges to around 2.13. What distinguishes these cases? We observe that the geometry—specifically, the number of negative directions—is highly correlated with performance.</p> <p>When there are 2 or 3 negative directions, the optimal perplexity is achievable. With only 1 negative direction, optimization appears to get stuck in a local minimum.</p> <table class="table table-bordered"> <thead> <tr> <th>Random Seed</th> <th>Perplexity</th> <th>Eigenvalues</th> <th>Negative Number</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>2.13</td> <td>-7.6, 5.6, 5.7, 6.7</td> <td>1</td> </tr> <tr> <td>1</td> <td>2.13</td> <td>-7.7, 5.1, 6.1, 6.2</td> <td>1</td> </tr> <tr> <td>2</td> <td>2.13</td> <td>-7.2, 5.7, 6.4, 6.5</td> <td>1</td> </tr> <tr> <td>3</td> <td>2.00</td> <td>-6.1, -5.8, -4.4, 5.6</td> <td>3</td> </tr> <tr> <td>4</td> <td>2.00</td> <td>-5.9, -5.4, 5.4, 6.4</td> <td>2</td> </tr> <tr> <td>5</td> <td>2.00</td> <td>-6.7, -6.0, -5.7, 6.5</td> <td>3</td> </tr> <tr> <td>6</td> <td>2.10</td> <td>-7.0, 5.3, 5.7, 6.2</td> <td>1</td> </tr> <tr> <td>7</td> <td>2.00</td> <td>-5.7, -5.2, -4.9, 5.4</td> <td>3</td> </tr> <tr> <td>8</td> <td>2.00</td> <td>-5.8, -4.8, 4.6, 5.9</td> <td>2</td> </tr> <tr> <td>9</td> <td>2.10</td> <td>-7.0, 5.1, 6.1, 7.1</td> <td>1</td> </tr> <tr> <td>42</td> <td>2.10</td> <td>-7.1, 6.0, 6.3, 6.8</td> <td>1</td> </tr> <tr> <td>2026</td> <td>2.00</td> <td>-5.1, -4.8, 4.7, 5.8</td> <td>2</td> </tr> </tbody> </table> <hr/> <h2 id="sanity-check-why-do-we-need-a">Sanity check: why do we need \(A\)?</h2> <p>When we remove \(A\) or set \(A = I\), the perplexity remains far above 2, regardless of how large the embedding dimension is. When \(A = I\) (so the embedding space is purely Euclidean), an input token has no mechanism to “un-attend” to itself. In contrast, a negative eigen-direction can serve precisely this un-attention role, a mechanism also explored in <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>. As a result, \(A\) is essential for performing the random-walk task.</p> <p>That said, when \(A = I\) (i.e., attention is completely removed) and \(n_{\rm embd} = 2\), the embeddings can evolve into extremely wild (and aesthetically pleasing) patterns. Even though these patterns may not shed much light on what is really happening in language models, it is pure pleasure to watch the resulting animations.</p> <p>Seed = 0</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_0-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_0-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 1</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_1-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_1-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 4</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_4-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_4-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 6</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_6-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_6-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 9</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_9-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_9-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_9.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NmfqzshitvHwZZ3RNobwlJNH2Ebrlqj0?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 2 -- Emergence of Hyperbolic Spaces}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 1 – Walk on a Circle</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-1/" rel="alternate" type="text/html" title="Bigram 1 – Walk on a Circle"/><published>2026-01-15T00:00:00+00:00</published><updated>2026-01-15T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Markov chains are simple yet remarkably powerful structures. A natural question is therefore: <strong>can transformers learn Markov chains efficiently?</strong> For a first-order Markov chain, the next state depends only on the current state, which is equivalent to a <strong>bigram</strong> structure. This post is the first in a new <em>Bigram</em> series, where we will feed various bigram datasets to transformers and study their behavior.</p> <p>In this article, we focus on one of the simplest nontrivial cases: a <strong>random walk on a circle</strong>.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong></p> <p>We consider a random walk on a 1D circular manifold, obtained via the following simplifications:</p> <ul> <li>from a 2D Manhattan map to a 2D grid (ignoring, for example, diagonal streets),</li> <li>from a 2D grid to a 1D grid,</li> <li>imposing periodicity (the added symmetry may simplify analysis).</li> </ul> <p>Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>Since the next token depends only on the current token, a context length of 1 is sufficient. This allows us to greatly simplify the model:</p> <ul> <li>Context length = 1</li> <li>No positional embedding (positional information is unnecessary when the context length is 1)</li> </ul> <p>The dataset thus reduces to a pure bigram problem. The model consists of</p> <ul> <li>a single attention layer (1L), no MLP,</li> <li>embedding and unembedding layers with tied weights.</li> </ul> <p>Because the context length is 1, only the <strong>OV matrix</strong> plays a role. Without the OV matrix, and with tied embedding/unembedding weights, each token can only map to itself, making the random-walk task impossible.</p> <p>Our primary interest is in visualizing the evolution of the embedding vectors, as these can reveal whether a <em>world model</em>—namely, the circle—is being learned. If such a world model is learned, we would expect the tokens \(0, 1, 2, \cdots, 10\) to arrange themselves along a circle in embedding space.</p> <p>We will vary the embedding dimension \(n_{\rm embd}\). In principle, an embedding dimension of 2 should suffice, since a circle can be naturally embedded in 2D Euclidean space.</p> <hr/> <h2 id="observation-1-n_rm-embd--2-is-not-enough">Observation 1: \(n_{\rm embd} = 2\) is not enough</h2> <p>We fix the vocabulary size to 10.</p> <p>For \(n_{\rm embd} = 2\), the left plot below shows that the perplexity only decreases to about 3—corresponding to confusion among roughly three choices—whereas the optimal perplexity is 2. This indicates that \(n_{\rm embd} = 2\) is insufficient. The right plot shows that the embedding vectors do learn some notion of continuity (for example, 4 is close to 3 and 5, and 9 is close to 0 and 8), but the overall geometry is inconsistent (for instance, 1/7 lies diagonally relative to 2/6).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_2-480.webp 480w,/assets/img/blogs/bigram-1/embd_2-800.webp 800w,/assets/img/blogs/bigram-1/embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In contrast, \(n_{\rm embd} = 4\) is sufficient to reach the optimal perplexity of 2.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_234-480.webp 480w,/assets/img/blogs/bigram-1/embd_234-800.webp 800w,/assets/img/blogs/bigram-1/embd_234-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_234.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Moreover, the required embedding dimension appears to be independent of vocabulary size. For example, with vocab sizes of 100 or even 1000, the model still achieves the optimal perplexity of 2 with \(n_{\rm embd} = 4\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_4-480.webp 480w,/assets/img/blogs/bigram-1/embd_4-800.webp 800w,/assets/img/blogs/bigram-1/embd_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, once the embedding dimension reaches 4, the learned geometry becomes harder to interpret directly. Further study is needed. One possible hypothesis involves <em>nearest-neighbor hopping</em>: if we allow hopping to the nearest \(k\) neighbors, perhaps the required embedding dimension scales as \(2k\).</p> <p>To probe this further, we now study an even simpler dataset with a deterministic drift.</p> <hr/> <h2 id="observation-2-existence-of-many-equivalent-embeddings">Observation 2: existence of many equivalent embeddings</h2> <p><strong>Dataset</strong> We restrict the walk to move only to the right. For example, \([5]\) can only transition to \([6]\) and never to \([4]\). In this case, the prediction task becomes deterministic. We find that \(n_{\rm embd} = 2\) is sufficient to achieve zero loss and perplexity 1.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/drift_embd_2-480.webp 480w,/assets/img/blogs/bigram-1/drift_embd_2-800.webp 800w,/assets/img/blogs/bigram-1/drift_embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/drift_embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Interestingly, there are many equivalent embedding configurations that solve this task, and the model converges to one of them depending on initialization.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_zoo-480.webp 480w,/assets/img/blogs/bigram-1/embd_zoo-800.webp 800w,/assets/img/blogs/bigram-1/embd_zoo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_zoo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/18QzDHiSfVYcO3HhyY5Y3uPup0hybWNTh?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 1 -- Walk on a Circle}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Diffusion 1 – Sparse and Dense Neurons</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-1/" rel="alternate" type="text/html" title="Diffusion 1 – Sparse and Dense Neurons"/><published>2026-01-14T00:00:00+00:00</published><updated>2026-01-14T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading <a href="https://arxiv.org/abs/2506.01912">this paper, <em>“Unconditional CNN denoisers contain sparse semantic representation of images”</em></a>, which reports an intriguing observation: some neurons (CNN channels) are sparsely activated, while others are densely activated. The authors interpret this as follows: sparse neurons correspond to specific semantic features (e.g., a dog), and therefore activate only when that feature is present in the input image; dense neurons, by contrast, capture more global properties such as lighting, and hence activate across almost all images.</p> <p>The goal of this article is to propose a <strong>minimal toy setup</strong> in which such a division of labor naturally emerges.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We consider a simple scenario in which a “dog” may or may not appear under various “lighting conditions.” We model the dog as a Gaussian packet and the lighting as a constant offset. An “image” (a 1D function) is therefore given by \(I(x) = L + A {\rm exp}(-\frac{-(x-x_c)^2}{2 w^2}),\) where \(L\sim U[0,0.5]\), \(A\sim {\rm Bern}(p)\) (with probability \(p=0.2\) of being 1 and probability \(1-p=0.8\) of being 0), \(x_c\sim U[-0.8,0.8]\), and \(w=0.05\). We sample 100 such “images.”</p> <p><strong>Model</strong><br/> We adopt a 1D U-Net architecture. The input dimension is 128. The network consists of 3 downsampling blocks (each producing 8 output channels) and 3 upsampling blocks. We deliberately choose a small number of channels (8), since the task is simple and our goal is to understand what each individual channel is doing. The U-Net is trained as a denoiser with noise level \(\sigma = 0.1\) (again for simplicity).</p> <hr/> <h2 id="output-of-the-first-downsampling-block">Output of the first downsampling block</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block1-480.webp 480w,/assets/img/blogs/diffusion-1/block1-800.webp 800w,/assets/img/blogs/diffusion-1/block1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Each column corresponds to one neuron (channel), for a total of 8 channels.</p> <ul> <li><strong>First row</strong>: activation distribution of each neuron over the dataset.</li> <li><strong>Second row</strong>: activation versus lighting.</li> <li><strong>Third row</strong>: activation versus \(A\) (1 when the Gaussian is present, 0 otherwise).</li> </ul> <p>We find four distinct types of neurons:</p> <ul> <li><strong>Dead neurons</strong>: neurons 1, 2, 3, and 6.</li> <li><strong>Pure lighting neuron</strong>: neuron 0.</li> <li><strong>Pure Gaussian-detection neurons</strong>: neurons 4 and 7.</li> <li><strong>Mixed lighting + Gaussian detection neuron</strong>: neuron 5.</li> </ul> <p>We also include results for blocks 2 and 3.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block2-480.webp 480w,/assets/img/blogs/diffusion-1/block2-800.webp 800w,/assets/img/blogs/diffusion-1/block2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In block 2, neurons 2–7 are all dead. Neuron 0 is selective to the Gaussian, while neuron 1 exhibits mixed behavior.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block3-480.webp 480w,/assets/img/blogs/diffusion-1/block3-800.webp 800w,/assets/img/blogs/diffusion-1/block3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In block 3, neurons 0, 2, and 5 are dead. Neurons 1 and 3 show mixed behavior, while neurons 4, 6, and 7 are selective to the Gaussian.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1tHISSSA3ldoY2wTTfAMtD6J1H2BlPC_a?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Diffusion 1 -- Sparse and Dense Neurons}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 4 – previous token head</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-4/" rel="alternate" type="text/html" title="Sparse attention 4 – previous token head"/><published>2026-01-13T00:00:00+00:00</published><updated>2026-01-13T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/sparse-attention-2/">sparse-attention-2</a>, we found that a single-layer attention model <strong>without positional embeddings cannot</strong> reliably copy any earlier token based on position. In this article, we demonstrate how positional embeddings enable the model to learn a <em>previous-token head</em>.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to copy the previous token. Taking context length = 4 as an example, this corresponds to<br/> \([A][B][C][D] \rightarrow [C]\).</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>, with the addition of a positional embedding layer. The model consists only of a Token Embedding layer, a Positional Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers.</p> <hr/> <h2 id="with-positional-embeddings-the-previous-token-head-can-be-easily-learned">With positional embeddings, the previous-token head can be easily learned</h2> <p>We choose context length 4, vocab size 30, and embedding dimension 2. The left plot shows that the task cannot be learned without positional embeddings. The middle plot shows that the task can be reasonably learned with positional embeddings. The right plot shows the evolution of the positional embeddings: the positional embedding of the previous token (-1) moves in the opposite direction from tokens at other positions (0, -2, -3). The separation direction is roughly \(s = (1,1)^T\). When projecting positional embeddings along \(s\), \(p_{-1}\) is negative, while \(p_0, p_{-2}, p_{-3}\) are positive.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/compare_pos-480.webp 480w,/assets/img/blogs/sparse-attention-4/compare_pos-800.webp 800w,/assets/img/blogs/sparse-attention-4/compare_pos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/compare_pos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="whats-happening">What’s happening?</h2> <p>We can compute the \(W_QW_K^T\) matrix, obtaining</p> \[W_QW_K^T = \begin{pmatrix} -0.41 &amp; 1.35 \\ 0.19 &amp; -2.25 \\ \end{pmatrix}.\] <p>Note that \(s^T W_QW_K^T s = -1.1 &lt; 0\). If two positional embeddings have the same (opposite) sign along \(s\), they will receive less (more) attention. As a result, since only \(p_{-1}\) has the opposite sign relative to \(p_0\), the attention is biased toward the previous token.</p> <hr/> <h2 id="hyperparameter-dependence">Hyperparameter dependence</h2> <p>However, the task is not solved exactly, but only approximately. With larger vocab size or larger context length, the task becomes harder for the model to approximate, so the relative perplexity \(({\rm perplexity} - 1)/V\) increases. In contrast, a larger embedding dimension helps reduce the relative perplexity.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/hyper-dependence-480.webp 480w,/assets/img/blogs/sparse-attention-4/hyper-dependence-800.webp 800w,/assets/img/blogs/sparse-attention-4/hyper-dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/hyper-dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I want to argue that the need for higher embedding dimensions suggests inefficiency. Ideally, a 1D positional embedding should suffice if the attention kernel is chosen appropriately (here the attention kernel is the inner product).</p> <hr/> <h2 id="dependence-on-learning-rate">Dependence on learning rate</h2> <p>When \(V = 30\), a learning rate of 0.1 is faster than 0.01. However, when \(V = 100\), a learning rate of 0.1 leads to slower learning than 0.01.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/lr_dependence-480.webp 480w,/assets/img/blogs/sparse-attention-4/lr_dependence-800.webp 800w,/assets/img/blogs/sparse-attention-4/lr_dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/lr_dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It appears that lr = 0.1 still learns the previous-token head (since there exists a separation direction in the positional embeddings):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/big_lr-480.webp 480w,/assets/img/blogs/sparse-attention-4/big_lr-800.webp 800w,/assets/img/blogs/sparse-attention-4/big_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/big_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, because the learning rate is too large, the token embeddings fluctuate wildly and fail to converge to a maximally separable solution (nearby points are placed equidistantly on a circle). The learning rate is so large that tokens swap positions. The loss spike around 6000 steps corresponds to this swapping process, during which the model is <strong>confidently wrong</strong> for some tokens, leading to very large losses. This GIF illustrates the behavior more clearly:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/gif2-480.webp 480w,/assets/img/blogs/sparse-attention-4/gif2-800.webp 800w,/assets/img/blogs/sparse-attention-4/gif2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/gif2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that the tokens in this task have no semantic meaning. The reason they form a circle is simply maximal distinguishability, and their ordering is random. The loss spike corresponds to jumping from one order to another order.</p> <hr/> <h2 id="learning-rate-decay">Learning rate decay</h2> <p>The observation above suggests a possible reason for why <strong>learning rate decay</strong> is needed. When two token embeddings are very close to each other, the learning rate should be small enough so that (i) swampping cannot happen (be trapped in one basin of attraction), otherwise creating loss spikes and (ii) can converge smoothly to the bottom of the basin of attraction (maximal separation of token embeddings). In this article, all tokens have the same frequency so they form a circle due to symmetry. But for natural languages, tokens have different frequencies and so different token embeddings may have different norms, requiring different learning rates. How we can adjust learning rates based on token frequency (which can be easily known) is investigated in future posts.</p> <hr/> <h2 id="generality">Generality</h2> <ul> <li>Although we exemplify the analysis with the previous token (the token right before the current token), the analysis applies to any earlier token at any position, e.g., 3 tokens away in the past.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1AFKB8DcToRncxwE2vI_g0tkLzHg02iRm?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 4 -- previous token head}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>