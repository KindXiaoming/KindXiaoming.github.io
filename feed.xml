<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-28T08:20:13+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">MLP 1 – Gating is good for polynomials</title><link href="https://kindxiaoming.github.io/blog/2026/mlp-1/" rel="alternate" type="text/html" title="MLP 1 – Gating is good for polynomials"/><published>2026-01-28T00:00:00+00:00</published><updated>2026-01-28T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/mlp-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/mlp-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Gated MLPs are widely used in modern language models. Given an input \(x\), the hidden representation of a gated MLP is \(\sigma(W_g x) \odot W_v x .\) It is often argued that gating promotes sparsity, multiplicative structure, conditional computation, and more stable training. This is far too large a topic to be fully explored in a single blog post. In this article, we narrow the scope to polynomial fitting. If learning a Taylor expansion is important for neural networks, then learning polynomials is a necessary prerequisite.</p> <hr/> <h2 id="polynomials">Polynomials</h2> <p>For both ReLU and SiLU activations, gated MLPs (green) consistently outperform non-gated MLPs (blue) when fitting \(x^n\) for all \(n \ge 2\). We also compare against non-gated MLP but with the activation function squared (orange), and find that squared activation functions can achieve strong performance—sometimes even better than gated networks.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-1/poly-480.webp 480w,/assets/img/blogs/mlp-1/poly-800.webp 800w,/assets/img/blogs/mlp-1/poly-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-1/poly.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="a-rough-equivalence">A rough equivalence</h2> <p>We observe that as \(\epsilon \to 0^+\), a Taylor expansion gives \(\sigma((w+\epsilon)x) - \sigma((w-\epsilon)x) \approx 2\epsilon x \sigma'(wx) .\) This suggests a rough “equivalence” between \(\sigma(x)\) and \(x \sigma'(x)\), where \(x \sigma'(x)\) can be interpreted as a gate. For gated ReLU networks, since \(\sigma'(x) = {\rm ReLU}\), this implies \(\sigma(x) = {\rm ReLU}^2\). In other words, a gated ReLU network is roughly equivalent to a non-gated ReLU2 network.</p> <p>So far, this is not meant to be mathematically rigorous—just a hand-wavy but potentially useful intuition. Once we have a useful gate, it can be converted to a useful activation function for free (except for doing integration).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Dqn6dkxW0hP04s30F3zbT3wYZivsXv65?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026mlp-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MLP 1 -- Gating is good for polynomials}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/mlp-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 4 – Loss Spikes</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-4/" rel="alternate" type="text/html" title="Optimization 4 – Loss Spikes"/><published>2026-01-27T00:00:00+00:00</published><updated>2026-01-27T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>During neural network training, we often observe sudden loss spikes. The goal of this article is to construct a minimal model that helps us understand the origin of such loss spikes.</p> <hr/> <h2 id="2d-toy-model">2D toy model</h2> <p>We aim to regress the function \(y(x) = x^2\) using an MLP. I first found that a 2-layer MLP is sufficient to produce loss spikes. Then I found that a width of 2 is already enough. Further, removing biases does not eliminate the spikes. At this point, the model has only four parameters, and clear symmetries emerge: the two weights in the first layer are almost identical up to a sign flip, and the two weights in the second layer are nearly the same.</p> <p>After all these simplifications, we are naturally led to study the following toy model, which has only two parameters \((a, w)\): \(f(x) = a \bigl(\sigma(wx) + \sigma(-wx)\bigr),\ \sigma(x) = {\rm silu}(x) = \frac{x}{1+e^{-x}}\)</p> <p>We train this “model” to fit the squared function for \(x \in [-1, 1]\).</p> <hr/> <h2 id="loss-spikes">Loss spikes</h2> <p>We initialize \(w = 1\) and \(a = 0\), and train using the Adam optimizer (learning rate 0.1) for 5000 steps. The left plot shows that the training loss exhibits spikes. When zoomed in, each spike is asymmetric: a rapid instability followed by a gradual recovery. The right plot shows the training trajectory, which appears to consist of two phases whose dominant movement directions are nearly orthogonal. Loss spikes occur in the second phase, where the parameters seem to navigate a long, narrow valley—known as a <a href="https://arxiv.org/abs/2410.05192">river-valley landscape</a>.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/loss-spike-480.webp 480w,/assets/img/blogs/optimization-4/loss-spike-800.webp 800w,/assets/img/blogs/optimization-4/loss-spike-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/loss-spike.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="loss-landscape">Loss landscape</h2> <p>We further verify the presence of a river-valley landscape:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/loss-landscape-480.webp 480w,/assets/img/blogs/optimization-4/loss-landscape-800.webp 800w,/assets/img/blogs/optimization-4/loss-landscape-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/loss-landscape.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Loss spikes occur when the effective learning rate becomes too large relative to the edge-of-stability threshold \(2/C\), where \(C\) denotes the curvature (see, e.g., <a href="https://arxiv.org/abs/2103.00065">edge of stability</a> and the <a href="https://arxiv.org/abs/2206.04817">slingshot mechanism</a>). We measure that the curvature at the bottom of the valley is approximately proportional to \(a\) (see the left plot below). As \(a\) increases, \(C\) increases, making spikes more likely.</p> <hr/> <h2 id="periodicity">Periodicity</h2> <p>After the first spike, subsequent spikes appear periodically, roughly every 250 steps. As discussed above, the occurrence of a spike depends on two factors: the effective learning rate and the curvature. For Adam, the effective learning rate depends on the second-moment accumulator, <code class="language-plaintext highlighter-rouge">exp_avg_sq</code>, which is shown in the middle plot below. Aside from the initial growth and the spikes themselves, <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> decays exponentially as \(0.999^t\), where \(0.999\) is Adam’s \(\beta_2\).</p> <p>A plausible explanation for the spike periodicity is therefore the following: each spike amplifies <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> by some constant factor, and the interval between spikes corresponds to the time it takes for this factor to decay back to 1 (or slightly above 1, since the curvature increases slowly over time). The curvature is roughly a linear function of \(a\) (left plot), and Adam statistics are also shown in the right plot.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/adam-480.webp 480w,/assets/img/blogs/optimization-4/adam-800.webp 800w,/assets/img/blogs/optimization-4/adam-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/adam.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If this explanation is correct, we would expect a larger \(\beta_2\) to result in less frequent spikes, which is indeed what we observe.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/beta2-480.webp 480w,/assets/img/blogs/optimization-4/beta2-800.webp 800w,/assets/img/blogs/optimization-4/beta2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/beta2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="video">Video</h2> <p>A close-up view of how a spike unfolds:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-4/spike_animation-480.webp 480w,/assets/img/blogs/optimization-4/spike_animation-800.webp 800w,/assets/img/blogs/optimization-4/spike_animation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-4/spike_animation.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It’s interesting to observe that the parameter could temporarily go backwards along the river during the spike, probably due to the <a href="https://arxiv.org/abs/2505.10559">entropic force</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1goCzaQj9JAhvI4jJ-m77gsA0SAHRzwDR?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 4 -- Loss Spikes}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 3 / Depth 2 – Adding Bias After ReLU</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-3/" rel="alternate" type="text/html" title="Optimization 3 / Depth 2 – Adding Bias After ReLU"/><published>2026-01-25T00:00:00+00:00</published><updated>2026-01-25T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I have been playing with the toy model of residual networks defined in <a href="/blog/2026/depth-1/">depth-1</a>. There is no specific goal—just exploratory poking around.</p> <hr/> <h2 id="representation-collapse">Representation collapse</h2> <p>Although the input \(x\) to the deep network is drawn from an isotropic Gaussian distribution, I observe that the output \(y\) appears to have a clearly non-zero mean across the batch dimension.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/print_y-480.webp 480w,/assets/img/blogs/optimization-3/print_y-800.webp 800w,/assets/img/blogs/optimization-3/print_y-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/print_y.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The histogram confirms this observation: while the mean of \(x\) is close to 0, the mean of \(y\) clearly deviates from 0.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/hist_x_y_mean-480.webp 480w,/assets/img/blogs/optimization-3/hist_x_y_mean-800.webp 800w,/assets/img/blogs/optimization-3/hist_x_y_mean-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/hist_x_y_mean.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To quantify this deviation, we define \(\Delta\) as the standard deviation (across width) of the mean (across data points) of the output \(y\) at initialization.</p> <p>We make two comments regarding the fact that \(\Delta &gt; 0\):</p> <ul> <li><strong>Origin:</strong> We hypothesize that this effect arises from ReLU inducing distribution shifts (see <a href="/blog/2026/activation-anisotropy/">activation anisotropy</a>).</li> <li><strong>Judgment:</strong> This behavior is undesirable, and we would like to eliminate it (i.e., make \(\Delta \sim 0\)).</li> </ul> <p>This raises the question: how can we eliminate it, and is it truly a feature or a bug?</p> <hr/> <h2 id="trick-adding-bias-after-relu">Trick: adding bias after ReLU</h2> <p>Since the issue originates from ReLU shifting distributions toward the positive side, we can counteract this effect by adding a negative bias \(b\) after ReLU. Equivalently, the activation function becomes \({\rm ReLU}(\cdot) - b\).</p> <p>We use a teacher–student setup. The teacher network is 10 layers deep, with input dimension 50 and hidden dimension 200. It uses post-norm and sets \(b = 0\). The student network shares the same architecture, but we sweep over different values of \(b\). Student networks are trained using the Adam optimizer with learning rate \(10^{-3}\) for 1000 steps. The results are shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-3/loss_delta-480.webp 480w,/assets/img/blogs/optimization-3/loss_delta-800.webp 800w,/assets/img/blogs/optimization-3/loss_delta-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-3/loss_delta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the middle plot, we see that \(\Delta\) exhibits a U-shaped dependence on \(b\). Values \(b = 0.2, 0.3\) yield low \(\Delta\), which correspond to fast and smooth learning curves in the left plot. In the right plot, we explicitly show how the final loss correlates with \(\Delta\), revealing a clear positive correlation.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1KE2T0VdxA_1GZRDGFqWXVZFAQX-GmhYI?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 3 / Depth 2 -- Adding Bias After ReLU}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 2 – Elementwise Scale Reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-2/" rel="alternate" type="text/html" title="Optimization 2 – Elementwise Scale Reparametrization"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, we showed that a re-parametrization trick can significantly speed up optimization. The key ideas were: (1) decompose the weight as \(W = s \hat{W}\), where \(s\) is a scale factor and \(\hat{W}\) has unit norm and represents the direction; (2) make both \(s\) and \(\hat{W}\) learnable. After each update, we rescale \(\hat{W}\) back to the unit sphere and adjust \(s\) accordingly so that the product \(W = s \hat{W}\) remains unchanged.</p> <p>In this blog, we continue along this direction, but with two additional questions in mind:</p> <ul> <li>The toy example in the previous blog is too simple. We would like to extend it to a slightly more realistic setup (a linear layer).</li> <li>Previously, we used a scalar \(s\) to normalize the vector. Is it possible to perform the normalization element-wise instead?</li> </ul> <hr/> <h2 id="failure-mode-when-normalizing-element-wise">Failure mode when normalizing element-wise</h2> <p>In the <a href="/blog/2026/optimization-2/">previous blog</a>, the toy example was two-dimensional. This was a deliberate choice, because the one-dimensional case has a failure mode: when \(s\) is not learnable, the weight cannot change sign, since \(\hat{W}\) can only take values \(+1\) or \(-1\), which are not connected. This issue does not arise in dimensions greater than or equal to two, because unit vectors form a continuous manifold and the two poles are connected, hence reachable through learning.</p> <p>In practice, however, we find that as long as \(s\) is learnable, the weight parameter can change its sign even in 1D, because \(s\) itself can change sign (by crossing zero) during training.</p> <p>We numerically test the four modes discussed in the <a href="/blog/2026/optimization-2/">previous blog</a> for the 1D case, with initial weight \(W_0 = -1\) and target \(W^* = 100\). With re-parametrization and a learnable scale, learning is much faster than in the other modes.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/1D_compare_new-480.webp 480w,/assets/img/blogs/optimization-2/1D_compare_new-800.webp 800w,/assets/img/blogs/optimization-2/1D_compare_new-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/1D_compare_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="linear-regression">Linear regression</h2> <p>We consider a linear regression setup where \(y = W^* x\), with \(x \in \mathbb{R}^{10}\), \(y \in \mathbb{R}^{10}\), and \(W \in \mathbb{R}^{10 \times 10}\) (for simplicity, we ignore the bias term). We draw \(x \in \mathbb{R}^{10}\) from a standard Gaussian distribution. To generate labels \(y\), we deliberately make \(W^*\) anisotropic by setting \(W^*_{ij} = s_i s_j v_{ij},\) where \(v_{ij}\) is drawn from a standard Gaussian, and \(s_i = 10^{-1 + 2 i / 9}, \quad (i = 0, 1, \dots, 9),\) which spans a wide range in \([0.1, 10]\). The loss function is the MSE loss, and we use the Adam optimizer with learning rate \(\eta = 0.01\).</p> <p>We compare the following eight modes:</p> <ul> <li>8 = [Scalar, Matrix (element-wise)] × [fixed, learnable] × [Re-parametrization, No re-parametrization]</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-2/linear_regression-480.webp 480w,/assets/img/blogs/optimization-2/linear_regression-800.webp 800w,/assets/img/blogs/optimization-2/linear_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-2/linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Scalar normalization is faster than element-wise normalization in the early stage (compare green and pink).</li> <li>Re-parametrization helps during early training, but not as much—and can even slow things down—during later training (e.g., compare green and red; compare pink and gray).</li> <li>A learnable scale is essential for element-wise normalization (compare purple and pink; compare brown and gray).</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/19ykGEIlSxlnftuTAcycGoffVdFQ1j8aW?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 2 -- Elementwise scale reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Optimization 1 – Norm reparametrization</title><link href="https://kindxiaoming.github.io/blog/2026/optimization-1/" rel="alternate" type="text/html" title="Optimization 1 – Norm reparametrization"/><published>2026-01-23T00:00:00+00:00</published><updated>2026-01-23T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/optimization-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/optimization-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Recently, a number of architectures and optimizers have emphasized the role of normalization and/or learnable scales. To name a few:</p> <ul> <li>2026-01-21: Hyperball optimization (<a href="https://psychedelic-sunstone-851.notion.site/Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them-2-1-Hyperball-Optimization-2e924306e6f280e7a5ffee00eb40a0dd">link</a>)</li> <li>2026-01-13: Controlled LLM training on a spectral hypersphere (<a href="https://arxiv.org/pdf/2601.08393">Link</a>)</li> <li>2026-01-08: Learnable Multipliers (<a href="https://arxiv.org/pdf/2601.04890">Link</a>)</li> </ul> <p>Slightly less recent (last year :-) ), there are also the <a href="https://kellerjordan.github.io/posts/muon/">Muon optimizer</a> and <a href="https://arxiv.org/abs/2410.01131v1">NVIDIA’s nGPT</a>. These examples give a sense of how popular this direction has become.</p> <p>On the one hand, the intuition is clear: when the step size is fixed, constraining parameters to lie on a small (but not too small) sphere allows the optimizer to make reasonable <em>angular</em> updates. On the other hand, we typically do not know the optimal scale in advance, and in some cases the optimal scale may be very large—or may even be infinite (e.g., logistic regression on linearly separable data). As a result, we would like an optimizer that can make both effective <em>angular</em> updates and effective <em>radial (scale)</em> updates.</p> <p>To gain insight, we start with a toy example below.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>Assume the optimal (flattened) weight of a model is \(W^*\in\mathbb{R}^N\). The initial weight is \(W_0\in\mathbb{R}^N\), and the loss is the MSE loss between \(W\) and \(W^*\), i.e., \(L = \|W - W^*\|_2^2 .\) An Adam optimizer with learning rate \(\eta\) is used to minimize the MSE loss.</p> <p><strong>When the target \(W^*\) is very far away</strong>, such that \(R \equiv \|W^*\| \gg \|W_0\| ,\) the time to reach the target is roughly \(t = \frac{R}{\sqrt{N}\eta} ,\) assuming (1) Adam behaves like SignGD with step size \(\eta\) so that each update has norm \(\sqrt{N}\eta\), and (2) updates from different steps are approximately parallel. In summary, the time complexity is \(O(R)\), which is not very good when \(R\) is large.</p> <p><strong>Can we do better?</strong><br/> Yes, we can!</p> <ul> <li>By introducing a learnable norm scalar, the time can be reduced to \(O(\sqrt{R})\).</li> <li>By further using a re-parametrization trick, the time can be reduced to \(O(\log R)\).</li> </ul> <hr/> <h2 id="step-1-introducing-learnable-multipliers">Step 1: Introducing learnable multipliers</h2> <p>To control both the angular direction and the norm, it is natural to introduce a learnable multiplier \(s\). The actual weight \(W\) is parameterized as \(W = s \hat{W},\) where \(\hat{W}\) may or may not be normalized.</p> <p>When \(\hat{W}\) is learnable and not normalized, it is likely that \(|s| \to O(\sqrt{R}), \quad \|\hat{W}\| \to O(\sqrt{R}),\) since their roles are symmetric in the multiplication. Hence, the convergence time becomes \(t = O(\sqrt{R}/\eta).\) This is already better than \(O(R)\), but can we do even better?</p> <hr/> <h2 id="step-2-re-parametrization">Step 2: Re-parametrization</h2> <p>At each step, we compute the norm \(a \equiv \|\hat{W}\|_2\) and apply the following re-parametrization: \(\hat{W} \to \hat{W}/a, \quad s \to a s .\) This places \(\hat{W}\) on the unit hypersphere, representing the angular direction, while \(s\) captures the norm. Their product remains invariant under this re-parametrization. Training proceeds by interleaving gradient updates and re-parametrization steps.</p> <p>In each update, the norm of \(\hat{W}\) changes from 1 (due to re-parametrization in the previous step) to \(1 \pm A\eta\), where \(A = A(N)\) depends on \(N\) and the gradient directions, which we do not worry about here. The key point is that this is a <em>multiplicative</em> (rather than additive) change. Therefore, growing the norm from 1 to \(R\) only takes \(O(\log R)\) steps.</p> <hr/> <h2 id="experiment">Experiment</h2> <p>We conduct a 2D toy experiment. We set \(W_0 = (-50, 0), \quad W^* = (100, 100).\) We use Adam with learning rate \(\eta = 0.01\) and train for 3000 steps. We compare four modes:</p> <ul> <li><strong>no_scale</strong>: standard optimization (fixed multiplier \(s = 1\))</li> <li><strong>learnable_scale</strong>: learnable \(s\)</li> <li><strong>reparametrized_learnable_scale</strong>: re-parametrization, fix \(s = 1\) during the update step</li> <li><strong>reparametrized_fixed_scale</strong>: re-parametrization, allow \(s\) to update during the update step</li> </ul> <p>Optimization trajectory:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/trajectory-480.webp 480w,/assets/img/blogs/optimization-1/trajectory-800.webp 800w,/assets/img/blogs/optimization-1/trajectory-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/trajectory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Loss:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/optimization-1/loss-480.webp 480w,/assets/img/blogs/optimization-1/loss-800.webp 800w,/assets/img/blogs/optimization-1/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/optimization-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>As expected, re-parametrization &gt; learnable scale &gt; no scale.</li> <li>With re-parametrization, whether the scale \(s\) is learnable or fixed during the update step does not seem to make a noticeable difference.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Bx3ojrssLueShvJdU-ba7DQMWLFuqRVx?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026optimization-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Optimization 1 -- Norm reparametrization}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/optimization-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 5 – Attention sink</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/" rel="alternate" type="text/html" title="Sparse attention 5 – Attention sink"/><published>2026-01-22T00:00:00+00:00</published><updated>2026-01-22T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-5</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-5/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>This article studies how an <strong>attention sink</strong> can emerge in a single-layer (1L), attention-only transformer.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> When all previous tokens are useless for predicting the next token, the ideal attention weights would be zero everywhere. However, due to softmax normalization, attention must be allocated somewhere. As a result, if a <code class="language-plaintext highlighter-rouge">[BOS]</code> token is present at the beginning of the sequence, it can serve as a reservoir that absorbs attention.</p> <p>We use the unigram toy dataset (defined in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>), where the next token does not depend on any previous tokens, but is instead drawn independently from a token-frequency distribution.</p> <p><strong>Model</strong><br/> A 1L attention-only transformer. For simplicity, we remove positional embeddings.</p> <hr/> <h2 id="experiment-1-unigram-dataset">Experiment 1: Unigram dataset</h2> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>We choose the embedding dimension to be 2 and the vocabulary size to be 101 (<code class="language-plaintext highlighter-rouge">[BOS]</code> + 100 tokens whose frequencies follow Zipf’s law). This configuration already performs the task perfectly (in fact, \(n_{\rm embd}=1\) suffices, as discussed in <a href="/blog/2026/unigram-toy-1/">unigram toy</a>).</p> <p>The attention maps are shown below. Each row corresponds to a different sample, and each column corresponds to a different training step. Besides the <code class="language-plaintext highlighter-rouge">[BOS]</code> token, frequent tokens are also used to absorb attention.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram_bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram_bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram_bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram_bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The token embeddings for four different random seeds are shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/token_embd-480.webp 480w,/assets/img/blogs/sparse-attention-5/token_embd-800.webp 800w,/assets/img/blogs/sparse-attention-5/token_embd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/token_embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Recall that we set token frequencies to follow a Zipfian distribution, so small-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[0]</code>) appear more often than large-index tokens (e.g., <code class="language-plaintext highlighter-rouge">[99]</code>). Interestingly, large-index tokens are closer to <code class="language-plaintext highlighter-rouge">[BOS]</code> than small-index tokens in embedding space. Since <code class="language-plaintext highlighter-rouge">[BOS]</code> is heavily attended to, this implies that large-index (infrequent) tokens receive more attention than small-index (frequent) tokens. This likely occurs because attention to frequent tokens is more strongly suppressed than attention to infrequent tokens.</p> <p>Indeed, it has been observed in large language models that their embedding spaces contain a direction correlated with token frequency. Our toy model offers one plausible explanation: this structure emerges for the same reason as the attention sink.</p> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>When <code class="language-plaintext highlighter-rouge">[BOS]</code> is not present, attention sinks still emerge, and these sinks correspond to frequent tokens (with low indices, e.g., <code class="language-plaintext highlighter-rouge">[0]</code>, <code class="language-plaintext highlighter-rouge">[1]</code>).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/unigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/unigram-wo-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/unigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The five sequences are:</p> \[[ 1, 4, 88, 0, 6, 93, 1, 63, 9, 32]\] \[[ 3, 2, 57, 0, 5, 9, 3, 20, 67, 0]\] \[[ 1, 15, 1, 84, 0, 0, 5, 18, 53, 18]\] \[[ 5, 1, 7, 10, 6, 1, 5, 73, 2, 0]\] \[[ 1, 1, 17, 25, 46, 0, 0, 88, 0, 5]\] <hr/> <h2 id="experiment-2-bigram-dataset">Experiment 2: Bigram dataset</h2> <p>We now switch to a bigram toy dataset defined in <a href="/blog/2026/bigram-3/">bigram-3</a>. In this setting, the next token depends on the current token, so the ideal attention pattern would be a diagonal matrix.</p> <p><strong>Including <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is nearly diagonal, except for splitting among identical previous tokens. In addition, some tokens incorrectly attend to the <code class="language-plaintext highlighter-rouge">[BOS]</code> token (e.g., the 3rd token in example 10 and the 3rd token in example 12).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-w-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-w-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-w-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Excluding <code class="language-plaintext highlighter-rouge">[BOS]</code></strong></p> <p>The attention pattern is again almost diagonal, with the main deviation being splitting among identical previous tokens.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-5/bigram-wo-bos-480.webp 480w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-800.webp 800w,/assets/img/blogs/sparse-attention-5/bigram-wo-bos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-5/bigram-wo-bos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1f64J1efN-p_OCfcVo2oxbwYjeUEOT_bb?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-5</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 5 -- Attention sink}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-5/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 4 – On the difficulty of spatial map emergence</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-4/" rel="alternate" type="text/html" title="Bigram 4 – On the difficulty of spatial map emergence"/><published>2026-01-21T00:00:00+00:00</published><updated>2026-01-21T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In two previous posts (<a href="/blog/2026/bigram-1/">bigram-1</a> and <a href="/blog/2026/bigram-2/">bigram-2</a>), we studied a toy task—random walk on a circle. We showed that standard embeddings fail to learn the circular structure underlying the task. This raises a natural question: <strong>why is it hard for a spatial map to emerge?</strong></p> <p>In this article, we deliberately construct a network that can perform the task <em>perfectly</em> when its weights are set appropriately, yet still exhibits failure modes when the weights are randomly initialized and trained via gradient descent. We will analyze these failure modes and propose a simple fix.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We use the same dataset as in the previous blogs. Suppose there are \(p=10\) points on a circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong><br/> We now construct a network that can solve this task perfectly. Suppose there are \(p\) points in total. The \(i^{\rm th}\) point is embedded as<br/> \(E_i = A\bigl(\cos(\tfrac{2\pi i}{p}), \sin(\tfrac{2\pi i}{p})\bigr).\)</p> <p>To walk clockwise to the neighboring point, we need a rotation matrix with angle \(\theta=\frac{2\pi}{p}\): \(R(\theta)= \begin{pmatrix} \cos\theta &amp; \sin\theta \\ -\sin\theta &amp; \cos\theta \end{pmatrix},\) parameterized as \(W_1\).<br/> To walk counter-clockwise to the neighboring point, we need \(R(-\theta)\), parameterized as \(W_2\).</p> <p>To read out each location, the unembedding matrix shares the same weights as the embedding matrix. To create a bimodal distribution, we must <em>average two probability distributions</em> (not add two logits; note that this cannot be efficiently simulated by standard layers).</p> <p>In summary, when embeddings lie on a circle with a large radius \(A\), and when \(W_1\) and \(W_2\) learn the corresponding rotation matrices, the task can be solved perfectly.</p> <hr/> <h2 id="observation-random-initialization-fails-to-learn-a-circle">Observation: random initialization fails to learn a circle</h2> <p>We set \(n_{\rm embd}=2\) and \(p=29\). We show results from six different random seeds; in none of them does a circular structure emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/random_w_trained-480.webp 480w,/assets/img/blogs/bigram-4/random_w_trained-800.webp 800w,/assets/img/blogs/bigram-4/random_w_trained-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/random_w_trained.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From <a href="/blog/2026/bigram-2/">bigram-2</a>, we learned that there may exist <em>negative directions</em> in which points that are close in physical space are actually far apart in embedding space. This motivates the following experiments—<strong>fixing \(W_1\) and \(W_2\)</strong>.</p> <hr/> <h2 id="experiment-1">Experiment 1</h2> <p>We initialize \(W_1\) and \(W_2\) to simple matrices (corresponding to Euclidean, hyperbolic, and anti-Euclidean geometries) and keep them fixed during training.</p> <p><strong>First case</strong> – \(W_1=W_2=\mathrm{diag}(1,1)\):<br/> A circular structure can be learned for most random seeds. The perplexity (4.15) is still high because a token is closest to itself—the model therefore predicts the most probable next point to be the current point. However, the ground truth is either the left or the right neighbor, leading to high loss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/1_1-480.webp 480w,/assets/img/blogs/bigram-4/1_1-800.webp 800w,/assets/img/blogs/bigram-4/1_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/1_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Second case</strong> – \(W_1=W_2=\mathrm{diag}(1,-1)\):<br/> A global circular structure is visible, but it is locally incorrect (pairs of points collapse together), and the numerical order is not clean.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/1_-1-480.webp 480w,/assets/img/blogs/bigram-4/1_-1-800.webp 800w,/assets/img/blogs/bigram-4/1_-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/1_-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Third case</strong> – \(W_1=W_2=\mathrm{diag}(-1,-1)\):</p> <p>At first glance, a perfect circle seems to emerge. However, upon closer inspection, the ordering is incorrect—for example, 9 is opposite to 8/10 rather than being nearby. Interestingly, the perplexity is lower than in the first case. In fact, the perplexity (2.01) is close to the theoretical optimum of 2. This is because this embedding geometry places a token far from itself, whereas in the first case a token is closest to itself (corresponding to staying in place, which is not allowed by the task).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/-1_-1-480.webp 480w,/assets/img/blogs/bigram-4/-1_-1-800.webp 800w,/assets/img/blogs/bigram-4/-1_-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/-1_-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="experiment-2">Experiment 2</h2> <p>We set \(W_1=W_2=\mathrm{diag}(1,1)\) (injecting small noise to break symmetry) and allow them to be trainable. During training, the perplexity first converges to 4.14 (embeddings evolve while \(W_1/W_2\) remain nearly fixed), and then transitions to 2 (embeddings stabilize while \(W_1/W_2\) evolve).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-4/trainable_W-480.webp 480w,/assets/img/blogs/bigram-4/trainable_W-800.webp 800w,/assets/img/blogs/bigram-4/trainable_W-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-4/trainable_W.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <ul> <li>When a sequence of tokens is generated by an underlying <em>continuous process</em>, it may be beneficial to initialize projection matrices as the identity, allowing spatial locality to be learned.</li> <li>Does this initialization trick work for natural language? Not necessarily, for two reasons:<br/> (1) Although one may argue that neural activity (which determines speech) is continuous, the time scales differ—neural activity operates on millisecond scales, whereas natural language unfolds over seconds.<br/> (2) Natural language appears more discrete than continuous. The random-walk dataset is therefore not a good abstraction of language. Instead, we need <em>toy language datasets</em> that capture discrete dependency graphs.</li> <li>The toy model we studied in this article is non-standard, because it is specifically designed for the random walk task.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1B1mqAIva9WMDXGJCFdjJaOTeC78FBjMX?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 4 -- On the difficulty of spatial map emergence}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 1 – Understanding Pre-LN and Post-LN</title><link href="https://kindxiaoming.github.io/blog/2026/depth-1/" rel="alternate" type="text/html" title="Depth 1 – Understanding Pre-LN and Post-LN"/><published>2026-01-20T00:00:00+00:00</published><updated>2026-01-20T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>My line of reasoning is as follows.</p> <p>I want to understand DeepSeek’s <a href="https://arxiv.org/pdf/2512.24880">mHC paper</a>. To do that, I first need to understand ByteDance’s earlier <a href="https://arxiv.org/pdf/2409.19606">HC paper</a>.</p> <p>In the Hyper-connection paper, the introduction states:</p> <blockquote> <p>The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing (Bengio et al., 1994; Glorot &amp; Bengio, 2010). However, it can also lead to the issue of collapse in deep representations (Liu et al., 2020), where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers. This approach can alleviate the issue of representation collapse but also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer.</p> </blockquote> <p>In short: <strong>Pre-Norm tends to lead to representation collapse, while Post-Norm tends to lead to vanishing gradients</strong>. I have heard this conclusion many times, but I never really had the chance to internalize it. The goal of this article is therefore to construct a minimal toy model that helps me better understand these claims about Pre-Norm and Post-Norm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> This article focuses only on forward computations, so we do not need to worry about outputs. The inputs are i.i.d. samples drawn from a Gaussian distribution.</p> <p><strong>Model</strong><br/> The model consists of residual blocks, where each block is a simple MLP. Each MLP has two fully connected layers, FC1 and FC2. We explicitly control the scale (\(\alpha\)) of FC2 so that we can tune the relative importance of the residual update versus the input.</p> <p>In practice, during training, \(\alpha\) often increases from 1. Even though we never actually train the model here, we can still vary \(\alpha\) to gain intuition about how a trained model might behave. A large \(\alpha\) means that the update dominates the input, while a small \(\alpha\) means the update is small compared to the input.</p> <p>The architectures of Post-LN and Pre-LN are shown below.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-vs-pre-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN-vs-pre-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When the number of samples is much larger than the input dimension, the explained variances from PCA are roughly uniform, so the effective dimension is close to the input dimension. We are interested in how this effective dimension (and other statistics) evolve across layers.</p> <hr/> <h2 id="observation-1-pre-ln-and-representation-collapse">Observation 1: Pre-LN and representation collapse</h2> <p>We set the depth to \(L = 100\) layers. Let \(h^l\) denote the residual stream after the \(l\)-th block. We focus on the following quantities:</p> <ul> <li>the cosine similarity between \(h^l\) and \(h^{l+1}\)</li> <li>the norm of \(h^l\)</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/pre-norm-repr-collapse-480.webp 480w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-800.webp 800w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/pre-norm-repr-collapse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe two asymptotic behaviors:</p> <ul> <li> <p>\(\lVert h^l \rVert \sim \sqrt{l}\).<br/> Since the update at each block is random (due to random initialization), the evolution along the depth direction is effectively a random walk. Its variance grows linearly with \(l\), so the norm grows like \(\sqrt{l}\). This likely explains why <a href="https://arxiv.org/pdf/2206.03126">this paper</a> uses a \(1/\sqrt{L}\) scaling, in order to control the norm.</p> </li> <li> <p>\(1 - {\rm cos}(h^l, h^{l+1}) \sim 1/l\).<br/> The norm of the update is roughly constant, while the norm of \(h^l\) keeps growing. Therefore, the relative angular update satisfies \(\theta \sim 1/\lVert h^l \rVert \sim 1/\sqrt{l}\), which implies \(1 - {\rm cos}(h^l, h^{l+1}) \sim \theta^2 \sim 1/l\).</p> </li> </ul> <hr/> <h2 id="observation-2-post-ln-and-vanishing-influence-of-early-layers">Observation 2: Post-LN and vanishing influence of early layers</h2> <p>If we say that Pre-LN suffers from the problem that <em>later layers become “useless”</em>, then Post-LN suffers from the opposite problem: <em>early layers become “useless”</em>. Due to Post-LN, \(\lVert h^l \rVert\) remains approximately constant across layers. As a result, signals originating from early layers keep shrinking as they propagate forward, leading to vanishing influence on the output (and correspondingly, vanishing gradients).</p> <p>We verify that Post-LN does not exhibit strong representation collapse, in the sense that \({\rm cos}(h^l, h^{l_1})\) does not asymptotically approach 1 as the layer index increases. However, even when cosine similarity is not close to 1, the influence of early-layer representations can still decay rapidly as they propagate to later layers.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We also compute the effective rank of \(h^l\): we apply PCA to a point cloud of \(h^l\), normalize the singular values into a probability distribution, compute its entropy, and then exponentiate it. The results suggest that Pre-LN maintains representation rank better than Post-LN.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/eff-rank-480.webp 480w,/assets/img/blogs/depth-1/eff-rank-800.webp 800w,/assets/img/blogs/depth-1/eff-rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/eff-rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At first, this result confused me, because I had incorrectly equated “representation collapse” with “low representation rank.” In fact, what people usually mean by representation collapse in this context is <strong>strong alignment between the representations of consecutive layers</strong>, rather than a literal drop in rank.</p> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Gradient analysis:</strong> So far, we have only analyzed forward computations. How do gradients propagate across layers in this setup?</li> <li>Many papers (including the Hyper-connection paper) attempt to find a better trade-off between vanishing gradients and representation collapse. Is it possible to eliminate both? Or is there fundamentally no free lunch?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1KV3VFzaxtYTiDhU-kdltmu9Jx1_gJCDm?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 1 -- Understanding Pre-LN and Post-LN}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 3 – Low Rank Structure</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-3/" rel="alternate" type="text/html" title="Bigram 3 – Low Rank Structure"/><published>2026-01-19T00:00:00+00:00</published><updated>2026-01-19T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In large language models (LLMs), the embedding dimension is typically <strong>much smaller</strong> than the vocabulary size, yet models still perform remarkably well. Does this suggest that language itself possesses some kind of <strong>low-rank structure</strong>?</p> <p>In this article, we study a <strong>Bigram dataset</strong>, where each token depends only on the immediately preceding token. We further assume that the (log) transition probability matrix has a <strong>low-rank structure</strong>. This naturally raises the following question: <strong>Is it sufficient for the embedding dimension to match this rank, rather than scaling all the way up to the vocabulary size?</strong></p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> Let the vocabulary size be \(V\) and the rank be \(R\). We generate two random matrices \(A\in\mathbb{R}^{V\times R}, \quad B\in\mathbb{R}^{R\times V}.\) Their matrix product \(L = AB/\sqrt{R}\) is a low-rank matrix. The factor \(1/\sqrt{R}\) ensures that the scale of \(L\) is independent of \(R\). Applying a row-wise softmax to \(L\) yields the transition matrix \(P = {\rm Softmax}(L, {\rm dim}=1).\)</p> <p>From the transition matrix, we can compute the steady-state distribution \(\pi\), which is interpreted as the token frequency (i.e., the unigram distribution). To generate a batch of data, we proceed in two steps:</p> <ul> <li><strong>Step 1:</strong> sample the input token from the unigram distribution.</li> <li><strong>Step 2:</strong> sample the output token from the transition matrix, conditioned on the input token.</li> </ul> <p>The best achievable loss \(L_0\) is given by the <strong>conditional entropy</strong>, averaged over input tokens.</p> <p>We can introduce additional knobs to control the dataset:</p> <ul> <li>Instead of assuming all \(R\) ranks are equally important, we can assign different weights by inserting a diagonal matrix between \(A\) and \(B\): \(AB \;\to\; A\Lambda B.\)</li> <li>We can also control the overall scale of the logit matrix. When this scale is large, the dataset becomes more deterministic.</li> </ul> <p><strong>Model</strong><br/> Our model consists only of an <strong>Embedding layer</strong> and an <strong>Unembedding layer</strong>, whose weights are <strong>not tied</strong>. The main quantity of interest in this article is the embedding dimension \(N\).</p> <hr/> <h2 id="observation-1-critical-embedding-dimension-n_capprox-r">Observation 1: critical embedding dimension \(N_c\approx R\)</h2> <p>We set \(V=10\) and \(R=3\). We expect that when \(N=N_c=3\), the model can achieve the optimal loss \(L_0\), whereas for \(N&lt;N_c\) the loss should be strictly higher. This is indeed what we observe.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/critical_dim-480.webp 480w,/assets/img/blogs/bigram-3/critical_dim-800.webp 800w,/assets/img/blogs/bigram-3/critical_dim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/critical_dim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="observation-2-scaling-laws-in-the-regime-nr">Observation 2: scaling laws in the regime \(N&lt;R\)</h2> <p>We now set \(V=100\) and \(R=20\), and sweep \(N\) from 1 to 20. Defining the loss gap \(\Delta \equiv L - L_0,\) we find that it closely follows a scaling law \(\Delta \sim N^{-1}.\) This can be viewed as a generalization of the result reported in <a href="https://arxiv.org/abs/2505.10465">this paper</a>.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/scaling_law-480.webp 480w,/assets/img/blogs/bigram-3/scaling_law-800.webp 800w,/assets/img/blogs/bigram-3/scaling_law-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/scaling_law.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Loss analysis:</strong> which token(s) incur the largest loss?</li> <li><strong>Training dynamics:</strong> how do the embeddings evolve during training?</li> <li><strong>Architecture choices:</strong> how do weight sharing, attention, MLPs, and layer normalization affect the results?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/18QrrL4LOwpgQ4ffxe_vz_CO0tEKWtAqg?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 3 -- Low Rank Structure}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 2 – Emergence of Hyperbolic Spaces</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-2/" rel="alternate" type="text/html" title="Bigram 2 – Emergence of Hyperbolic Spaces"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/bigram-1/">yesterday’s blog post</a>, we studied a simple Markov chain—a random walk on a circle. We found that when the embedding dimension is 2, the model fails to perform the task (even though a circle can, in principle, be perfectly embedded in 2D). However, when the embedding dimension is increased to 4, the model can solve the task perfectly. At the time, we did not understand <em>what</em> the 4D solution was actually doing. This article is an attempt to understand the mechanism behind this 4D algorithm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We use the same dataset as in the previous blog. Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>The model consists only of an Embedding layer, an Unembedding layer (with tied weights), and a linear layer \(A\) in between. This is equivalent to an Attention layer with context length 1, where \(A = OV\).<br/> When there is no linear layer \(A\) (or equivalently \(A = I\)), the model completely fails to perform this task. In that case, the model most likely repeats the current token, rather than predicting the token to its left or right (we will discuss this more carefully at the end of the article, but for now we take this as a given fact). Therefore, the linear layer \(A\) is necessary.</p> <hr/> <h2 id="observation-1-a-is-symmetric-and-has-negative-eigenvalues-hyperbolic-directions">Observation 1: \(A\) is symmetric, and has negative eigenvalues (hyperbolic directions)</h2> <p>When \(n_{\rm embd} = 4\), the perplexity can go down to 2 (the best achievable perplexity). By directly inspecting the embeddings or their PCA projections, we fail to observe any obvious pattern.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/embd-480.webp 480w,/assets/img/blogs/bigram-2/embd-800.webp 800w,/assets/img/blogs/bigram-2/embd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, we notice that the linear matrix \(A\) is close to a symmetric matrix:</p> \[A = \begin{pmatrix} -3.3250 &amp; -2.0423 &amp; 2.0838 &amp; -3.5954 \\ -2.0604 &amp; -3.2431 &amp; -2.8658 &amp; 2.8647 \\ 2.0648 &amp; -2.8236 &amp; -2.5492 &amp; -2.8768 \\ -3.5372 &amp; 2.7750 &amp; -2.8383 &amp; -1.5314 \\ \end{pmatrix}\] <p>A symmetric matrix can be diagonalized over the real numbers. The four eigenvalues consist of two positive and two negative values: \([-5.88, -5.41, 5.42, 6.40],\) all of comparable magnitude. It is therefore more natural to project the embeddings onto the eigen-directions:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/eigen-480.webp 480w,/assets/img/blogs/bigram-2/eigen-800.webp 800w,/assets/img/blogs/bigram-2/eigen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/eigen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the two negative-eigenvalue directions are highly oscillatory (adjacent tokens have opposite signs), whereas the two positive-eigenvalue directions are much smoother (adjacent tokens have similar values).</p> <p>Consider an input token \(m\) with embedding \(E_m\). Its logit for token \(n\) is given by \(E_n^T A E_m\), which is a quadratic form. Along positive eigen-directions, the logit is <em>lower</em> if two embeddings have opposite signs. In contrast, along negative eigen-directions, the logit is <em>higher</em> if two embeddings have opposite signs. The coexistence of positive and negative eigenvalues effectively turns the embedding space into a hyperbolic space, which can strongly conflict with our Euclidean geometric intuition.</p> <p>More formally, for two vectors \(x, y\) in this space, their similarity can be written as \(L(i,j) \sim -x_1 y_1 - x_2 y_2 + x_3 y_3 + x_4 y_4.\)</p> <p>For nearby tokens, we want the similarity to be large. This can be achieved by having opposite signs along the negative eigen-directions, and the same signs (similar values) along the positive eigen-directions. This mixed strategy makes interpretation difficult: although tokens \(i\) and \(i+1\) correspond to nearby points on the circle, they are not necessarily close in the embedding space, because the negative eigen-directions actively push them as far apart as possible.</p> <p>Two remarks are in order:</p> <p><strong>\(A\) is not necessarily symmetric.</strong><br/> In our case, \(A\) is symmetric because the Markov process we study is reversible. In general, \(A\) need not be symmetric, and it is unclear how to deal with a non-symmetric \(A\), where the left and right eigenspaces are no longer aligned.</p> <p><strong>More to understand.</strong><br/> So far, we have established that the embedding space is hyperbolic, which is already a somewhat surprising result with potentially significant implications for interpretability. However, many details are still missing, in particular how the tokens are arranged <em>quantitatively</em> within this hyperbolic space.</p> <hr/> <h2 id="observation-2-geometry-matters">Observation 2: Geometry matters</h2> <p>We find that \(n_{\rm embd} = 4\) works (i.e., the perplexity converges to 2) only for lucky random seeds. For unlucky random seeds, the perplexity instead converges to around 2.13. What distinguishes these cases? We observe that the geometry—specifically, the number of negative directions—is highly correlated with performance.</p> <p>When there are 2 or 3 negative directions, the optimal perplexity is achievable. With only 1 negative direction, optimization appears to get stuck in a local minimum.</p> <table class="table table-bordered"> <thead> <tr> <th>Random Seed</th> <th>Perplexity</th> <th>Eigenvalues</th> <th>Negative Number</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>2.13</td> <td>-7.6, 5.6, 5.7, 6.7</td> <td>1</td> </tr> <tr> <td>1</td> <td>2.13</td> <td>-7.7, 5.1, 6.1, 6.2</td> <td>1</td> </tr> <tr> <td>2</td> <td>2.13</td> <td>-7.2, 5.7, 6.4, 6.5</td> <td>1</td> </tr> <tr> <td>3</td> <td>2.00</td> <td>-6.1, -5.8, -4.4, 5.6</td> <td>3</td> </tr> <tr> <td>4</td> <td>2.00</td> <td>-5.9, -5.4, 5.4, 6.4</td> <td>2</td> </tr> <tr> <td>5</td> <td>2.00</td> <td>-6.7, -6.0, -5.7, 6.5</td> <td>3</td> </tr> <tr> <td>6</td> <td>2.10</td> <td>-7.0, 5.3, 5.7, 6.2</td> <td>1</td> </tr> <tr> <td>7</td> <td>2.00</td> <td>-5.7, -5.2, -4.9, 5.4</td> <td>3</td> </tr> <tr> <td>8</td> <td>2.00</td> <td>-5.8, -4.8, 4.6, 5.9</td> <td>2</td> </tr> <tr> <td>9</td> <td>2.10</td> <td>-7.0, 5.1, 6.1, 7.1</td> <td>1</td> </tr> <tr> <td>42</td> <td>2.10</td> <td>-7.1, 6.0, 6.3, 6.8</td> <td>1</td> </tr> <tr> <td>2026</td> <td>2.00</td> <td>-5.1, -4.8, 4.7, 5.8</td> <td>2</td> </tr> </tbody> </table> <hr/> <h2 id="sanity-check-why-do-we-need-a">Sanity check: why do we need \(A\)?</h2> <p>When we remove \(A\) or set \(A = I\), the perplexity remains far above 2, regardless of how large the embedding dimension is. When \(A = I\) (so the embedding space is purely Euclidean), an input token has no mechanism to “un-attend” to itself. In contrast, a negative eigen-direction can serve precisely this un-attention role, a mechanism also explored in <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>. As a result, \(A\) is essential for performing the random-walk task.</p> <p>That said, when \(A = I\) (i.e., attention is completely removed) and \(n_{\rm embd} = 2\), the embeddings can evolve into extremely wild (and aesthetically pleasing) patterns. Even though these patterns may not shed much light on what is really happening in language models, it is pure pleasure to watch the resulting animations.</p> <p>Seed = 0</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_0-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_0-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 1</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_1-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_1-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 4</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_4-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_4-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 6</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_6-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_6-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 9</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_9-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_9-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_9.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NmfqzshitvHwZZ3RNobwlJNH2Ebrlqj0?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 2 -- Emergence of Hyperbolic Spaces}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>