<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-20T04:51:59+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Depth 1 – Understanding Pre-LN and Post-LN</title><link href="https://kindxiaoming.github.io/blog/2026/depth-1/" rel="alternate" type="text/html" title="Depth 1 – Understanding Pre-LN and Post-LN"/><published>2026-01-20T00:00:00+00:00</published><updated>2026-01-20T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>My line of reasoning is as follows.</p> <p>I want to understand DeepSeek’s <a href="https://arxiv.org/pdf/2512.24880">mHC paper</a>. To do that, I first need to understand ByteDance’s earlier <a href="https://arxiv.org/pdf/2409.19606">HC paper</a>.</p> <p>In the Hyper-connection paper, the introduction states:</p> <blockquote> <p>The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing (Bengio et al., 1994; Glorot &amp; Bengio, 2010). However, it can also lead to the issue of collapse in deep representations (Liu et al., 2020), where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers. This approach can alleviate the issue of representation collapse but also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer.</p> </blockquote> <p>In short: <strong>Pre-Norm tends to lead to representation collapse, while Post-Norm tends to lead to vanishing gradients</strong>. I have heard this conclusion many times, but I never really had the chance to internalize it. The goal of this article is therefore to construct a minimal toy model that helps me better understand these claims about Pre-Norm and Post-Norm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> This article focuses only on forward computations, so we do not need to worry about outputs. The inputs are i.i.d. samples drawn from a Gaussian distribution.</p> <p><strong>Model</strong><br/> The model consists of residual blocks, where each block is a simple MLP. Each MLP has two fully connected layers, FC1 and FC2. We explicitly control the scale (\(\alpha\)) of FC2 so that we can tune the relative importance of the residual update versus the input.</p> <p>In practice, during training, \(\alpha\) often increases from 1. Even though we never actually train the model here, we can still vary \(\alpha\) to gain intuition about how a trained model might behave. A large \(\alpha\) means that the update dominates the input, while a small \(\alpha\) means the update is small compared to the input.</p> <p>The architectures of Post-LN and Pre-LN are shown below.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-vs-pre-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-vs-pre-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN-vs-pre-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When the number of samples is much larger than the input dimension, the explained variances from PCA are roughly uniform, so the effective dimension is close to the input dimension. We are interested in how this effective dimension (and other statistics) evolve across layers.</p> <hr/> <h2 id="observation-1-pre-ln-and-representation-collapse">Observation 1: Pre-LN and representation collapse</h2> <p>We set the depth to \(L = 100\) layers. Let \(h^l\) denote the residual stream after the \(l\)-th block. We focus on the following quantities:</p> <ul> <li>the cosine similarity between \(h^l\) and \(h^{l+1}\)</li> <li>the norm of \(h^l\)</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/pre-norm-repr-collapse-480.webp 480w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-800.webp 800w,/assets/img/blogs/depth-1/pre-norm-repr-collapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/pre-norm-repr-collapse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe two asymptotic behaviors:</p> <ul> <li> <p>\(\lVert h^l \rVert \sim \sqrt{l}\).<br/> Since the update at each block is random (due to random initialization), the evolution along the depth direction is effectively a random walk. Its variance grows linearly with \(l\), so the norm grows like \(\sqrt{l}\). This likely explains why <a href="https://arxiv.org/pdf/2206.03126">this paper</a> uses a \(1/\sqrt{L}\) scaling, in order to control the norm.</p> </li> <li> <p>\(1 - {\rm cos}(h^l, h^{l+1}) \sim 1/l\).<br/> The norm of the update is roughly constant, while the norm of \(h^l\) keeps growing. Therefore, the relative angular update satisfies \(\theta \sim 1/\lVert h^l \rVert \sim 1/\sqrt{l}\), which implies \(1 - {\rm cos}(h^l, h^{l+1}) \sim \theta^2 \sim 1/l\).</p> </li> </ul> <hr/> <h2 id="observation-2-post-ln-and-vanishing-influence-of-early-layers">Observation 2: Post-LN and vanishing influence of early layers</h2> <p>If we say that Pre-LN suffers from the problem that <em>later layers become “useless”</em>, then Post-LN suffers from the opposite problem: <em>early layers become “useless”</em>. Due to Post-LN, \(\lVert h^l \rVert\) remains approximately constant across layers. As a result, signals originating from early layers keep shrinking as they propagate forward, leading to vanishing influence on the output (and correspondingly, vanishing gradients).</p> <p>We verify that Post-LN does not exhibit strong representation collapse, in the sense that \({\rm cos}(h^l, h^{l_1})\) does not asymptotically approach 1 as the layer index increases. However, even when cosine similarity is not close to 1, the influence of early-layer representations can still decay rapidly as they propagate to later layers.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/post-LN-480.webp 480w,/assets/img/blogs/depth-1/post-LN-800.webp 800w,/assets/img/blogs/depth-1/post-LN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/post-LN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We also compute the effective rank of \(h^l\): we apply PCA to a point cloud of \(h^l\), normalize the singular values into a probability distribution, compute its entropy, and then exponentiate it. The results suggest that Pre-LN maintains representation rank better than Post-LN.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-1/eff-rank-480.webp 480w,/assets/img/blogs/depth-1/eff-rank-800.webp 800w,/assets/img/blogs/depth-1/eff-rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-1/eff-rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At first, this result confused me, because I had incorrectly equated “representation collapse” with “low representation rank.” In fact, what people usually mean by representation collapse in this context is <strong>strong alignment between the representations of consecutive layers</strong>, rather than a literal drop in rank.</p> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Gradient analysis:</strong> So far, we have only analyzed forward computations. How do gradients propagate across layers in this setup?</li> <li>Many papers (including the Hyper-connection paper) attempt to find a better trade-off between vanishing gradients and representation collapse. Is it possible to eliminate both? Or is there fundamentally no free lunch?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1KV3VFzaxtYTiDhU-kdltmu9Jx1_gJCDm?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 1 -- Understanding Pre-LN and Post-LN}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 3 – Low Rank Structure</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-3/" rel="alternate" type="text/html" title="Bigram 3 – Low Rank Structure"/><published>2026-01-19T00:00:00+00:00</published><updated>2026-01-19T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In large language models (LLMs), the embedding dimension is typically <strong>much smaller</strong> than the vocabulary size, yet models still perform remarkably well. Does this suggest that language itself possesses some kind of <strong>low-rank structure</strong>?</p> <p>In this article, we study a <strong>Bigram dataset</strong>, where each token depends only on the immediately preceding token. We further assume that the (log) transition probability matrix has a <strong>low-rank structure</strong>. This naturally raises the following question: <strong>Is it sufficient for the embedding dimension to match this rank, rather than scaling all the way up to the vocabulary size?</strong></p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> Let the vocabulary size be \(V\) and the rank be \(R\). We generate two random matrices \(A\in\mathbb{R}^{V\times R}, \quad B\in\mathbb{R}^{R\times V}.\) Their matrix product \(L = AB/\sqrt{R}\) is a low-rank matrix. The factor \(1/\sqrt{R}\) ensures that the scale of \(L\) is independent of \(R\). Applying a row-wise softmax to \(L\) yields the transition matrix \(P = {\rm Softmax}(L, {\rm dim}=1).\)</p> <p>From the transition matrix, we can compute the steady-state distribution \(\pi\), which is interpreted as the token frequency (i.e., the unigram distribution). To generate a batch of data, we proceed in two steps:</p> <ul> <li><strong>Step 1:</strong> sample the input token from the unigram distribution.</li> <li><strong>Step 2:</strong> sample the output token from the transition matrix, conditioned on the input token.</li> </ul> <p>The best achievable loss \(L_0\) is given by the <strong>conditional entropy</strong>, averaged over input tokens.</p> <p>We can introduce additional knobs to control the dataset:</p> <ul> <li>Instead of assuming all \(R\) ranks are equally important, we can assign different weights by inserting a diagonal matrix between \(A\) and \(B\): \(AB \;\to\; A\Lambda B.\)</li> <li>We can also control the overall scale of the logit matrix. When this scale is large, the dataset becomes more deterministic.</li> </ul> <p><strong>Model</strong><br/> Our model consists only of an <strong>Embedding layer</strong> and an <strong>Unembedding layer</strong>, whose weights are <strong>not tied</strong>. The main quantity of interest in this article is the embedding dimension \(N\).</p> <hr/> <h2 id="observation-1-critical-embedding-dimension-n_capprox-r">Observation 1: critical embedding dimension \(N_c\approx R\)</h2> <p>We set \(V=10\) and \(R=3\). We expect that when \(N=N_c=3\), the model can achieve the optimal loss \(L_0\), whereas for \(N&lt;N_c\) the loss should be strictly higher. This is indeed what we observe.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/critical_dim-480.webp 480w,/assets/img/blogs/bigram-3/critical_dim-800.webp 800w,/assets/img/blogs/bigram-3/critical_dim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/critical_dim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="observation-2-scaling-laws-in-the-regime-nr">Observation 2: scaling laws in the regime \(N&lt;R\)</h2> <p>We now set \(V=100\) and \(R=20\), and sweep \(N\) from 1 to 20. Defining the loss gap \(\Delta \equiv L - L_0,\) we find that it closely follows a scaling law \(\Delta \sim N^{-1}.\) This can be viewed as a generalization of the result reported in <a href="https://arxiv.org/abs/2505.10465">this paper</a>.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-3/scaling_law-480.webp 480w,/assets/img/blogs/bigram-3/scaling_law-800.webp 800w,/assets/img/blogs/bigram-3/scaling_law-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-3/scaling_law.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="questions">Questions</h2> <p>Many questions about this toy model remain open:</p> <ul> <li><strong>Loss analysis:</strong> which token(s) incur the largest loss?</li> <li><strong>Training dynamics:</strong> how do the embeddings evolve during training?</li> <li><strong>Architecture choices:</strong> how do weight sharing, attention, MLPs, and layer normalization affect the results?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/18QrrL4LOwpgQ4ffxe_vz_CO0tEKWtAqg?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 3 -- Low Rank Structure}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 2 – Emergence of Hyperbolic Spaces</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-2/" rel="alternate" type="text/html" title="Bigram 2 – Emergence of Hyperbolic Spaces"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/bigram-1/">yesterday’s blog post</a>, we studied a simple Markov chain—a random walk on a circle. We found that when the embedding dimension is 2, the model fails to perform the task (even though a circle can, in principle, be perfectly embedded in 2D). However, when the embedding dimension is increased to 4, the model can solve the task perfectly. At the time, we did not understand <em>what</em> the 4D solution was actually doing. This article is an attempt to understand the mechanism behind this 4D algorithm.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We use the same dataset as in the previous blog. Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>The model consists only of an Embedding layer, an Unembedding layer (with tied weights), and a linear layer \(A\) in between. This is equivalent to an Attention layer with context length 1, where \(A = OV\).<br/> When there is no linear layer \(A\) (or equivalently \(A = I\)), the model completely fails to perform this task. In that case, the model most likely repeats the current token, rather than predicting the token to its left or right (we will discuss this more carefully at the end of the article, but for now we take this as a given fact). Therefore, the linear layer \(A\) is necessary.</p> <hr/> <h2 id="observation-1-a-is-symmetric-and-has-negative-eigenvalues-hyperbolic-directions">Observation 1: \(A\) is symmetric, and has negative eigenvalues (hyperbolic directions)</h2> <p>When \(n_{\rm embd} = 4\), the perplexity can go down to 2 (the best achievable perplexity). By directly inspecting the embeddings or their PCA projections, we fail to observe any obvious pattern.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/embd-480.webp 480w,/assets/img/blogs/bigram-2/embd-800.webp 800w,/assets/img/blogs/bigram-2/embd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, we notice that the linear matrix \(A\) is close to a symmetric matrix:</p> \[A = \begin{pmatrix} -3.3250 &amp; -2.0423 &amp; 2.0838 &amp; -3.5954 \\ -2.0604 &amp; -3.2431 &amp; -2.8658 &amp; 2.8647 \\ 2.0648 &amp; -2.8236 &amp; -2.5492 &amp; -2.8768 \\ -3.5372 &amp; 2.7750 &amp; -2.8383 &amp; -1.5314 \\ \end{pmatrix}\] <p>A symmetric matrix can be diagonalized over the real numbers. The four eigenvalues consist of two positive and two negative values: \([-5.88, -5.41, 5.42, 6.40],\) all of comparable magnitude. It is therefore more natural to project the embeddings onto the eigen-directions:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/eigen-480.webp 480w,/assets/img/blogs/bigram-2/eigen-800.webp 800w,/assets/img/blogs/bigram-2/eigen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/eigen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the two negative-eigenvalue directions are highly oscillatory (adjacent tokens have opposite signs), whereas the two positive-eigenvalue directions are much smoother (adjacent tokens have similar values).</p> <p>Consider an input token \(m\) with embedding \(E_m\). Its logit for token \(n\) is given by \(E_n^T A E_m\), which is a quadratic form. Along positive eigen-directions, the logit is <em>lower</em> if two embeddings have opposite signs. In contrast, along negative eigen-directions, the logit is <em>higher</em> if two embeddings have opposite signs. The coexistence of positive and negative eigenvalues effectively turns the embedding space into a hyperbolic space, which can strongly conflict with our Euclidean geometric intuition.</p> <p>More formally, for two vectors \(x, y\) in this space, their similarity can be written as \(L(i,j) \sim -x_1 y_1 - x_2 y_2 + x_3 y_3 + x_4 y_4.\)</p> <p>For nearby tokens, we want the similarity to be large. This can be achieved by having opposite signs along the negative eigen-directions, and the same signs (similar values) along the positive eigen-directions. This mixed strategy makes interpretation difficult: although tokens \(i\) and \(i+1\) correspond to nearby points on the circle, they are not necessarily close in the embedding space, because the negative eigen-directions actively push them as far apart as possible.</p> <p>Two remarks are in order:</p> <p><strong>\(A\) is not necessarily symmetric.</strong><br/> In our case, \(A\) is symmetric because the Markov process we study is reversible. In general, \(A\) need not be symmetric, and it is unclear how to deal with a non-symmetric \(A\), where the left and right eigenspaces are no longer aligned.</p> <p><strong>More to understand.</strong><br/> So far, we have established that the embedding space is hyperbolic, which is already a somewhat surprising result with potentially significant implications for interpretability. However, many details are still missing, in particular how the tokens are arranged <em>quantitatively</em> within this hyperbolic space.</p> <hr/> <h2 id="observation-2-geometry-matters">Observation 2: Geometry matters</h2> <p>We find that \(n_{\rm embd} = 4\) works (i.e., the perplexity converges to 2) only for lucky random seeds. For unlucky random seeds, the perplexity instead converges to around 2.13. What distinguishes these cases? We observe that the geometry—specifically, the number of negative directions—is highly correlated with performance.</p> <p>When there are 2 or 3 negative directions, the optimal perplexity is achievable. With only 1 negative direction, optimization appears to get stuck in a local minimum.</p> <table class="table table-bordered"> <thead> <tr> <th>Random Seed</th> <th>Perplexity</th> <th>Eigenvalues</th> <th>Negative Number</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>2.13</td> <td>-7.6, 5.6, 5.7, 6.7</td> <td>1</td> </tr> <tr> <td>1</td> <td>2.13</td> <td>-7.7, 5.1, 6.1, 6.2</td> <td>1</td> </tr> <tr> <td>2</td> <td>2.13</td> <td>-7.2, 5.7, 6.4, 6.5</td> <td>1</td> </tr> <tr> <td>3</td> <td>2.00</td> <td>-6.1, -5.8, -4.4, 5.6</td> <td>3</td> </tr> <tr> <td>4</td> <td>2.00</td> <td>-5.9, -5.4, 5.4, 6.4</td> <td>2</td> </tr> <tr> <td>5</td> <td>2.00</td> <td>-6.7, -6.0, -5.7, 6.5</td> <td>3</td> </tr> <tr> <td>6</td> <td>2.10</td> <td>-7.0, 5.3, 5.7, 6.2</td> <td>1</td> </tr> <tr> <td>7</td> <td>2.00</td> <td>-5.7, -5.2, -4.9, 5.4</td> <td>3</td> </tr> <tr> <td>8</td> <td>2.00</td> <td>-5.8, -4.8, 4.6, 5.9</td> <td>2</td> </tr> <tr> <td>9</td> <td>2.10</td> <td>-7.0, 5.1, 6.1, 7.1</td> <td>1</td> </tr> <tr> <td>42</td> <td>2.10</td> <td>-7.1, 6.0, 6.3, 6.8</td> <td>1</td> </tr> <tr> <td>2026</td> <td>2.00</td> <td>-5.1, -4.8, 4.7, 5.8</td> <td>2</td> </tr> </tbody> </table> <hr/> <h2 id="sanity-check-why-do-we-need-a">Sanity check: why do we need \(A\)?</h2> <p>When we remove \(A\) or set \(A = I\), the perplexity remains far above 2, regardless of how large the embedding dimension is. When \(A = I\) (so the embedding space is purely Euclidean), an input token has no mechanism to “un-attend” to itself. In contrast, a negative eigen-direction can serve precisely this un-attention role, a mechanism also explored in <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>. As a result, \(A\) is essential for performing the random-walk task.</p> <p>That said, when \(A = I\) (i.e., attention is completely removed) and \(n_{\rm embd} = 2\), the embeddings can evolve into extremely wild (and aesthetically pleasing) patterns. Even though these patterns may not shed much light on what is really happening in language models, it is pure pleasure to watch the resulting animations.</p> <p>Seed = 0</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_0-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_0-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 1</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_1-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_1-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 4</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_4-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_4-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 6</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_6-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_6-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Seed = 9</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-2/gif_seed_9-480.webp 480w,/assets/img/blogs/bigram-2/gif_seed_9-800.webp 800w,/assets/img/blogs/bigram-2/gif_seed_9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-2/gif_seed_9.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NmfqzshitvHwZZ3RNobwlJNH2Ebrlqj0?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 2 -- Emergence of Hyperbolic Spaces}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Bigram 1 – Walk on a Circle</title><link href="https://kindxiaoming.github.io/blog/2026/bigram-1/" rel="alternate" type="text/html" title="Bigram 1 – Walk on a Circle"/><published>2026-01-15T00:00:00+00:00</published><updated>2026-01-15T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/bigram-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/bigram-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Markov chains are simple yet remarkably powerful structures. A natural question is therefore: <strong>can transformers learn Markov chains efficiently?</strong> For a first-order Markov chain, the next state depends only on the current state, which is equivalent to a <strong>bigram</strong> structure. This post is the first in a new <em>Bigram</em> series, where we will feed various bigram datasets to transformers and study their behavior.</p> <p>In this article, we focus on one of the simplest nontrivial cases: a <strong>random walk on a circle</strong>.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong></p> <p>We consider a random walk on a 1D circular manifold, obtained via the following simplifications:</p> <ul> <li>from a 2D Manhattan map to a 2D grid (ignoring, for example, diagonal streets),</li> <li>from a 2D grid to a 1D grid,</li> <li>imposing periodicity (the added symmetry may simplify analysis).</li> </ul> <p>Suppose there are 10 points on the circle. Typical trajectories look like</p> \[[0] \rightarrow [1] \rightarrow [0] \rightarrow [9] \rightarrow [8] \rightarrow [7] \rightarrow [8] \rightarrow [7] \rightarrow \cdots\] \[[2] \rightarrow [1] \rightarrow [2] \rightarrow [3] \rightarrow [4] \rightarrow [5] \rightarrow [4] \rightarrow [3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>Since the next token depends only on the current token, a context length of 1 is sufficient. This allows us to greatly simplify the model:</p> <ul> <li>Context length = 1</li> <li>No positional embedding (positional information is unnecessary when the context length is 1)</li> </ul> <p>The dataset thus reduces to a pure bigram problem. The model consists of</p> <ul> <li>a single attention layer (1L), no MLP,</li> <li>embedding and unembedding layers with tied weights.</li> </ul> <p>Because the context length is 1, only the <strong>OV matrix</strong> plays a role. Without the OV matrix, and with tied embedding/unembedding weights, each token can only map to itself, making the random-walk task impossible.</p> <p>Our primary interest is in visualizing the evolution of the embedding vectors, as these can reveal whether a <em>world model</em>—namely, the circle—is being learned. If such a world model is learned, we would expect the tokens \(0, 1, 2, \cdots, 10\) to arrange themselves along a circle in embedding space.</p> <p>We will vary the embedding dimension \(n_{\rm embd}\). In principle, an embedding dimension of 2 should suffice, since a circle can be naturally embedded in 2D Euclidean space.</p> <hr/> <h2 id="observation-1-n_rm-embd--2-is-not-enough">Observation 1: \(n_{\rm embd} = 2\) is not enough</h2> <p>We fix the vocabulary size to 10.</p> <p>For \(n_{\rm embd} = 2\), the left plot below shows that the perplexity only decreases to about 3—corresponding to confusion among roughly three choices—whereas the optimal perplexity is 2. This indicates that \(n_{\rm embd} = 2\) is insufficient. The right plot shows that the embedding vectors do learn some notion of continuity (for example, 4 is close to 3 and 5, and 9 is close to 0 and 8), but the overall geometry is inconsistent (for instance, 1/7 lies diagonally relative to 2/6).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_2-480.webp 480w,/assets/img/blogs/bigram-1/embd_2-800.webp 800w,/assets/img/blogs/bigram-1/embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In contrast, \(n_{\rm embd} = 4\) is sufficient to reach the optimal perplexity of 2.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_234-480.webp 480w,/assets/img/blogs/bigram-1/embd_234-800.webp 800w,/assets/img/blogs/bigram-1/embd_234-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_234.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Moreover, the required embedding dimension appears to be independent of vocabulary size. For example, with vocab sizes of 100 or even 1000, the model still achieves the optimal perplexity of 2 with \(n_{\rm embd} = 4\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_4-480.webp 480w,/assets/img/blogs/bigram-1/embd_4-800.webp 800w,/assets/img/blogs/bigram-1/embd_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, once the embedding dimension reaches 4, the learned geometry becomes harder to interpret directly. Further study is needed. One possible hypothesis involves <em>nearest-neighbor hopping</em>: if we allow hopping to the nearest \(k\) neighbors, perhaps the required embedding dimension scales as \(2k\).</p> <p>To probe this further, we now study an even simpler dataset with a deterministic drift.</p> <hr/> <h2 id="observation-2-existence-of-many-equivalent-embeddings">Observation 2: existence of many equivalent embeddings</h2> <p><strong>Dataset</strong> We restrict the walk to move only to the right. For example, \([5]\) can only transition to \([6]\) and never to \([4]\). In this case, the prediction task becomes deterministic. We find that \(n_{\rm embd} = 2\) is sufficient to achieve zero loss and perplexity 1.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/drift_embd_2-480.webp 480w,/assets/img/blogs/bigram-1/drift_embd_2-800.webp 800w,/assets/img/blogs/bigram-1/drift_embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/drift_embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Interestingly, there are many equivalent embedding configurations that solve this task, and the model converges to one of them depending on initialization.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/bigram-1/embd_zoo-480.webp 480w,/assets/img/blogs/bigram-1/embd_zoo-800.webp 800w,/assets/img/blogs/bigram-1/embd_zoo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/bigram-1/embd_zoo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/18QzDHiSfVYcO3HhyY5Y3uPup0hybWNTh?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026bigram-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Bigram 1 -- Walk on a Circle}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/bigram-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Diffusion 1 – Sparse and Dense Neurons</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-1/" rel="alternate" type="text/html" title="Diffusion 1 – Sparse and Dense Neurons"/><published>2026-01-14T00:00:00+00:00</published><updated>2026-01-14T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading <a href="https://arxiv.org/abs/2506.01912">this paper, <em>“Unconditional CNN denoisers contain sparse semantic representation of images”</em></a>, which reports an intriguing observation: some neurons (CNN channels) are sparsely activated, while others are densely activated. The authors interpret this as follows: sparse neurons correspond to specific semantic features (e.g., a dog), and therefore activate only when that feature is present in the input image; dense neurons, by contrast, capture more global properties such as lighting, and hence activate across almost all images.</p> <p>The goal of this article is to propose a <strong>minimal toy setup</strong> in which such a division of labor naturally emerges.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> We consider a simple scenario in which a “dog” may or may not appear under various “lighting conditions.” We model the dog as a Gaussian packet and the lighting as a constant offset. An “image” (a 1D function) is therefore given by \(I(x) = L + A {\rm exp}(-\frac{-(x-x_c)^2}{2 w^2}),\) where \(L\sim U[0,0.5]\), \(A\sim {\rm Bern}(p)\) (with probability \(p=0.2\) of being 1 and probability \(1-p=0.8\) of being 0), \(x_c\sim U[-0.8,0.8]\), and \(w=0.05\). We sample 100 such “images.”</p> <p><strong>Model</strong><br/> We adopt a 1D U-Net architecture. The input dimension is 128. The network consists of 3 downsampling blocks (each producing 8 output channels) and 3 upsampling blocks. We deliberately choose a small number of channels (8), since the task is simple and our goal is to understand what each individual channel is doing. The U-Net is trained as a denoiser with noise level \(\sigma = 0.1\) (again for simplicity).</p> <hr/> <h2 id="output-of-the-first-downsampling-block">Output of the first downsampling block</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block1-480.webp 480w,/assets/img/blogs/diffusion-1/block1-800.webp 800w,/assets/img/blogs/diffusion-1/block1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Each column corresponds to one neuron (channel), for a total of 8 channels.</p> <ul> <li><strong>First row</strong>: activation distribution of each neuron over the dataset.</li> <li><strong>Second row</strong>: activation versus lighting.</li> <li><strong>Third row</strong>: activation versus \(A\) (1 when the Gaussian is present, 0 otherwise).</li> </ul> <p>We find four distinct types of neurons:</p> <ul> <li><strong>Dead neurons</strong>: neurons 1, 2, 3, and 6.</li> <li><strong>Pure lighting neuron</strong>: neuron 0.</li> <li><strong>Pure Gaussian-detection neurons</strong>: neurons 4 and 7.</li> <li><strong>Mixed lighting + Gaussian detection neuron</strong>: neuron 5.</li> </ul> <p>We also include results for blocks 2 and 3.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block2-480.webp 480w,/assets/img/blogs/diffusion-1/block2-800.webp 800w,/assets/img/blogs/diffusion-1/block2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In block 2, neurons 2–7 are all dead. Neuron 0 is selective to the Gaussian, while neuron 1 exhibits mixed behavior.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-1/block3-480.webp 480w,/assets/img/blogs/diffusion-1/block3-800.webp 800w,/assets/img/blogs/diffusion-1/block3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-1/block3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In block 3, neurons 0, 2, and 5 are dead. Neurons 1 and 3 show mixed behavior, while neurons 4, 6, and 7 are selective to the Gaussian.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1tHISSSA3ldoY2wTTfAMtD6J1H2BlPC_a?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Diffusion 1 -- Sparse and Dense Neurons}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 4 – previous token head</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-4/" rel="alternate" type="text/html" title="Sparse attention 4 – previous token head"/><published>2026-01-13T00:00:00+00:00</published><updated>2026-01-13T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/sparse-attention-2/">sparse-attention-2</a>, we found that a single-layer attention model <strong>without positional embeddings cannot</strong> reliably copy any earlier token based on position. In this article, we demonstrate how positional embeddings enable the model to learn a <em>previous-token head</em>.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to copy the previous token. Taking context length = 4 as an example, this corresponds to<br/> \([A][B][C][D] \rightarrow [C]\).</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>, with the addition of a positional embedding layer. The model consists only of a Token Embedding layer, a Positional Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers.</p> <hr/> <h2 id="with-positional-embeddings-the-previous-token-head-can-be-easily-learned">With positional embeddings, the previous-token head can be easily learned</h2> <p>We choose context length 4, vocab size 30, and embedding dimension 2. The left plot shows that the task cannot be learned without positional embeddings. The middle plot shows that the task can be reasonably learned with positional embeddings. The right plot shows the evolution of the positional embeddings: the positional embedding of the previous token (-1) moves in the opposite direction from tokens at other positions (0, -2, -3). The separation direction is roughly \(s = (1,1)^T\). When projecting positional embeddings along \(s\), \(p_{-1}\) is negative, while \(p_0, p_{-2}, p_{-3}\) are positive.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/compare_pos-480.webp 480w,/assets/img/blogs/sparse-attention-4/compare_pos-800.webp 800w,/assets/img/blogs/sparse-attention-4/compare_pos-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/compare_pos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="whats-happening">What’s happening?</h2> <p>We can compute the \(W_QW_K^T\) matrix, obtaining</p> \[W_QW_K^T = \begin{pmatrix} -0.41 &amp; 1.35 \\ 0.19 &amp; -2.25 \\ \end{pmatrix}.\] <p>Note that \(s^T W_QW_K^T s = -1.1 &lt; 0\). If two positional embeddings have the same (opposite) sign along \(s\), they will receive less (more) attention. As a result, since only \(p_{-1}\) has the opposite sign relative to \(p_0\), the attention is biased toward the previous token.</p> <hr/> <h2 id="hyperparameter-dependence">Hyperparameter dependence</h2> <p>However, the task is not solved exactly, but only approximately. With larger vocab size or larger context length, the task becomes harder for the model to approximate, so the relative perplexity \(({\rm perplexity} - 1)/V\) increases. In contrast, a larger embedding dimension helps reduce the relative perplexity.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/hyper-dependence-480.webp 480w,/assets/img/blogs/sparse-attention-4/hyper-dependence-800.webp 800w,/assets/img/blogs/sparse-attention-4/hyper-dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/hyper-dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I want to argue that the need for higher embedding dimensions suggests inefficiency. Ideally, a 1D positional embedding should suffice if the attention kernel is chosen appropriately (here the attention kernel is the inner product).</p> <hr/> <h2 id="dependence-on-learning-rate">Dependence on learning rate</h2> <p>When \(V = 30\), a learning rate of 0.1 is faster than 0.01. However, when \(V = 100\), a learning rate of 0.1 leads to slower learning than 0.01.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/lr_dependence-480.webp 480w,/assets/img/blogs/sparse-attention-4/lr_dependence-800.webp 800w,/assets/img/blogs/sparse-attention-4/lr_dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/lr_dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It appears that lr = 0.1 still learns the previous-token head (since there exists a separation direction in the positional embeddings):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/big_lr-480.webp 480w,/assets/img/blogs/sparse-attention-4/big_lr-800.webp 800w,/assets/img/blogs/sparse-attention-4/big_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/big_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, because the learning rate is too large, the token embeddings fluctuate wildly and fail to converge to a maximally separable solution (nearby points are placed equidistantly on a circle). The learning rate is so large that tokens swap positions. The loss spike around 6000 steps corresponds to this swapping process, during which the model is <strong>confidently wrong</strong> for some tokens, leading to very large losses. This GIF illustrates the behavior more clearly:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-4/gif2-480.webp 480w,/assets/img/blogs/sparse-attention-4/gif2-800.webp 800w,/assets/img/blogs/sparse-attention-4/gif2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-4/gif2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that the tokens in this task have no semantic meaning. The reason they form a circle is simply maximal distinguishability, and their ordering is random. The loss spike corresponds to jumping from one order to another order.</p> <hr/> <h2 id="learning-rate-decay">Learning rate decay</h2> <p>The observation above suggests a possible reason for why <strong>learning rate decay</strong> is needed. When two token embeddings are very close to each other, the learning rate should be small enough so that (i) swampping cannot happen (be trapped in one basin of attraction), otherwise creating loss spikes and (ii) can converge smoothly to the bottom of the basin of attraction (maximal separation of token embeddings). In this article, all tokens have the same frequency so they form a circle due to symmetry. But for natural languages, tokens have different frequencies and so different token embeddings may have different norms, requiring different learning rates. How we can adjust learning rates based on token frequency (which can be easily known) is investigated in future posts.</p> <hr/> <h2 id="generality">Generality</h2> <ul> <li>Although we exemplify the analysis with the previous token (the token right before the current token), the analysis applies to any earlier token at any position, e.g., 3 tokens away in the past.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1AFKB8DcToRncxwE2vI_g0tkLzHg02iRm?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 4 -- previous token head}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 3 – inefficiency of extracting similar content</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-3/" rel="alternate" type="text/html" title="Sparse attention 3 – inefficiency of extracting similar content"/><published>2026-01-12T00:00:00+00:00</published><updated>2026-01-12T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/sparse-attention-1/">Sparse-attention-1</a>, we showed that a single-layer attention model (without positional embeddings) can learn to copy the <strong>current</strong> token. In <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>, we showed that the same model is unable to extract a specific previous token based on <em>position</em>, e.g., the last token. But what about extracting a previous token based on <em>content</em> (semantic meaning)? This is a more natural task, because it is precisely what attention is designed to do—attend to similar content.</p> <p>As we show below, somewhat surprisingly, a single attention layer is very inefficient at performing this extraction task.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to extract the token that is most similar to the current token. We assume tokens have numerical meanings; for example, token \([5]\) represents the number \(5\). Then \([5]\) is closer to \([6]\) than to \([8]\) because \(1=|5-6| &lt; |5-8|=3.\)</p> <p>Taking context length = 4 as an example, the task is to predict \([1][5][10][6] \rightarrow [5]\).<br/> This is because \([5]\) is the closest to \([6]\), than \([1]\) or \([10]\).</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>. The model consists only of an Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers and no positional embeddings.</p> <hr/> <h2 id="failure-even-for-context-length--3-with-small-embedding-dimension">Failure even for context length = 3 (with small embedding dimension)</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e., \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 3 (task: \([1][5][6]\to[5]\)), as we vary the vocabulary size, we consistently observe that perplexity converges to \(V/2\) (or lower, for smaller \(V\)).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_2-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_2-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By visualizing the learned embeddings, we find that they exhibit a continuous structure; that is, numerically closer tokens are embedded closer together:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_final_2D-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_final_2D-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_final_2D-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_final_2D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-vocab-size-v">Dependence on vocab size \(V\)</h2> <p>The existence of structure may explain why the perplexity can fall slightly below \(V/2\): the continuous geometric structure is partially learned and leveraged to reduce the loss. However, the model largely becomes confused (similar to what we observed in the <a href="/blog/2026/sparse-attention-2/">previous blog</a>) and effectively guesses a random token from the previous context. This leads to a perplexity of \(\frac{C-2}{C-1}V\), which qualitatively (though not quantitatively) matches the experimental results:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/perplexity_C-480.webp 480w,/assets/img/blogs/sparse-attention-3/perplexity_C-800.webp 800w,/assets/img/blogs/sparse-attention-3/perplexity_C-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/perplexity_C.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-embedding-dimension">Dependence on embedding dimension</h2> <p>We find that the best achievable perplexity decreases with \(n_{\rm embd}\), in fact faster than \(V/n_{\rm embd}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/n_embd_dependence-480.webp 480w,/assets/img/blogs/sparse-attention-3/n_embd_dependence-800.webp 800w,/assets/img/blogs/sparse-attention-3/n_embd_dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/n_embd_dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This may be explained by the increasingly improved geometry of the embeddings (as seen in the first two principal components) as we increase \(n_{\rm embd}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_geometry-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_geometry-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_geometry-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_geometry.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="questions--ideas">Questions / Ideas</h2> <ul> <li>Our results demonstrate the ineffectiveness of attention in extracting similar content in this setting.</li> <li>One possible fix is to replace the inner product of Q/K with the Euclidean distance between Q and K. Ideally, a 1D embedding would already be sufficient if the kernel computes Euclidean distances and weighs probabilities inversely proportional to distance (similar to <a href="https://arxiv.org/abs/2502.01628v1">harmonic loss</a>). The innner-product in attention probably produces the inefficiency, and we should try other distance measures.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1k8IEg_zc12_8XHOE0SjWv5LyeaZBC5cK?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 3 -- inefficiency of extracting similar content}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Emergence of Induction Head Depends on Learning Rate Schedule</title><link href="https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule/" rel="alternate" type="text/html" title="Emergence of Induction Head Depends on Learning Rate Schedule"/><published>2026-01-11T00:00:00+00:00</published><updated>2026-01-11T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule/"><![CDATA[<p><strong>Author: Qingyu Qu (屈清宇), Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Induction heads are crucial for in-context learning abilities. However, the dependence of its emergence on learning rate schedule is fewly discussed. Indeed, at first glance, the emergence of induction heads should depend more, if not exclusively, on <strong>structure of data</strong>. But it makes sense to think of the <strong>learning rate schedule</strong> because such a delicate module like induction head should not use a large learning rate to construct. Below we make an early exploration of the dependence of induction head emergence on learning rate schedule.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We adopt a similar setup as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>, where a two layer transformer(with mlps) is trained on openwebtext data. The details are as follows.</p> <p><strong>Model</strong><br/> The model here is a two layer transformer(with mlps). Embedding dimension is 768, number of heads per layer is 12, these numbers are chosen to be the same as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>. Weight tying between embedding and unembedding is used. The implementation of the model is a modified version of <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>. The train loop is also basically train.py in <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>.</p> <p><strong>Dataset</strong><br/> We use <a href="https://huggingface.co/datasets/Skylion007/openwebtext">openwebtext</a> as the training dataset, because we found that natural language data is important for induction head emergence. We also found many papers arguing that induction heads emerge from synthetic data, but our “induction heads” emerging from synthetic data can rarely perform well on repeated random sequences, which we think is an important ability of induction heads shown in <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>.</p> <p><strong>Induction score</strong> To measure the emergence of induction heads, we adopt a similar strategy as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>, which they called prefix matching score. We test the model checkpoint on a repeated random sequence, and calculate a head’s attention weight given to the token we expect an induction head to attend to – the token where the prefix matches the present context. We adopt the largest induction score among all heads to be the induction score of the model. In other words, a large induction score means that there is at least one head showing induction attention pattern on repeated random sequences.</p> <hr/> <h2 id="fixed-learning-rate-fails-to-give-rise-to-induction-heads">Fixed learning rate fails to give rise to induction heads.</h2> <p>We train the same model using the same data. The only difference is that the learning rate is among \(\{6\times 10^{-5}, 6\times 10^{-4}, 6\times 10^{-3}\}\). We train the model until the accumulated learning rate reaches threshold 3, i.e. we train the model for \(\{5\times 10^4, 5\times 10^3, 5\times 10^2\}\) steps. The goal of this trick is to balance the time effect introduced by learning rate, i.e. 1 step under 0.1 learning rate is roughly 100 steps under 0.01 learning rate.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/fix_lr-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/fix_lr-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/fix_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/fix_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As shown by the figure, the induction score is low. In fact, in our setup, a random sequence of length 10 is repeated 5 times, so the random guess induction score is around 0.1. One might argue we do not train enough, but below we show that we can get induction heads with these many steps(or accumulated learning rates) using learning rate warmup.</p> <hr/> <h2 id="learning-rate-warmup-to-the-right-plateau-is-important-for-induction-heads-emergence">Learning rate warmup to the right plateau is important for induction heads emergence</h2> <p>In this section, we first warm up the learning rate for 100 steps to a plateau among \(\{6\times 10^{-5}, 6\times 10^{-4}, 6\times 10^{-3}\}\), then the learning rate will be stable until the end of training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/warmup_lr-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/warmup_lr-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/warmup_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/warmup_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that, in our experiment, the plateau learning rate can be neither too large nor too small. One may doubt that our training in \(6\times 10^{-3}\)case is not enough. The following figure shows that, even though we train 5000 steps in \(6\times 10^{-3}\)case, induction heads still miss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/verify-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/verify-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/verify-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/verify.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="learning-rate-warmup-iters-affect-the-speed-of-emergence-of-induction-heads">Learning rate warmup iters affect the speed of emergence of induction heads</h2> <p>Since we found that \(6\times 10^{-4}\) is the right plateau for learning rate warmup, we test the learning rate schedule with different warmup iterations to this plateau. The result is as follows.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/warmup_steps-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/warmup_steps-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/warmup_steps-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/warmup_steps.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As shown by the figure above, warmup iter=100 leads to the fastest emergence of induction heads.</p> <hr/> <h2 id="learning-rate-decay-has-little-effect-on-induction-head-emergence">Learning rate decay has little effect on induction head emergence.</h2> <p>This may sound counter-intuitive since learning rate decay is crucial for constraining the model not to overfit the data, which will destroy the generalisation solution like induction head. However, for a small model trained on a big dataset like openwebtext, maybe overfitting can hardly happen, and this is also verified by the train and validation loss are always the same during the course of training. At the same time, learning rate decay has little effect on the final accumulated learning rate since most of the accumulated learning rate is contributed by the peak learning rate. Below, we train the model for 5000 steps and ignore the little difference in final accumulated learning rate.</p> <p>We use a linear warmup of 100 steps and use cosine decay to a min_lr.</p> <p>If the plateau learning rate = \(6\times 10^{-3}\), induction heads will not emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/decay_6e-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If the plateau learning rate = \(6\times 10^{-4}\), induction head will emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/decay_6e-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qu2026induction-head-lr-schedule</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Emergence of Induction Head Depends on Learning Rate Schedule}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Qu, Qingyu; Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/induction-head-lr-schedule/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Qingyu Qu (屈清宇), Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 2 – Unattention head, branching dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/" rel="alternate" type="text/html" title="Sparse attention 2 – Unattention head, branching dynamics"/><published>2026-01-10T00:00:00+00:00</published><updated>2026-01-10T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/sparse-attention-1/">previous blog</a>, we found that a single-layer attention model (without positional embeddings) can learn to copy the <strong>current</strong> token. But what about copying <strong>earlier</strong> tokens? Since positional embeddings are missing, it seems impossible to determine where each token is located in the sequence. Under this reasoning, the task should be unsolvable, and I would expect the loss to be \({\rm log}(V)\), or equivalently the perplexity to be \(V\), where \(V\) is the vocabulary size.</p> <p>As we show below, however, the story is more subtle.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to copy the previous token. Taking context length = 4 as an example, this corresponds to<br/> \([A][B][C][D] \rightarrow [C]\).<br/> In fact, due to the absence of positional embeddings, and because attention is equivariant under permutations of previous tokens, copying <em>any</em> previous token defines an equivalently difficult task.</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>. The model consists only of an Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers.</p> <hr/> <h2 id="observation-loss-plateau-embedding-branching-dynamics">Observation: loss plateau, embedding branching dynamics</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 2 (task: \([A][B]\to[A]\)), as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em> (as in the <a href="/blog/2026/sparse-attention-1/">previous blog</a>), where the perplexity is approximately half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/perplexity-embed-480.webp 480w,/assets/img/blogs/sparse-attention-2/perplexity-embed-800.webp 800w,/assets/img/blogs/sparse-attention-2/perplexity-embed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/perplexity-embed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By visualizing the evolution of the embeddings, we find that the dynamics proceed in two stages. In the first stage, there is a dominant direction along which the embeddings expand. Only in the second stage do the embeddings begin to move along the orthogonal direction, eventually forming a circular structure. This second stage also exhibits interesting branching dynamics:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/gif-480.webp 480w,/assets/img/blogs/sparse-attention-2/gif-800.webp 800w,/assets/img/blogs/sparse-attention-2/gif-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/gif.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="whats-happening">What’s happening?</h2> <p>My first reaction was: how could this task possibly be solved? The low final loss clearly indicates that the model must have developed some internal algorithm to accomplish it. The emergence of circular embeddings is particularly suggestive, hinting that the model has learned something nontrivial and elegant.</p> <p>We can inspect the learned \(W_Q/W_K\) matrices (\(W_V\) is unimportant here since we only care about the attention pattern):</p> \[W_Q = \begin{pmatrix} 13.24 &amp; -3.40 \\ -3.28 &amp; -14.7 \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -14.03 &amp; 3.56 \\ 3.19 &amp; 13.99 \\ \end{pmatrix}\] <p>We observe two numerical clusters (ignoring signs): one around 13 (denoted \(a\)), and another around 3 (denoted \(b\)). We therefore approximate them by the following symbolic forms:</p> \[W_Q = \begin{pmatrix} a &amp; -b \\ -b &amp; -a \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -a &amp; b \\ b &amp; a \\ \end{pmatrix},\] <p>Let the two inputs to the attention layer be \(E_1 = (x_1, y_1)^T\) and \(E_2 = (x_2, y_2)^T\). The attention logit between them is</p> \[E_2 W_K^T W_Q E_1 = (a x_1 - b y_1,\,-b x_1 - a y_1) \cdot (-a x_2 + b y_2,\, b x_2 + a y_2) = -(a^2 + b^2)(x_1 x_2 + y_1 y_2).\] <p>Since both \(E_1\) and \(E_2\) lie on a circle, the term \(x_1 x_2 + y_1 y_2\) is simply their inner product. The self-attention logit is</p> \[E_2 W_K^T W_Q E_2 = -(a^2 + b^2)(x_2^2 + y_2^2) &lt; E_2 W_K^T W_Q E_1.\] <p>This means that the model attends more strongly to the previous token than to the current token. Importantly, however, this effect is achieved not by <em>increasing</em> attention to the previous token, but by <em>suppressing</em> attention to the current token. This distinction is subtle for context length = 2, but becomes crucial for longer contexts.</p> <hr/> <h2 id="longer-context-length">Longer context length</h2> <p>The attention layer has discovered a way to <em>unattend</em> the current token, but it still cannot distinguish among earlier tokens due to permutation equivariance in the absence of positional embeddings. As a result, the model can do no better than guessing: for context length \(C\), it effectively selects one token uniformly at random from the first \(C-1\) tokens, with probability \(\frac{1}{C-1}\) of choosing any particular one. The probability of guessing the wrong token is therefore \(\frac{C-2}{C-1}\).</p> <p>With vocabulary size \(V\), the best achievable perplexity is thus \(\frac{C-2}{C-1} V\), which is better than pure random guessing \(V\). We verify empirically below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/best_perplexity-480.webp 480w,/assets/img/blogs/sparse-attention-2/best_perplexity-800.webp 800w,/assets/img/blogs/sparse-attention-2/best_perplexity-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/best_perplexity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This suggests that for long sequences, an attention layer without positional embeddings cannot reliably select a specific previous token. Positional embeddings are therefore necessary to implement, for example, a previous-token head. That said, the discovered strategy of <em>unattending the current token</em> is itself an interesting and nontrivial phenomenon.</p> <hr/> <h2 id="acknowledgement">Acknowledgement</h2> <p>Kaiyue Wen suggests that the sticky plateau might be due to the fact that Sub-n-grams are near-stationary Points (<a href="https://arxiv.org/pdf/2508.12837v1">paper</a>).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/135t06Hz2FCrEKiTVErR8P4_MiPhl4pnE?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 2 -- Unattention head, branching dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 1 – sticky plateau and rank collapse</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/" rel="alternate" type="text/html" title="Sparse attention 1 – sticky plateau and rank collapse"/><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>The idea of attention is to select what we want from a collection of items. Token embeddings select based on content itself, while positional embeddings select based on position.</p> <p>In this article, we examine attention’s ability to select based on content. Therefore, for simplicity, we ignore positional embeddings.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is: select the current token. Taking context length = 4 as an example, this is<br/> \([A][B][C][D] \rightarrow [D]\).</p> <p><strong>Model</strong><br/> The model consists of only Embedding, Unembedding, and a single Attention layer, with no MLP layers. The main tunable parameters are the vocabulary size, embedding dimension, and context length.</p> <hr/> <h2 id="observation-sticky-plateau">Observation: sticky plateau</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 4, as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em>, where the perplexity corresponds to half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/sticky-plateau-480.webp 480w,/assets/img/blogs/sparse-attention-1/sticky-plateau-800.webp 800w,/assets/img/blogs/sparse-attention-1/sticky-plateau-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/sticky-plateau.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Explanation</strong><br/> We visualize the evolution of the embeddings (vocab size = 4) and find that there are two stages. In the first stage, the blue and yellow directions coincide, and the green and red directions coincide. As a result, the model cannot distinguish blue from yellow, or green from red, but it can distinguish which group a token belongs to. In the second stage, the elements within each group are finally separated.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/branching-480.webp 480w,/assets/img/blogs/sparse-attention-1/branching-800.webp 800w,/assets/img/blogs/sparse-attention-1/branching-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/branching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="how-general-is-the-sticky-plateau">How general is the sticky plateau?</h2> <p>Although the sticky plateau phenomenon is interesting, how general is it? When the embedding dimension increases, there is enough resolution to distinguish different tokens, and the plateau may disappear. We indeed observe that this is the case. However, if we increase the context length, the sticky plateau comes back again. The picture, therefore, is that packing as many items as the context length into an embedding space of a given dimension leads to a situation where the presence or absence of a sticky plateau seems to correspond to a percolation phase transition.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/generality-480.webp 480w,/assets/img/blogs/sparse-attention-1/generality-800.webp 800w,/assets/img/blogs/sparse-attention-1/generality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/generality.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="rank-collapse">Rank collapse</h2> <p>From the embedding visualizations above, we find that the embeddings initially expand along a single direction, and only later expand along another direction. Therefore, we expect that the effective dimensionality of the embeddings (defined in the same way as in the previous <a href="/blog/2026/unigram-toy-1/">blog post</a>), as well as the effective rank of the Q/K/V matrices, should exhibit corresponding dynamics.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/eff_rank-480.webp 480w,/assets/img/blogs/sparse-attention-1/eff_rank-800.webp 800w,/assets/img/blogs/sparse-attention-1/eff_rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/eff_rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Left: Q/K also appear very sticky during the loss plateau, which may indicate that the dynamics of Q/K are the primary cause of the loss plateau.</p> <p>Right: when all parameters are scaled up, Q/K become less sticky. Another observation that differs from the low-dimensional case is that when the loss decreases, the rank also decreases; when the loss plateaus, the rank increases; and when the loss decreases again, the rank decreases accordingly. The picture is that when the model is very clear about which features are useful, it aggressively exploits (grows) those features while (relatively) suppressing others, leading to a decrease in rank. When the loss reaches a plateau, the model explores which new features should grow, leading to an increase in rank. Once a useful new feature emerges, the model again exploits it, causing the rank to decrease.</p> <hr/> <h2 id="test-on-large-models">Test on large models</h2> <p>The sticky plateau phenomenon predicts a loss plateau at \({\rm Log}(V/2)\), about 0.3 nats below \({\rm Log}(V)\). To the best of my knowledge, this plateau has never been observed in LLMs, maybe due to</p> <ul> <li>learning rate warmup has already nicely handled this problem</li> <li>we did not zoom in enough to see the plateau</li> <li>the sticky plateau is simply less relevant for large models</li> </ul> <p>Either way, no matter if the sticky plateua is relevant for LLMs, we shoud better understand/control representation/rank collapses and the percolation phase transition.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1lXsAuzQ2nw009an8lNt9PYv_HqR1GhoV?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 1 -- sticky plateau and rank collapse}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>