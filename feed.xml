<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-06T08:05:44+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On the physical interpretation of drifting generative models</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-3/" rel="alternate" type="text/html" title="On the physical interpretation of drifting generative models"/><published>2026-02-06T00:00:00+00:00</published><updated>2026-02-06T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading with interest the paper <a href="https://arxiv.org/pdf/2602.04770">“Generative Modeling via Drifting”</a>, as its use of attraction and repulsion is closely related to our earlier work on <a href="https://arxiv.org/abs/2209.11178">PFGM</a> and <a href="https://arxiv.org/abs/2302.04265">PFGM++</a>.</p> <p>The goals of this blog are to:</p> <ul> <li>provide a clear <strong>physical interpretation</strong> of drifting generative models;</li> <li>present <strong>visualizations across many settings</strong>, offering guidance on kernel choice and highlighting potential failure modes.</li> </ul> <hr/> <h2 id="physical-interpretation">Physical interpretation</h2> <p>The involvement of neural networks can obscure the intuition, so we first remove them from the story.</p> <p>The physical picture is straightforward:</p> <ul> <li>There are \(N\) positively charged particles (the data), fixed in space.</li> <li>There are \(N\) negatively charged particles, randomly initialized.</li> <li>The negative particles are released and move according to a force field determined by interactions with positive particles (\(V^+\)) and with other negative particles (\(V^-\)). For any pair of particles, the force magnitude is \(f(r)\) (corresponding to their kernel \(K\)). The paper uses \(K(r) = \exp\!\left(-\frac{r}{\tau}\right),\) which corresponds to a Yukawa-type potential. Negative charges are attracted to positive charges and repelled by other negative charges.</li> <li>Intuitively, the final equilibrium should consist of one-to-one positive–negative pairs, which appear neutral to the outside world, yielding a net force \(V = 0\).</li> </ul> <p>Importantly, the evolution of the negative charges is entirely determined by the force field and does <strong>not</strong> require a neural network. The role of the network is simply to <em>learn and approximate</em> this evolution, so that it can generalize to unseen configurations at inference time.</p> <p>In what follows, I focus purely on visualizing the forward dynamics, without training any neural network.</p> <hr/> <h2 id="dependence-on-tau">Dependence on \(\tau\)</h2> <p>The paper uses the force \(f(r) = r \exp\!\left(-\frac{r}{\tau}\right).\) We first illustrate how the dynamics depend on the parameter \(\tau\).</p> <h3 id="tau02">\(\tau=0.2\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tau10">\(\tau=1.0\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tau50">\(\tau=5.0\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="other-force-fields">Other force fields</h2> <p>We also explore alternative force laws to understand how sensitive the dynamics are to kernel choice.</p> <h3 id="fr1r01">\(f(r)=1/(r+0.1)\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_inv_r_0d1_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="fr1r1">\(f(r)=1/(r+1)\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_inv_r_1_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="frr">\(f(r)=r\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="frr2">\(f(r)=r^2\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r%5E2_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r%5E2_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r%5E2_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r%5E2_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r%5E2_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r%5E2_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="explaining-balance">Explaining balance</h2> <p>The paper reports that the combination \(V^+ - V^-\) performs best, which is exactly what the physical picture suggests. For alternatives such as \(2V^+ - V^-\) or \(V^+ - 2V^-\), the system is no longer neutral. As a result, the negative particles either collapse toward the center of the positive particles or are driven too far away.</p> <p>We therefore focus on \(f(r) = r \exp(-r),\) and verify this intuition through toy experiments.</p> <h3 id="v---2v-">\(V^+ - 2V^-\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/1v+_2v-_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="2v---v-">\(2V^+ - V^-\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/2v+_1v-_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="more-examples">More examples</h2> <p>Finally, we keep \(f(r) = r \exp(-r)\) fixed and vary the data distribution.</p> <h3 id="circle">Circle</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/circle_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/circle_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/circle_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/circle_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/circle_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/circle_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="square">Square</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/square_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/square_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/square_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/square_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/square_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/square_trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Overall, drifting dynamics appear surprisingly intricate. While these “failure modes” may not necessarily arise in practical settings, further study is needed to understand how kernels should be chosen and what failure modes may emerge.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Kn4sDVouuwZpb1Nhe9bqtwQu2Tj0-Qw9?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{On the physical interpretation of drifting generative models}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics 2 – Transformers fail to maintain physical cosistency for circular motion</title><link href="https://kindxiaoming.github.io/blog/2026/physics-2/" rel="alternate" type="text/html" title="Physics 2 – Transformers fail to maintain physical cosistency for circular motion"/><published>2026-02-05T00:00:00+00:00</published><updated>2026-02-05T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Most current world models and video generative models suffer from <strong>physical or semantic inconsistency</strong>. While a generated video may appear coherent for a few seconds, it often starts producing inconsistent content over longer horizons—even though the new content may still look locally plausible. The origin of this inconsistency is difficult to pin down, because both the data (natural scenes) and the models are largely black boxes.</p> <p>In this article, we instead study a <strong>toy dataset</strong>—circular motion—which is highly controlled and well understood. By training transformers on this toy dataset, we uncover failure modes in the generated trajectories that closely resemble the inconsistency problems observed in large-scale world models.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider a circular motion dataset defined as follows. The radius is sampled as \(r \sim U[0.15, 0.35]\), and the angle evolves as \(\varphi_t = \omega t + \varphi_0,\) where \(\varphi_0 \sim U[0, 2\pi]\), the angular velocity is \(\omega = 0.5\), and \(t = 1, 2, \ldots, 50\). This corresponds to Cartesian coordinates \(x_t = r \cos \varphi_t, \quad y_t = r \sin \varphi_t.\)</p> <p>We follow the tokenization scheme of <a href="https://arxiv.org/pdf/2507.06952">Vafa et al.</a>. Each coordinate pair \((x, y)\) is discretized into tokens, with \(x\) and \(y\) tokenized independently. Specifically, the \(x\) coordinate is mapped to \(\lfloor N x \rfloor\), and the \(y\) coordinate is mapped to \(\lfloor N y \rfloor + N\). In total, this yields \(2N\) distinct tokens.</p> <p>The transformer takes in and outputs <strong>two tokens per time step</strong> (corresponding to \(x\) and \(y\)), rather than a single token. The training objective is the standard next-token(s) prediction cross-entropy loss.</p> <hr/> <h2 id="results">Results</h2> <p>After training, we condition the model on the first 10 points (shown in red), and then generate the next 90 points autoregressively (shown in blue).</p> <p>We show results for models of different sizes below.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_8_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_8_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_32_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_32_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_128_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_128_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_128_n_layer_2-480.webp 480w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-800.webp 800w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_128_n_layer_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <p>When the model is sufficiently expressive, we observe the following behavior:</p> <ul> <li>During the first few generation steps, the object follows the correct circular trajectory.</li> <li>Over longer generation horizons, the object remains approximately on the circle, but begins to jump all over it.</li> </ul> <p>We hypothesize that:</p> <ul> <li>The transformer is able to learn a <strong>conservation law</strong>—in this case, conservation of radius—and infer its value from the context.</li> <li>While the model initially follows the correct motion law (circular motion), it fails to maintain this dynamics over long contexts, leading to trajectory inconsistency.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1YHcqwvAk28M60XXwa9bJM_XXnv4-o77x?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026physics-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics 2 -- Transformers fail to maintain physical cosistency for circular motion}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics 1 – Attention can’t exactly simulate uniform linear motion</title><link href="https://kindxiaoming.github.io/blog/2026/physics-1/" rel="alternate" type="text/html" title="Physics 1 – Attention can’t exactly simulate uniform linear motion"/><published>2026-02-04T00:00:00+00:00</published><updated>2026-02-04T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a recent paper, <a href="https://arxiv.org/pdf/2507.06952">“What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models”</a>, the authors show that transformers fail to learn planetary motion governed by Newton’s second law under Newtonian gravity. However, the underlying failure modes are not well understood.</p> <p>As a physicist, I would like to further simplify the setup while preserving the failure mode. In this blog post, we study whether an attention layer can learn <strong>Newton’s first law</strong>—uniform linear motion. Given the current state \(x_t\) and the previous state \(x_{t-1}\), the next state is \(x_{t+1} = 2x_t - x_{t-1}.\) While this relation is trivial for an MLP mapping \((x_{t-1}, x_t) \mapsto x_{t+1}\) (since it is linear), it is not immediately clear whether an attention layer can learn it efficiently or exactly, because:</p> <ul> <li>linear coefficients must be constructed through softmax attention;</li> <li>softmax attention produces positive weights, whereas the coefficients 2 and −1 have opposite signs.</li> </ul> <p>These concerns may not be fatal, but they are suspicious enough to warrant a closer look through toy experiments.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>As Einstein put it, we want a model that is <em>as simple as possible, but not simpler</em>. In this spirit, I start from the simplest possible models, gradually add complexity (sometimes removing it again), and eventually arrive at an architecture that can perform the task reasonably well. I then inspect the weights and representations to understand what kind of algorithm has emerged.</p> <p>The exploration process is summarized below. Readers who are only interested in the final result can safely skip this part.</p> <ul> <li>I start with <code class="language-plaintext highlighter-rouge">input_dim = 3</code> (since Euclidean space is 3D), using a 1-layer, attention-only model with a single head and no positional embeddings. It does not learn.</li> <li>To break the symmetry between positions (coefficients 2 and −1), I add absolute positional embeddings (APE). It still does not learn.</li> <li>I then suspect the model is too narrow, since I tied the embedding dimension to the input dimension (3). I decouple them by adding a projection-up layer from 3D to 20D and a projection-down layer from 20D back to 3D. Now it finally learns.</li> <li>The model now feels overly complex, so I begin simplifying. I reduce <code class="language-plaintext highlighter-rouge">input_dim</code> from 3 to 1. It still learns (not surprising, since the task is easier).</li> <li>I reduce the embedding dimension to 4. It still learns.</li> <li>I reduce the embedding dimension to 2. It fails for one random seed, but succeeds for another.</li> <li>I reduce the embedding dimension to 1. With a lucky random seed (after trying a few), it can learn.</li> <li>With both <code class="language-plaintext highlighter-rouge">input_dim = 1</code> and <code class="language-plaintext highlighter-rouge">embedding_dim = 1</code>, the projection-up and projection-down layers seem unnecessary. However, after removing both, the model fails for all 10 random seeds I tried. If I remove only the projection-down layer but keep projection-up, it can still learn.</li> <li>I notice that when projection-up is learned, its weight is small (around 0.08). This suggests that to remove it entirely, I should manually rescale the input by a factor of 0.1. With this change, the network learns again.</li> <li>Finally, I try removing the bias terms in the attention layer. The network no longer learns. At this point, I stop—the model is already simple enough.</li> </ul> <p>We thus arrive at a model that is as simple as possible while still being able to perform the task \(x_{t+1} = 2x_t - x_{t-1}\) reasonably well. Note that cherry-picking random seeds is not an issue here: the goal is not to demonstrate superior performance (where cherry-picking would be inappropriate), but to gain insight into how simple networks represent and compute.</p> <hr/> <h2 id="a-simple-model-can-numerically-approximate-the-mapping">A simple model can numerically approximate the mapping</h2> <p>In summary, we now have a “transformer” model with:</p> <ul> <li>1D input (and 1D embedding),</li> <li>context length 2 (using \(x_0\) and \(x_1\) to predict \(x_2\)),</li> <li>absolute positional embeddings,</li> <li>a single-head, 1D attention layer,</li> <li>no MLPs.</li> </ul> <p>This model can predict \(x_2\) from \(x_1\) and \(x_0\) reasonably well, achieving an MSE loss of about \(10^{-3}\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/training-480.webp 480w,/assets/img/blogs/physics-1/training-800.webp 800w,/assets/img/blogs/physics-1/training-1400.webp 1400w,/assets/img/blogs/physics-1/training-1920.webp 1920w,/assets/img/blogs/physics-1/training-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The sticky plateau of the loss curve at the beginning of training seems to correspond with the learning of positional embeddings (\(p_0\) and \(p_1\) repulse).</p> <hr/> <h2 id="the-model-does-not-exactly-represent-the-mapping">The model does not exactly represent the mapping</h2> <p>We now want to better understand <em>how</em> the model implements the task. A first hypothesis is the following: when positional embeddings dominate the input (i.e., \(x_0, x_1 \ll p_0, p_1\)), the attention matrix becomes approximately constant—roughly independent of \(x_0\) and \(x_1\). In that case, the network would represent \(f(x) = W_v(\alpha_1 x_1 + \alpha_0 x_0),\) where \(\alpha_1 + \alpha_0 = 1\) and \(\alpha_1, \alpha_0 &gt; 0\) (positivity guaranteed by softmax). However, \(2x_1 - x_0\) does not belong to this family, since the two terms have opposite signs. Therefore, the network must be doing something more subtle—in particular, the attention weights must depend on \(x_0\) and \(x_1\).</p> <p>To investigate this, we visualize how \(\alpha_1\) depends on \(x_0\) and \(x_1\). Empirically, \(\alpha_1\) depends primarily on \(x_1\), and the relationship appears roughly linear.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/alpha1-480.webp 480w,/assets/img/blogs/physics-1/alpha1-800.webp 800w,/assets/img/blogs/physics-1/alpha1-1400.webp 1400w,/assets/img/blogs/physics-1/alpha1-1920.webp 1920w,/assets/img/blogs/physics-1/alpha1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/alpha1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A linear regression gives \(\alpha_1 \approx -0.006 x_0 + 0.068 x_1 + 0.300 \quad (R^2 = 0.996).\) This indicates that although softmax is nonlinear, it is being used only locally, so its dependence is well approximated by a first-order Taylor expansion.</p> <p>Since attention is the only nonlinear component of the model—and we can approximate it linearly—we can now write down the full computation graph explicitly. We use <em>Mathematica</em> for symbolic computation:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/mathematica-480.webp 480w,/assets/img/blogs/physics-1/mathematica-800.webp 800w,/assets/img/blogs/physics-1/mathematica-1400.webp 1400w,/assets/img/blogs/physics-1/mathematica-1920.webp 1920w,/assets/img/blogs/physics-1/mathematica-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/mathematica.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Ignoring quadratic terms, we obtain \(-0.088 x_0 + 0.171 x_1 \approx 0.088(-x_0 + 2x_1).\)</p> <hr/> <h2 id="comment">Comment</h2> <ul> <li>Although a 1-layer attention model can approximate \(x_{t+1} = 2x_t - x_{t-1}\), it does so only approximately—by leveraging a local Taylor expansion of the softmax—rather than computing the relation exactly.</li> <li>The linear mapping \(x_{t+1} = 2x_t - x_{t-1}\) has a clear physical interpretation as uniform linear motion. This blog shows that even such simple motion cannot be represented <em>exactly</em> by a 1-layer transformer. While adding depth and MLPs may improve the approximation (e.g., by effectively expanding around more nonlinearities to better cancel higher-order terms), I suspect the computation remains approximate rather than exact. This suggests that in order to build real “world models” or “AI Physicists”, we are gonna need model beyond transformers.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1N66LgKQTze8vv-x4qOWu3i01Hp1pWNv8?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026physics-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics 1 -- Attention can't exactly simulate uniform linear motion}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 4 – Flat directions (in weight space) are high frequency modes (in function space)</title><link href="https://kindxiaoming.github.io/blog/2026/depth-4/" rel="alternate" type="text/html" title="Depth 4 – Flat directions (in weight space) are high frequency modes (in function space)"/><published>2026-02-03T00:00:00+00:00</published><updated>2026-02-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a recent blog post <a href="/blog/2026/depth-3/">depth-3</a>, we found that the <em>normalized</em> eigen-spectrum aligns across depths. Although this alignment may not hold in general, it naturally raises an important question: <strong>what is the benefit of depth, then?</strong></p> <p>To address this question, it is not sufficient to only examine eigenvalues—we also need to understand the corresponding <strong>eigenvectors</strong>. I can think of two ways to visualize eigenvectors:</p> <ol> <li>Load an eigenvector back into a neural network and visualize the resulting network graph.</li> <li>Study how perturbations along an eigenvector change the network’s output.</li> </ol> <p>In this article, we adopt the second approach. By choosing tasks with <strong>1D input and 1D output</strong>, we can visualize each eigenvector as a <strong>1D curve</strong> that represents how the learned function changes under a weight perturbation along that eigenvector.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider MLPs with hidden width 20, and compare <strong>2-layer</strong> and <strong>3-layer</strong> architectures. The task is 1D regression, either \(f(x) = x^2 \quad \text{(smooth)}\) or \(f(x) = \sin(5x) \quad \text{(oscillatory)}.\) We sample \(x \sim U[-1, 1]\). The networks are trained using the MSE loss with the Adam optimizer.</p> <p>After training, we obtain the model parameters \(\theta\). We then compute the Hessian \(H \equiv \frac{\partial^2 \ell}{\partial \theta^2},\) and its eigenvalues \(\lambda_i\) and eigenvectors \(v_i\) satisfying \(H v_i = \lambda_i v_i.\)</p> <p>The trained network represents a function \(f(x; \theta)\). To understand the <em>functional meaning</em> of an eigenvector \(v_i\), we perturb the parameters along this direction and obtain the perturbed function \(f(x; \theta + a v_i),\) where \(a\) is a small scalar. We visualize the induced functional change as \(\delta f_i \equiv \frac{1}{a}\bigl(f(x; \theta + a v_i) - f(x; \theta)\bigr).\)</p> <hr/> <h2 id="results-for-fx--x2">Results for \(f(x) = x^2\)</h2> <p><strong>Eigenvalues:</strong></p> <p>(We take the absoluate value. Note that there are negative eigenvalues at the tail, after the dip)</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Eigenvectors (perturbed functions):</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="results-for-fx--sin5x">Results for \(f(x) = \sin(5x)\)</h2> <p><strong>Eigenvalues:</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Eigenvectors (perturbed functions):</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <ul> <li> <p>In both cases, we find that the leading eigenvalues and eigenmodes are very similar for 2-layer and 3-layer networks. However, the 3-layer networks exhibit more high-frequency modes in the tail of the spectrum. Notably, the highest-frequency modes correspond to eigenvalues that are close to zero.</p> </li> <li> <p>These results help explain why continual learning is challenging: because these eigenfunctions are not localized, modifying the model to fit a single new data point can induce changes in the function globally, rather than locally.</p> </li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1pNSXF_DPza5LPeKEHOuRxoJuPaOKpRba?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 3 – Fun facts about loss hessian eigenvalues</title><link href="https://kindxiaoming.github.io/blog/2026/depth-3/" rel="alternate" type="text/html" title="Depth 3 – Fun facts about loss hessian eigenvalues"/><published>2026-02-02T00:00:00+00:00</published><updated>2026-02-02T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>The loss landscape is a first-principles object governing neural network learning. To characterize the loss landscape, <em>sharpness</em> is commonly used. Specifically, sharpness is defined in terms of the eigenvalues of \(H \equiv \frac{\partial^2 \ell}{\partial \theta^2},\) where \(\ell\) is the loss function, \(\theta\) denotes the model parameters, and \(H\) is the Hessian. Although computing sharpness is prohibitively expensive for large models, it is essential for understanding training dynamics.</p> <p>In this article, we explore several fun and perhaps surprising facts about sharpness in toy models. While we ultimately hope these observations can shed light on larger models, the goal here is simply to make careful observations and document simple empirical findings.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We use the ResNet toy model described in <a href="/blog/2026/depth-1/">depth-1</a>. The input (embedding) dimension is \(n_{\rm embd}\). Each block is a two-layer MLP with hidden dimension \(n_{\rm hidden}\). Residual connections are used, and \(n_{\rm block}\) blocks are stacked in depth to form the toy ResNet.</p> <p>We adopt a teacher–student regression setup: a teacher network generates the targets, and a student network of the same architecture (but with different initialization) is trained using the MSE loss.</p> <hr/> <h2 id="spectrum-is-gapped-or-eigenvalues-form-clusters">Spectrum is gapped, or eigenvalues form clusters</h2> <p>We set \(n_{\rm embd} = 10 \equiv N\), \(n_{\rm hidden} = 40 = 4N\), and \(n_{\rm block} = 2 \equiv D\). The total number of parameters is \(8N^2D = 1600\). Accordingly, the Hessian is a \(1600 \times 1600\) matrix with 1600 eigenvalues. We rank the eigenvalues from largest to smallest:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/gap-480.webp 480w,/assets/img/blogs/depth-3/gap-800.webp 800w,/assets/img/blogs/depth-3/gap-1400.webp 1400w,/assets/img/blogs/depth-3/gap-1920.webp 1920w,/assets/img/blogs/depth-3/gap-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/gap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the spectrum (eigenvalue distribution) is clearly gapped:</p> <ul> <li>The first 9 eigenvalues form a cluster.</li> <li>The next 9 eigenvalues form another cluster.</li> <li>The first 100 eigenvalues are significantly larger than zero, while the remaining eigenvalues are close to zero.</li> </ul> <p>These observations naturally raise several interesting questions and hypotheses:</p> <ul> <li>Why 9? We hypothesize that this number is \(N - 1\) (where \(N\) is the embedding dimension). We will show this is true.</li> <li>Why two large clusters? We initially hypothesize that this is because \(n_{\rm block} = 2\). We will show this is false—there are always two clusters, regardless of how deep the ResNet is.</li> <li>Why 100? We hypothesize that this number is \(N^2\). We will show this is true.</li> </ul> <hr/> <h2 id="experiment-1-vary-n-embedding-dimension">Experiment 1: Vary \(N\) (embedding dimension)</h2> <p>We vary \(N \in \{10, 15, 20\}\) while keeping \(n_{\rm hidden} = 4N\) and using two blocks.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/N-480.webp 480w,/assets/img/blogs/depth-3/N-800.webp 800w,/assets/img/blogs/depth-3/N-1400.webp 1400w,/assets/img/blogs/depth-3/N-1920.webp 1920w,/assets/img/blogs/depth-3/N-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/N.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The number of eigenvalues in each cluster is \(N - 1\).</li> <li>The number of clusters remains equal to 2.</li> <li>The number of significantly non-zero eigenvalues is \(N^2\).</li> </ul> <hr/> <h2 id="experiment-2-vary-d-depth">Experiment 2: Vary \(D\) (depth)</h2> <p>We vary \(D \in \{2, 4, 8\}\) while keeping \(N = 10\) and \(n_{\rm hidden} = 4N\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/depth-480.webp 480w,/assets/img/blogs/depth-3/depth-800.webp 800w,/assets/img/blogs/depth-3/depth-1400.webp 1400w,/assets/img/blogs/depth-3/depth-1920.webp 1920w,/assets/img/blogs/depth-3/depth-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/depth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The overall eigenvalue structure remains almost unchanged.</li> <li>The largest eigenvalue (i.e., sharpness) grows roughly linearly with depth.</li> </ul> <p>In fact, if we rescale the eigenvalues by the inverse of the depth, the normalized spectra approximately align across different depths.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/depth-alignment-480.webp 480w,/assets/img/blogs/depth-3/depth-alignment-800.webp 800w,/assets/img/blogs/depth-3/depth-alignment-1400.webp 1400w,/assets/img/blogs/depth-3/depth-alignment-1920.webp 1920w,/assets/img/blogs/depth-3/depth-alignment-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/depth-alignment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="question">Question</h2> <ul> <li>Although we made a few conclusions via induction, we did not provide sufficient explanation.</li> <li>The depth alignment result naturally raises the question: why do we need deep neural networks at all? Given the depth-alignment results, it is very likely that increasing depth mainly introduces more <em>small</em> eigenvalues, whereas increasing width introduces more <em>large</em> eigenvalues. When do large eigenvalues matter, and when do small eigenvalues matter? The eigen-spectrum provides a new perspective into understanding width-depth trade-off.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Y5BHija00MLjY6mdgCoJKWClnCgt4dAB?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 3 -- Fun facts about loss hessian eigenvalues}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Diffusion 2 – Visualizing flow matching, temporal dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-2/" rel="alternate" type="text/html" title="Diffusion 2 – Visualizing flow matching, temporal dynamics"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reproducing Tianhong and Kaiming’s JIT paper, <a href="https://arxiv.org/pdf/2511.13720">“Back to Basics: Let Denoising Generative Models Denoise”</a>. The first step is to implement the v-prediction baseline on the spiral dataset (Figure 2 in the paper). Reproducing the baseline should be straightforward, but I ended up getting stuck for quite a while. Eventually, I realized that how \(t\) is sampled during training is crucial.</p> <p>Specifically, \(t\) is sampled from a logit-normal distribution: \(t = \frac{1}{1 + e^{-X}}, \quad X \sim \mathcal{N}(\mu, \sigma^2).\) However, the main paper does not clearly specify how \((\mu, \sigma)\) are chosen. I therefore experimented with different values to understand their effects. Note that \(t = 0\) corresponds to pure noise, while \(t = 1\) corresponds to clean data.</p> <hr/> <h2 id="tweaking-mu-and-sigma">Tweaking \(\mu\) and \(\sigma\)</h2> <p>We first fix \(\sigma = 2\) to ensure sufficient coverage of the time range, and sweep over \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/mu-480.webp 480w,/assets/img/blogs/diffusion-2/mu-800.webp 800w,/assets/img/blogs/diffusion-2/mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that \(\mu = 2\) yields the best generation results. We then fix \(\mu = 2\) and sweep over \(\sigma\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma-480.webp 480w,/assets/img/blogs/diffusion-2/sigma-800.webp 800w,/assets/img/blogs/diffusion-2/sigma-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Somewhat surprisingly, \(\sigma = 0\) can still lead to decent generation quality. We therefore fix \(\sigma = 0\) and vary \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma_0_mu-480.webp 480w,/assets/img/blogs/diffusion-2/sigma_0_mu-800.webp 800w,/assets/img/blogs/diffusion-2/sigma_0_mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma_0_mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma_0_mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma_0_mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To better understand why \(\sigma = 0\) can still produce good samples, note that \(\sigma = 0\) means training only ever sees a single time \(t = \frac{1}{1 + e^{-\mu}}.\) Below, instead of training a neural network, we numerically estimate the true velocity field and visualize it directly.</p> <hr/> <h2 id="visualizing-true-velocities">Visualizing true velocities</h2> <p>For different values of \(t\), we visualize the velocity field using two color plots (a full vector-field visualization would be difficult to interpret due to potential multi-scale structure). Top: \(v_x\); bottom: \(v_y\). In each subplot, black dots indicate data samples, blue dots indicate generated samples at that time step, and red dashed lines show the zero-level sets.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/v-vis-480.webp 480w,/assets/img/blogs/diffusion-2/v-vis-800.webp 800w,/assets/img/blogs/diffusion-2/v-vis-1400.webp 1400w,/assets/img/blogs/diffusion-2/v-vis-1920.webp 1920w,/assets/img/blogs/diffusion-2/v-vis-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/v-vis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This makes it clear why training at a single \(t \sim 0.9\) can already yield high-quality samples: the zero-velocity contours align well with the spiral manifold. Under this velocity field, samples naturally converge toward the zero-velocity region.</p> <p>We also notice that that scale of \(v\) diverges as \(1/(1-t)\) when \(t\to 1\), which is another reason why predicting \(v\) is hard, besides JIT paper’s reasoning about manifolds.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1G7JUb5HDh6hHgYirsVF7LAfPyNr0CKa2?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Diffusion 2 -- Visualizing flow matching, temporal dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/" rel="alternate" type="text/html" title="Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”"/><published>2026-01-31T00:00:00+00:00</published><updated>2026-01-31T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-7</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading the paper <a href="https://arxiv.org/pdf/2501.00659">“Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph”</a>, which makes an interesting claim: even without explicit positional embeddings, multi-layer causal attention may implicitly encode positional information, despite the fact that a single-layer causal attention is permutation equivariant. Figure 1 from the paper is shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/fig1-480.webp 480w,/assets/img/blogs/sparse-attention-7/fig1-800.webp 800w,/assets/img/blogs/sparse-attention-7/fig1-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/fig1-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/fig1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The goal of this article is to measure how well multi-layer causal attention can extract previous tokens based purely on position. Given a sequence of random tokens (with context length \(C\)), the target is the \(k^{\rm th}\) token in the sequence (\(0 \leq k \leq C-1\)). For example, when \(C = 5\) and \(k = 1\), example sequences are</p> \[[5]\ [2]\ [3]\ [7]\ [9] \to [2]\] \[[1]\ [9]\ [2]\ [5]\ [8] \to [9]\] <p>We set the vocabulary size to \(V = 10\), context length to \(C = 10\), and the number of attention heads to \(n_{\rm head} = 1\). We are interested in varying the number of layers \(n_{\rm layer}\), the embedding dimension \(n_{\rm embd}\), whether residual connections are used, and whether MLPs are included.</p> <hr/> <h2 id="2l-is-qualitatively-different-from-1l">2L is qualitatively different from 1L</h2> <p>For each \(0 \leq k \leq C-1\), we train the transformer to convergence and record the final loss. We then plot how the final loss depends on \(k\). This roughly measures positional bias: a high loss means that retrieving information from that position is difficult—the loss cannot be driven down even when the model is explicitly trained on the retrieval task.</p> <p>In <a href="/blog/2026/sparse-attention-2/">sparse-attention-2</a>, we showed that a 1-layer causal attention model cannot retrieve any previous token (\(k \neq C-1\)), but can retrieve the current token (\(k = C-1\)). This corresponds to the left plot below (high loss for \(k \neq 9\), and almost zero loss for \(k = 9\)). The right plot shows the result for a 2-layer causal attention model, which exhibits the well-known <a href="https://arxiv.org/pdf/2307.03172">“lost in the middle” phenomenon</a>: early and late tokens are easily retrieved, while tokens in the middle are difficult to extract.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/1L-2L-480.webp 480w,/assets/img/blogs/sparse-attention-7/1L-2L-800.webp 800w,/assets/img/blogs/sparse-attention-7/1L-2L-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/1L-2L-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/1L-2L-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/1L-2L.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="embedding-dimension-and-number-of-layers-do-not-change-the-qualitative-behavior">Embedding dimension and number of layers do not change the qualitative behavior</h2> <p>We experiment with embedding dimensions \(\{2, 10\}\) and number of layers \(\{2, 3\}\). The qualitative trend remains unchanged: only the first and last tokens can be retrieved reliably, while tokens in the middle remain difficult. This manifests as a long plateau in the loss curve across middle positions.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/embd-layer-480.webp 480w,/assets/img/blogs/sparse-attention-7/embd-layer-800.webp 800w,/assets/img/blogs/sparse-attention-7/embd-layer-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/embd-layer-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/embd-layer-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/embd-layer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="residual-connections-and-mlps-destroy-the-long-plateau">Residual connections and MLPs destroy the long plateau</h2> <p>When residual connections and MLPs are added, the long plateau disappears and is replaced by a more spiky structure.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/residual-mlp-480.webp 480w,/assets/img/blogs/sparse-attention-7/residual-mlp-800.webp 800w,/assets/img/blogs/sparse-attention-7/residual-mlp-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/residual-mlp-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/residual-mlp-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/residual-mlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <p>This study suggests a simple mechanism underlying the “lost in the middle” phenomenon: even without explicit positional embeddings, stacking multiple causal-attention layers naturally induces an architectural bias toward losing information in the middle of the sequence.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/14OOPXMz4C40aIqOoWSxBzCH7hp3g3GZ9?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-7</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning "Loss in the middle" }</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-7/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 6 – In-context Associative recall</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-6/" rel="alternate" type="text/html" title="Sparse attention 6 – In-context Associative recall"/><published>2026-01-30T00:00:00+00:00</published><updated>2026-01-30T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-6</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-6/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>This article aims to reproduce the in-context associative recall task from the paper <a href="https://arxiv.org/pdf/2505.17863">“The emergence of sparse attention: impact of data distribution and benefits of repetition”</a>. The task is as follows: given a sequence,</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/task-480.webp 480w,/assets/img/blogs/sparse-attention-6/task-800.webp 800w,/assets/img/blogs/sparse-attention-6/task-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/task-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/task-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/task.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>the model must output <strong>Y</strong>, since the query <strong>Z</strong> matches the key in the <strong>Z–Y</strong> pair appearing earlier in the context.</p> <p>I am particularly interested in Figure 4 (left) of the paper (shown below), which demonstrates that learning is significantly delayed when the context length is large (with the context length being twice the number of pairs). In that regime, it takes about \(10^5\) steps for in-context learning (ICL) to emerge. My intuition is that learning can be substantially accelerated by choosing appropriate hyperparameters. For example, I would expect that a smaller embedding dimension could lead to much faster learning, whereas the paper uses an embedding dimension of 256.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/phase-transition-480.webp 480w,/assets/img/blogs/sparse-attention-6/phase-transition-800.webp 800w,/assets/img/blogs/sparse-attention-6/phase-transition-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/phase-transition-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/phase-transition-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/phase-transition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-embedding-dimension">Dependence on embedding dimension</h2> <p>We use a 2-layer, attention-only transformer model with embedding dimension \(N\) (swept), training data size \(10^4\), vocabulary size 64, and 32 key–value pairs. The model is trained using the Adam optimizer with learning rate \(10^{-2}\). While the original paper uses an embedding dimension of 256, we hypothesize that much smaller embedding dimensions can already succeed.</p> <p>Indeed, we find a Goldilocks zone in which ICL emerges in fewer than 1000 steps. Both too small and too large embedding dimensions lead to failure. An embedding dimension of 60 is particularly interesting: the model initially learns but then becomes unstable, likely due to an excessively large learning rate.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/n_embd-480.webp 480w,/assets/img/blogs/sparse-attention-6/n_embd-800.webp 800w,/assets/img/blogs/sparse-attention-6/n_embd-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/n_embd-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/n_embd-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/n_embd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="how-to-make-large-embedding-dimensions-work">How to make large embedding dimensions work</h2> <p>When one is forced to use a large embedding dimension for some reason,</p> <ul> <li>the data size should be increased to mitigate overfitting;</li> <li>the learning rate should be reduced, in line with the common wisdom that wider networks require smaller learning rates. This is why learning is slow for large embedding dimensions.</li> </ul> <p>We sweep over data sizes \(\{10^4, 10^5\}\) and learning rates \(\{10^{-2}, 10^{-3}\}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-6/80-480.webp 480w,/assets/img/blogs/sparse-attention-6/80-800.webp 800w,/assets/img/blogs/sparse-attention-6/80-1400.webp 1400w,/assets/img/blogs/sparse-attention-6/80-1920.webp 1920w,/assets/img/blogs/sparse-attention-6/80-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-6/80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>More systematically, it would be interesting to draw phase diagrams capturing how learning curves depend on various hyper-parameters.</p> <hr/> <h2 id="comment-link-to-learning-rate-warmup">Comment: Link to learning rate warmup</h2> <p>For patterns with very strong signals that can be learned very early (induction is arguably a strong pattern due to its commonality), keeping a large learning rate will eventually make them unstable. Therefore, in the early stage, a relatively small learning rate is needed to prevent these strong-signal components from becoming unstable. In the later stage, however, we want to accelerate the learning of weaker signals, which calls for a larger learning rate. This is simply a hypothesis which requires further justification.</p> <p>Induction heads are likely such strong-signal components. This also connects nicely to the earlier discussions on <a href="/blog/2026/induction-head-lr-schedule/">learning-rate schedules</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/10wYqmEVn4CGJfkPdZHdGfb7Z3pdHR7Iu?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-6</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 6 -- In-context Associative recall}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-6/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">MLP 2 – Effective linearity, Generalized SiLU</title><link href="https://kindxiaoming.github.io/blog/2026/mlp-2/" rel="alternate" type="text/html" title="MLP 2 – Effective linearity, Generalized SiLU"/><published>2026-01-29T00:00:00+00:00</published><updated>2026-01-29T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/mlp-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/mlp-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>MLPs are powerful because they include nonlinear activation functions. But how effectively do they actually use this nonlinearity? If we perform a linear regression between the MLP inputs and outputs and obtain \(R^2 \approx 1\), then the MLP is not making effective use of its nonlinearity.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We use the same setup as in <a href="/blog/2026/depth-1/">depth-1</a>. Both the teacher and the student networks are residual networks with MLPs. The teacher network uses the SiLU activation function, while we vary the activation function of the student network. We measure the \(R^2\) values for the MLPs in all layers and track how these quantities evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/r2-480.webp 480w,/assets/img/blogs/mlp-2/r2-800.webp 800w,/assets/img/blogs/mlp-2/r2-1400.webp 1400w,/assets/img/blogs/mlp-2/r2-1920.webp 1920w,/assets/img/blogs/mlp-2/r2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/r2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that lower \(R^2\) values tend to correspond to lower losses, which aligns with the intuition that a lower \(R^2\) indicates stronger nonlinearity. One could attempt to minimize \(R^2\) further—for example, by using high-frequency sine functions—but this would likely harm trainability. There appears to be a trade-off between stability and nonlinearity.</p> <hr/> <h2 id="generalized-silu">Generalized SiLU</h2> <p>Can we define a family of activation functions for which SiLU is a special case, while also allowing for more oscillatory behavior? We define a generalized SiLU function as follows:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/gen_silu_shape-480.webp 480w,/assets/img/blogs/mlp-2/gen_silu_shape-800.webp 800w,/assets/img/blogs/mlp-2/gen_silu_shape-1400.webp 1400w,/assets/img/blogs/mlp-2/gen_silu_shape-1920.webp 1920w,/assets/img/blogs/mlp-2/gen_silu_shape-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/gen_silu_shape.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The definition in Python code is shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/code-480.webp 480w,/assets/img/blogs/mlp-2/code-800.webp 800w,/assets/img/blogs/mlp-2/code-1400.webp 1400w,/assets/img/blogs/mlp-2/code-1920.webp 1920w,/assets/img/blogs/mlp-2/code-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SiLU corresponds to generalized SiLU with parameters (0, 0). We find that generalized SiLU with parameters (0, 1) or (1, 1) can achieve extremely low loss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-2/gen_silu-480.webp 480w,/assets/img/blogs/mlp-2/gen_silu-800.webp 800w,/assets/img/blogs/mlp-2/gen_silu-1400.webp 1400w,/assets/img/blogs/mlp-2/gen_silu-1920.webp 1920w,/assets/img/blogs/mlp-2/gen_silu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-2/gen_silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1MH6l2vzH0BOcGV0g9CrKm-sc4IepQAet?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026mlp-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MLP 2 -- Effective linearity, Generalized SiLU}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/mlp-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">MLP 1 – Gating is good for polynomials</title><link href="https://kindxiaoming.github.io/blog/2026/mlp-1/" rel="alternate" type="text/html" title="MLP 1 – Gating is good for polynomials"/><published>2026-01-28T00:00:00+00:00</published><updated>2026-01-28T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/mlp-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/mlp-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Gated MLPs are widely used in modern language models. Given an input \(x\), the hidden representation of a gated MLP is \(\sigma(W_g x) \odot W_v x .\) It is often argued that gating promotes sparsity, multiplicative structure, conditional computation, and more stable training. This is far too large a topic to be fully explored in a single blog post. In this article, we narrow the scope to polynomial fitting. If learning a Taylor expansion is important for neural networks, then learning polynomials is a necessary prerequisite.</p> <hr/> <h2 id="polynomials">Polynomials</h2> <p>For both ReLU and SiLU activations, gated MLPs (green) consistently outperform non-gated MLPs (blue) when fitting \(x^n\) for all \(n \ge 2\). We also compare against non-gated MLP but with the activation function squared (orange), and find that squared activation functions can achieve strong performance—sometimes even better than gated networks.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/mlp-1/poly-480.webp 480w,/assets/img/blogs/mlp-1/poly-800.webp 800w,/assets/img/blogs/mlp-1/poly-1400.webp 1400w,/assets/img/blogs/mlp-1/poly-1920.webp 1920w,/assets/img/blogs/mlp-1/poly-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/mlp-1/poly.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="a-rough-equivalence">A rough equivalence</h2> <p>We observe that as \(\epsilon \to 0^+\), a Taylor expansion gives \(\sigma((w+\epsilon)x) - \sigma((w-\epsilon)x) \approx 2\epsilon x \sigma'(wx) .\) This suggests a rough “equivalence” between \(\sigma(x)\) and \(x \sigma'(x)\), where \(x \sigma'(x)\) can be interpreted as a gate. For gated ReLU networks, since \(\sigma'(x) = {\rm ReLU}\), this implies \(\sigma(x) = {\rm ReLU}^2\). In other words, a gated ReLU network is roughly equivalent to a non-gated ReLU2 network.</p> <p>So far, this is not meant to be mathematically rigorous—just a hand-wavy but potentially useful intuition. Once we have a useful gate, it can be converted to a useful activation function for free (except for doing integration).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Dqn6dkxW0hP04s30F3zbT3wYZivsXv65?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026mlp-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MLP 1 -- Gating is good for polynomials}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/mlp-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>