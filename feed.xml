<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-08T02:52:33+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Fine-tuning with sparse updates? A toy teacher-student Setup</title><link href="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/" rel="alternate" type="text/html" title="Fine-tuning with sparse updates? A toy teacher-student Setup"/><published>2026-01-07T00:00:00+00:00</published><updated>2026-01-07T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>When we fine-tune a model, do we really need to update <strong>all</strong> layers?<br/> For language models, my mental picture is that the model can be divided along depth into three parts:</p> <ul> <li>early layers: mapping <strong>tokens → latent space</strong></li> <li>middle layers: performing <strong>reasoning in latent space</strong></li> <li>late layers: mapping <strong>latent space → tokens</strong></li> </ul> <p>As long as a task is still expressed in natural language, the mapping between language space and latent space should be largely fixed. Therefore, during fine-tuning, perhaps it is sufficient to fine-tune only the <strong>middle layers</strong>. How can we test this hypothesis?</p> <p>There are three possible approaches:</p> <ul> <li> <p><strong>Method 1</strong>: Fine-tune only the middle layers and check whether it works.<br/> However, to judge effectiveness, we need baselines (e.g., only fine-tuning early layers, or only fine-tuning late layers). The number of baselines grows exponentially, since each layer can either be frozen or fine-tuned.</p> </li> <li> <p><strong>Method 2</strong>: Train normally and examine how the magnitude of parameter updates varies across layers.<br/> The problem is that update magnitudes are highly influenced by the optimizer. For example, even if some layers have very small gradients (and arguably do not need to change), adaptive optimizers may still update them.</p> </li> <li> <p><strong>Method 3</strong>: Train normally, but impose sparsity on the updates (by adding L1 regularization), and then examine how update magnitudes vary across layers.<br/> This approach may avoid the issue in Method 2.</p> </li> </ul> <p>In this article, we use a <strong>teacher–student model</strong> to explore Methods 2 and 3. We find that Method 2 indeed suffers from the suspected issue, while Method 3 can effectively resolve it.</p> <hr/> <h2 id="teacherstudent-setup">Teacher–student setup</h2> <p>We adopt a teacher–student setup: the <strong>Teacher Network</strong> and the <strong>Student Network</strong> are MLPs with identical architectures.<br/> The teacher network is randomly initialized and generates input–output pairs. The student network is trained in a supervised manner to minimize the MSE loss of output predictions.</p> <p>Some layers of the student network are copied from the teacher network, while the remaining layers are randomly initialized. Motivated by the discussion above, we consider a <strong>three-layer MLP</strong>. At initialization:</p> <ul> <li>the first and third layers of the student network are identical to those of the teacher network,</li> <li>the second layer is randomly initialized (and thus different from the teacher network).</li> </ul> <p>During training, we additionally compute the <strong>L1 distance</strong> between the student network at training time and its initialization, and use this as a regularization term to encourage sparse updates. The strength of this regularization is denoted by \(\lambda\).</p> <p>The student network can be interpreted as a <strong>pretrained model</strong>, while the teacher network serves as a <strong>fine-tuned data generator</strong>. We ask whether the student network can “realize” that it actually does <strong>not</strong> need to update the first and third layers.</p> <p>We define two sets of observables to characterize the training dynamics:</p> <ul> <li>the distance between the student network during training and its initialization,</li> <li>the distance between the student network during training and the teacher network.</li> </ul> <p>Specifically, we compute the L1 distances for \(W_1, W_2, W_3, b_1, b_2, b_3\).</p> <hr/> <h3 id="normal-training-lambda--0">Normal training (\(\lambda = 0\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the update magnitude of \(W_2\) is actually <strong>smaller</strong> than that of \(W_1\) and \(W_3\). This shows that Method 2 is unreliable.</p> <p><strong>Additional observation.</strong> Each spike in the loss curve corresponds to a “step” in the weights. This phenomenon may be related to the <a href="https://arxiv.org/abs/2206.04817">slingshot mechanism</a>.</p> <hr/> <h3 id="sparse-updates-lambda--0001">Sparse updates (\(\lambda = 0.001\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0d001-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0d001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that only \(W_2\) has a significantly non-zero update, while the updates of all other weights eventually approach zero (although they may move substantially at the beginning, which is likely an artifact of Adam). This indicates that <strong>sparse updates can reveal the intrinsic structure of the data</strong>—namely, that only the middle layer needs to be fine-tuned.</p> <p><strong>Additional observation.</strong> The distance between the trained student network and the teacher network for \(W_2\) does not converge to zero (and is in fact quite large). This may be because \(W_2\), acting on sparse inputs, is effectively low-rank, so the optimal \(W_2\) is not unique.</p> <hr/> <h2 id="questions--ideas">Questions / Ideas</h2> <ul> <li> <p>Fine-tuning with sparse updates can serve as an <strong>interpretability tool</strong>—revealing layer-wise similarity between two datasets.<br/> Given a model pretrained on dataset A and fine-tuned on dataset B with sparse updates, it would be interesting to see which layers undergo large updates and which remain nearly unchanged.</p> </li> <li> <p>L1 regularization could be replaced by other metrics (e.g., the <strong>nuclear norm</strong> to encourage low-rank structure).</p> </li> <li> <p>Can this method be scaled up, and if so, how?</p> </li> <li> <p>Could this approach help with <strong>continual learning</strong>?<br/> Sparse updates may mitigate catastrophic forgetting.</p> </li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NsviyXmeuCppxr1wuL53Q9PzK_5BQWnB?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026fine-tuning-sparsity</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Fine-tuning with sparse updates? A toy teacher-student Setup}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/fine-tuning-sparsity/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Multi-Head Cross Entropy Loss</title><link href="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/" rel="alternate" type="text/html" title="Multi-Head Cross Entropy Loss"/><published>2026-01-06T00:00:00+00:00</published><updated>2026-01-06T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In LLMs, next-token prediction is performed by passing the final-layer hidden representation through the LM head to produce logits, thereby projecting into the token space. The definition of cross-entropy implies that only the correct token receives a low loss, while <em>any</em> other token incurs a high loss. For example, if the correct token is “red,” predicting either “green” or “rabbit” results in a large loss. However, it is clear that <em>green</em> is much closer to <em>red</em> than <em>rabbit</em> is, since both are adjectives and both denote colors.</p> <p>This motivates the following question: when designing the loss, should we take <em>similarity between tokens</em> into account? If token A and token B are similar in some respects, then if the model mistakenly predicts token B instead of token A, should it really be penalized as heavily as predicting a completely unrelated token C?</p> <p>This idea is still quite abstract, and there are likely many concrete ways to implement it. In this post, we explore one possibility: <strong>multi-head cross-entropy loss</strong>. Inspired by multi-head attention—where different heads attend to different semantic aspects—multi-head cross-entropy aims to capture token similarity from multiple semantic perspectives.</p> <hr/> <h2 id="definition">Definition</h2> <p><strong>Standard Cross-Entropy</strong></p> <p>Let the input representation to the LM head be \(x \in \mathbb{R}^D\) and the LM head weights be \(W \in \mathbb{R}^{V \times D}.\) The LM head output (i.e., the logits) is \(y = W x \in \mathbb{R}^V,\) where \(V\) is the vocabulary size. Standard cross-entropy computes the loss between $$y$ and the ground-truth label.</p> <p><strong>Multi-Head Cross-Entropy</strong></p> <p>We split \(x\) into \(H\) heads: \(x = [x_1; x_2; \cdots; x_H], x_i \in \mathbb{R}^{D/H}\). Similarly, we split \(W\) into \(H\) parts: \(W_i \equiv W[:,i\frac{D}{H}:(i+1)\frac{D}{H}] \in \mathbb{R}^{V\times D/H}.\) The logits for the \(i\)-th head are \(y_i = W_i x_i \in \mathbb{R}^V.\) After applying Softmax, we define a probability distribution \(p_{i,j} = \frac{\exp(y_{i,j})}{\sum_{j=1}^{V} \exp(y_{i,j})},\) which represents the probability of token \(j\) under the \(i\)-th semantic head.</p> <p>How should we aggregate different heads? Here we simply sum the probabilities (without a rigorous justification, and many other choices are possible): \(p_j = \sum_{i=1}^{H} p_{i,j}.\) The corresponding aggregated logit is \(y_j = \log(p_j).\)</p> <hr/> <h2 id="example-toy-language">Example: Toy Language</h2> <p><strong>Dataset</strong></p> <p>We assume each token has two features:</p> <ul> <li><strong>Part of speech</strong>: noun or verb</li> <li><strong>Topic</strong>: mathematics or sports</li> </ul> <p><strong>Grammar:</strong> nouns and verbs alternate: \(\text{noun} \rightarrow \text{verb} \rightarrow \text{noun} \rightarrow \text{verb} \rightarrow \cdots\)</p> <p><strong>Topic:</strong> within a sentence, the topic is consistent—either mathematics or sports.</p> <p>Thus, we have four classes of tokens:</p> <ul> <li><strong>A</strong>: math nouns</li> <li><strong>B</strong>: math verbs</li> <li><strong>C</strong>: sports nouns</li> <li><strong>D</strong>: sports verbs</li> </ul> <p>Each class contains 10 tokens (randomly chosen). Valid sentences look like:</p> \[[A8] \rightarrow [B2] \rightarrow [A5] \rightarrow [B6] \rightarrow [A1] \rightarrow [B7] \rightarrow \cdots\] \[[C2] \rightarrow [D1] \rightarrow [C6] \rightarrow [D2] \rightarrow [C10] \rightarrow [D3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>We consider a <strong>bi-gram model</strong>, which predicts the next token based on the previous token. The model uses an MLP, with weight tying between the embedding layer and the LM head.</p> <hr/> <h2 id="results">Results</h2> <p>We perform PCA on the embedding space and project each token embedding onto PC1/PC2. We find that as the number of heads increases:</p> <ol> <li>Clustering improves.</li> <li>The explained variance increases (i.e., the representation becomes more low-dimensional).</li> </ol> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/multi-head-cross-entropy/toy-language-480.webp 480w,/assets/img/blogs/multi-head-cross-entropy/toy-language-800.webp 800w,/assets/img/blogs/multi-head-cross-entropy/toy-language-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/multi-head-cross-entropy/toy-language.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="grokking">Grokking</h2> <p>I also randomly tried a grokking (modular addition) setup and found that multi-head cross-entropy does not significantly accelerate grokking. This was not particularly surprising—I did not have a strong prior for why it should help. This result is reasonable because, in modular addition, tokens essentially have only a single semantic dimension (numerical value), so there is no meaningful notion of multiple semantics for different heads to exploit.</p> <hr/> <h2 id="questions">Questions</h2> <ul> <li>Can we design a more suitable toy dataset that enables more mechanistic interpretability?</li> <li>Does it make sense to apply multi-head cross-entropy loss to large language models?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1QviExbkM6yCz_-T7UfSmCVL89ZGy7Gj_?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026multi-head-cross-entropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Multi-Head Cross Entropy Loss}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/multi-head-cross-entropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">What’s the difference – (physics of) AI, physics, math and interpretability</title><link href="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/" rel="alternate" type="text/html" title="What’s the difference – (physics of) AI, physics, math and interpretability"/><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/"><![CDATA[<p>In a previous <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>, I discussed how to conduct <em>Physics of AI</em> research. Some friends later asked: <strong>Can AI and physics really be fully analogous?</strong> And <strong>what is the difference between Physics of AI and interpretability?</strong></p> <p>This article will discuss two points:</p> <ol> <li><strong>AI and physics are not fully analogous</strong>, but that does not prevent us from borrowing methodologies from physics. In fact, <em>Physics of AI</em> is technically a <strong>simpler game than physics</strong>. Its main obstacles lie in <strong>publication culture</strong> (as discussed in a previous <a href="/blog/2026/physics-of-ai/">blog post</a>), not in the intrinsic difficulty of the subject.</li> <li><strong>(As I define) Physics of AI is not the same as (what people usually define) interpretability</strong>. The key reason is that I see some genuinely new research perspectives that deserve a new name. What that name is does not really matter. If, in the future, the community expands the definition of interpretability, I would also be happy to simply call it interpretability.</li> </ol> <hr/> <p>First, what’s the difference between physics and (physics of) AI?</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-480.webp 480w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-800.webp 800w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="physics">Physics</h2> <p>If we count from the time of Newton, physics has taken about <strong>400 years</strong> to develop to its current state. Compared with the development of AI, this is extremely slow. This slowness comes from both <strong>practical constraints</strong> and <strong>philosophical constraints</strong>.</p> <p><strong>Practical constraints — physics research has strong directionality</strong></p> <ul> <li><strong>Experiment-driven</strong>: theories emerge from experimental observations.</li> <li><strong>“Human-centered” scales</strong>: starting from scales directly accessible to human perception, then extending in both directions—toward the microscopic (atoms, quarks) and the macroscopic (celestial bodies, the universe).</li> </ul> <p>Both directions are constrained by experimental and observational bottlenecks. It takes a long time to build microscopes and telescopes in the physical world.</p> <p><strong>Philosophical constraints</strong></p> <p>We do not actually know how the “creator” (physical laws) truly runs the universe. We can only infer laws from phenomena.<br/> <em>All models are wrong, but some are useful.</em></p> <p>Even the most committed reductionists are troubled by two questions:</p> <ul> <li><strong>Minimum and maximum scales</strong>: Are fundamental particles truly fundamental, e.g., can quarks or electrons be further divided? What lies beyond the universe?</li> <li><strong>Cross-scale emergence</strong>: How do phenomena at different levels “emerge” from one another?</li> </ul> <hr/> <h2 id="physics-of-ai">(Physics of) AI</h2> <p>For AI, <strong>none of the above constraints really exist</strong>.</p> <p><strong>No practical constraints</strong></p> <p>Apart from computational limits that prevent us from reaching arbitrarily large scales, we have <strong>no observational limitations</strong>. In principle, we can study the evolution of <strong>all weights and all neurons</strong>.</p> <p><strong>No philosophical constraints</strong></p> <p>We fully understand the “fundamental particles” of AI (neurons, weights, gradient descent), and we know the “boundary of the universe” (the entire neural network). <strong>We are the creator.</strong></p> <p>In principle, we can observe phenomena at any level at any time. There is no inherent directionality and no “human-centered” scale. AI systems are also <strong>closed systems</strong>—we know exactly how they evolve because we train them ourselves. Therefore, in principle, it must always be possible to explain large-scale phenomena using small-scale mechanisms, even if such explanations are not always useful.</p> <p>Physics is different: the continual discovery of “new physics” shows that its boundaries are still being broken.</p> <hr/> <h2 id="mathematics">Mathematics</h2> <p>At this point, it is tempting to equate the physics of AI with mathematics, since both have clearly defined “fundamental particles”—called <strong>axioms</strong> in mathematics. But mathematics is clearly more difficult, for several reasons:</p> <ul> <li><strong>Mathematics is also largely directional</strong>: starting from axioms and deriving results is a process from “microscopic” to “macroscopic.” In contrast, AI phenomenology can be studied simultaneously at multiple levels.</li> <li><strong>Symbolic spaces make the definition of scales/levels and observations difficult</strong>: although backward reasoning (induction, abduction) exists in mathematics, it is often hard to define what “intermediate reasoning” even means, because symbolic spaces can be infinite. Neural networks (current AI systems), by contrast, have bounded topologies — no smaller than a neuron and no larger than the entire network.</li> <li><strong>Cultural emphasis on rigor</strong>: mathematical rigor often comes at the cost of faithfulness to reality. AI research, in contrast, emphasizes practicality and is far less obsessed with rigor. Physics lies somewhere in between mathematics and AI in terms of rigor.</li> </ul> <hr/> <h2 id="does-the-physics-of-ai-have-no-difficulties-then">Does the Physics of AI Have No Difficulties, Then?</h2> <p>Of course not.</p> <p>Philosophically, the physics of AI does not suffer from reductionism problems (we know the minimum and maximum scales, and cross-scale explanations are possible in principle). However, it may still face <strong>technical challenges</strong>. Even so, I remain optimistic. Two major technical questions are:</p> <ul> <li><strong>How do we define levels and corresponding observations?</strong> Neurons, weights, and representations are clear, but how should we define circuits or modules? Once levels are defined, what observables should we introduce, and what phenomena should we observe? I gave partial answers in this <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>.</li> <li><strong>How do we characterize the connections (emergence) between levels?</strong> In principle, because AI systems are closed, lower-level phenomena should always be able to explain higher-level ones. In practice, however, finding explanations that are <em>simple</em> and <em>useful</em> is still a non-trivial problem. To be honest, I still have no clue how to answer this question. But more hands-on experiments will give the answer.</li> </ul> <hr/> <h2 id="response-to-critiques-from-the-scaling-camp">Response to Critiques from the Scaling Camp</h2> <p>The issue of emergence is the main line of attack from the <strong>scaling camp</strong> against the <strong>research camp</strong>:<br/> <em>How can phenomena observed in small models transfer to large models?</em></p> <p>Below is my response/attitude/belief:</p> <p><strong>Philosophical level</strong></p> <ul> <li>In natural science, analogies of emergence do not straightforwardly apply here (as discussed above). The ineffectiveness of reductionism in explaining emergence in nature (e.g., <a href="https://arxiv.org/abs/2503.01800">Hilbert’s sixth problem</a> is a hard problem) does not imply the same ineffectiveness in AI. Moreover, I personally do not like the word “emergence” since it seems to imply something mysterious (which might be appropriate for natural science, but not AI).</li> <li>Believe in <strong>shared explanations</strong>. Phenomenon A in small models and phenomenon B in large models may originate from the same cause. The absence of A in large models does not invalidate the value of studying it.</li> </ul> <p><strong>Methodological level</strong></p> <ul> <li><strong>Be pragmatic</strong>. There are many concrete things we can do that have not yet been done. Only by doing them can we know whether they are useful.</li> <li>Be specific. Cross-scale phenomena usually fall into three categories: <ul> <li><strong>Expansion</strong>: phenomenon A in small models becomes more pronounced in large models.</li> <li><strong>Shrinkage</strong>: phenomenon A in small models becomes less visible in large models.</li> <li><strong>Transformation</strong>: phenomenon A in small models turns into phenomenon B in large models.</li> </ul> <p>The scaling critique focuses on <em>shrinkage</em>. We are betting on <em>expansion</em> and <em>transformation</em>. Even if everything turns out to be shrinkage, our efforts are still not meaningless—at that point, I would more firmly side with the scaling camp.</p> </li> <li>We must acknowledge that academia cannot afford to train the largest models. We should call on large-model companies to open-source models—or at least open-source <em>phenomena</em>. Of course, academia must first identify interesting observables in toy and small models, so that industry knows <em>what</em> to observe.</li> </ul> <hr/> <h2 id="interpretability">Interpretability</h2> <p>In a broad sense, interpretability includes everything—one could even say that <em>physics itself is about interpreting the universe</em>. In that sense, the physics of AI certainly belongs to interpretability.</p> <p>However, what people usually mean by interpretability refers narrowly to <strong>interpretability research in AI</strong>, and their understanding is constrained by past work:</p> <ul> <li>Some believe interpretability is just storytelling—pleasant but useless.</li> <li>Others believe interpretability is mathematics—rigorous but useless.</li> <li>The characterizations are often too coarse, making it unclear whether we are studying causality or correlation. This is precisely what <em>mechanistic interpretability</em> seeks to break away from.</li> <li>There is a lack of methodology: <em>at what level does an explanation count as complete?</em></li> </ul> <p>Physics of AI differs from interpretability in all aspects:</p> <ul> <li>Starts with <strong>phenomenology</strong>, emphasizing faithful recording of phenomena and reducing the “storytelling” aspect (see previous <a href="/blog/2025/physics-of-ai/">blog post</a>).</li> <li>Is <strong>not mathematics</strong> (as discussed above). But we can adopt mathematical tools when they are useful.</li> <li>Characterizes phenomena at <strong>multiple levels</strong>, although I personally perfer startting from smallest “toy” models. You might have noticed that my recent technical blogs all study very simple models. The toy models already demonstrate very rich phenomena and I believe that the phenomena in toy models can transform (although maybe not directly transfer) to phenomena in larger models.</li> <li>Gradually builds a <strong>methodology</strong> to describe the connections between phenomena across levels.</li> </ul> <p>Therefore, we deserve a <strong>new name</strong> for what we are doing. <em>Physics of AI</em> is the name I chose.<br/> Again, the name itself is not important. What matters is that it signals <strong>new territory and new treasures</strong>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{What's the difference -- (physics of) AI, physics, math and interpretability}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/ai-physics-interpretability/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[In a previous blog post, I discussed how to conduct Physics of AI research. Some friends later asked: Can AI and physics really be fully analogous? And what is the difference between Physics of AI and interpretability?]]></summary></entry><entry><title type="html">Representation anisotropy from nonlinear functions</title><link href="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/" rel="alternate" type="text/html" title="Representation anisotropy from nonlinear functions"/><published>2026-01-04T00:00:00+00:00</published><updated>2026-01-04T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/activation-anisotropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Neural representations in deep neural networks are usually anisotrophic. This article aims to understand how nonlinear activation functions lead to representation anisotropy.</p> <hr/> <h2 id="toy-model">Toy model</h2> <p>We consider an LNL (linear-nonlinear) model, which is basically a two-layer MLP excluding the down projection layer: \(h_{\rm pre} = Wx, h_{\rm post} = \sigma(h_{\rm pre})\) where \(\sigma(\cdot)\) is the activation function. We set \(x\in\mathbb{R}^d\) to be isotropic – \(N\) samples are drawn from standard Gaussian distribution. \(W\in\mathbb{R}^{d\times D}\) is randomly initialized. We set \(d=100, D=400, N=10000\). We won’t train the model, and will only be interested in characterizing the anisotropy of \(h_{\rm post}\). We can stack \(h_{\rm post}\) of all \(N\) samples into a matrix \(H\in\mathbb{R}^{N\times D}\).</p> <p>To measure anisotropy, we apply singular value decomposition (SVD) to \(H\) to obtain singular values. The singular value distribution characterizes anisotropy of representations. We normalize singular values so that they sum up to 1.</p> <hr/> <h2 id="observation-1-relu-activation-leads-to-massive-sigma_1-while-linear-activation-does-not">Observation 1: ReLU activation leads to massive \(\sigma_1\), while linear activation does not.</h2> <p>ReLU function make \(\sigma_1\) stand out, but linear function does not:</p> <p>ReLU and linear:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/relu-linear-480.webp 480w,/assets/img/blogs/activation-anisotropy/relu-linear-800.webp 800w,/assets/img/blogs/activation-anisotropy/relu-linear-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/relu-linear.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SiLU is qualitatively similar to ReLU:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 45%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/silu-480.webp 480w,/assets/img/blogs/activation-anisotropy/silu-800.webp 800w,/assets/img/blogs/activation-anisotropy/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This begs the question: why? Before answering why, we first do an interpolation experiment – leaky relu interpolates between ReLU (\(p=0\)) and linear (\(p=1\)). We will use the ratio \(\sigma_1/\sigma_2\) to measure how “standing out’’ \(\sigma_1\) is.</p> <hr/> <h2 id="observation-2-first-order-phase-transition-of-leaky-relu">Observation 2: First order phase transition of Leaky ReLU</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/leaky_relu-480.webp 480w,/assets/img/blogs/activation-anisotropy/leaky_relu-800.webp 800w,/assets/img/blogs/activation-anisotropy/leaky_relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/leaky_relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Although I expect that \(\sigma_1/\sigma_2\) would smoothly interpolate between two extremes, there is a first order phase transition around \(p_c\approx 0.8\). For \(p&gt;p_c\), the ratio remains close to 1. For \(p&lt;p_c\), the ratio grows exponentially (the y axis is log-scale) as \(p\) decreases. I don’t understand why.</p> <hr/> <h2 id="explanation-relu-polarizes-activations">Explanation: ReLU polarizes activations</h2> <p>The intuition: because ReLU maps negative values to zero, this creates a notion of polarity. Indeed, we find the first eigenvector to be \([1, 1, 1, \cdots, 1]\).</p> <p>To better see this, we even drop the linear matrix and only keep the nonlinearity. For isotropic pre-activations, the spectrum for the post-activations look like this:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/isotropic-preact-480.webp 480w,/assets/img/blogs/activation-anisotropy/isotropic-preact-800.webp 800w,/assets/img/blogs/activation-anisotropy/isotropic-preact-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/isotropic-preact.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where \(\sigma_1\) stands out, while all rest singular values are small and almost the same.</p> <p>In hindsight, this explantion is almost trivial, but the consequence (large \(\sigma_1\)) is something I don’t think I had appreciated enough.</p> <hr/> <h2 id="observation-3-tanh-doesnt-make-sigma_1-stand-out">Observation 3: Tanh doesn’t make \(\sigma_1\) stand out</h2> <p>Based on the above explantion, non-polarized activation functions (like Tanh) don’t make \(\sigma_1\) stand out, which is indeed the case:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/tanh-480.webp 480w,/assets/img/blogs/activation-anisotropy/tanh-800.webp 800w,/assets/img/blogs/activation-anisotropy/tanh-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/tanh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="question">Question</h2> <ul> <li>Is this a feature or a bug? Right now I tend to think this is a bug. Anisotropy could be the reason for training inefficiency.</li> <li>If this is a bug, how can we avoid this? Can we simply try to find better activation functions, or do we need extra processing?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1k-vibPZIgB9e5--i87_zG5IfLDH_elLQ?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026activation-anisotropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Representation anisotropy from nonlinear functions}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/activation-anisotropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Training dynamics of A Single ReLU Neuron</title><link href="https://kindxiaoming.github.io/blog/2026/single-relu-neuron-copy/" rel="alternate" type="text/html" title="Training dynamics of A Single ReLU Neuron"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/single-relu-neuron%20copy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/single-relu-neuron-copy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a previous <a href="/blog/2026/feature-learning-1/">blog post</a>, we studied feature learning in shallow, wide MLPs. In this article, we consider an even simpler setting: an MLP with only <strong>one hidden layer</strong>, <strong>a single ReLU neuron</strong>, and a <strong>self-generated target function</strong> (a teacher network).</p> <p>In this setting, we know that there exists a set of weights for which the loss is exactly zero. But will gradient-based optimization run into difficulties? For example, if the neuron is initialized to be overly <strong>active</strong> (positive pre-activation for all inputs), or overly <strong>inactive</strong> (negative pre-activation for all inputs), will optimization fail? Even if the initialization is reasonable, can the training dynamics drive the neuron into a bad state (too active or too inactive)?</p> <p>For convenience, we define three states of a neuron:</p> <ul> <li><strong>Hyperactive</strong>: activated for all inputs (pre-activation always positive)</li> <li><strong>Inactive / Dead</strong>: never activated (pre-activation always negative)</li> <li><strong>Balanced</strong>: activated for some inputs and inactive for others</li> </ul> <p>Although this setup is extremely simple, our ultimate goal is to gain insights relevant to LLM training. One intuition is that many tricks used in LLM training—such as LR warmup, LR decay, weight decay, and MoE balancing—may implicitly control neuron activity levels, thereby influencing feature learning.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>An MLP with one hidden layer and one neuron can be written as (for simplicity, the input is also 1D):</p> \[f(x; w_1, b_1, w_2, b_2) = w_2\sigma(w_1x+b_1)+b_2, \quad \sigma(x) ={\rm ReLU}(x) \equiv {\rm max}(0,x)\] <p>For the teacher network, we set \(w_1^T = w_2^T = 1, \quad b_1^T = b_2^T = 0\), and so \(f(x)\equiv {\rm ReLU}(x)\). We take the input domain to be \(x \in [-1,1].\)</p> <p>For the student network, we initialize \(w_1^S = w_2^S = 1,\) and focus on varying the initializations of \(b_1^S\) and \(b_2^S\). Training uses MSE loss and the Adam optimizer (default LR = 0.01). Below, we only discuss the student’s weights, so we omit the superscript \(S\).</p> <hr/> <h2 id="observation-0-large-b_1-leads-to-local-minima">Observation 0: Large \(|b_1|\) Leads to Local Minima</h2> <p>We fix the initialization \(b_2 = 0\).</p> <ul> <li>When \(b_1 &gt; 1\), the neuron is initialized in the <strong>hyperactive</strong> state, and the loss gets stuck around 0.02 (corresponding to approximating ReLU with linear regression).</li> <li>When \(b_1 &lt; -1\), the neuron is initialized in the <strong>dead</strong> state, and the loss gets stuck around 0.1 (corresponding to approximating ReLU with a constant function).</li> </ul> <p>This may help explain why some initialization schemes (e.g., Kaiming initialization) set the bias to zero.</p> <hr/> <h2 id="observation-1-large-b_2-can-kill-the-neuron">Observation 1: Large \(|b_2|\) Can Kill the Neuron</h2> <p>We fix \(b_1 = 0\), so the neuron is <strong>balanced</strong> at initialization. Is everything fine then? Not quite. We find that when \(b_2 = -1.1\), the loss can go to zero, but when \(b_2 = -1.2\), the loss gets stuck around 0.02 (again corresponding to approximating ReLU with linear regression).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/loss-480.webp 480w,/assets/img/blogs/single-relu-neuron/loss-800.webp 800w,/assets/img/blogs/single-relu-neuron/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By closely examining the training dynamics, we find that the turning point \(x_t \equiv - b_1 / w_1\) moves left during training (starting from 0). When it moves past -1, the neuron transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>The local minimum for \(b_2 = -1.2\) corresponds to the linear regression solution (the neuron is in the hyperactive state):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/large_b2_init-480.webp 480w,/assets/img/blogs/single-relu-neuron/large_b2_init-800.webp 800w,/assets/img/blogs/single-relu-neuron/large_b2_init-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/large_b2_init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Conversely, if we increase the bias of the target function, the neuron also transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>This observation connects to two empirical practices:</p> <ul> <li><strong>Model bias</strong>: In some LLM training setups, bias terms are disabled. The observation above suggests that bias dynamics can drive neurons away from the balanced state.</li> <li><strong>Data bias</strong>: Whitening inputs and normalizing intermediate representations are common practices.</li> </ul> <hr/> <h2 id="observation-2-too-large-a-learning-rate-can-also-kill-the-neuron">Observation 2: Too Large a Learning Rate Can Also Kill the Neuron</h2> <p>We fix \(b_1 = 0\) and \(b_2 = -1\).</p> <ul> <li>When LR = 0.2, the model’s loss can be optimized to zero.</li> <li>When LR = 0.4, the loss only reaches 0.1 (again corresponding to fitting ReLU with a constant function), because the neuron becomes <strong>dead</strong>.</li> </ul> <p>This may be related to LR warmup in LLM training: if the initial learning rate is too large, the loss may decrease quickly, but at the cost of killing some neurons. Once dead, these neurons are hard (or impossible) to bring back to a balanced state.</p> <hr/> <h2 id="observation-3-silu-eventually-recovers-but-gets-stuck-at-a-saddle-point-for-a-long-time">Observation 3: SiLU Eventually Recovers, but Gets Stuck at a Saddle Point for a Long Time</h2> <p>For SiLU, we observe that the loss can eventually reach zero (up to machine precision), but training gets stuck at saddle points for very long time, reducing training efficiency.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/silu-480.webp 480w,/assets/img/blogs/single-relu-neuron/silu-800.webp 800w,/assets/img/blogs/single-relu-neuron/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="takeaway">Takeaway</h2> <p><strong>Irreversibility</strong>: once a neuron leaves the balanced state, it is very hard to return. Bias plays a crucial role. This might be why we have various ugly tricks for LLM (learning rate schedule, weight decay, etc).</p> <hr/> <h2 id="questionsideas">Questions/Ideas</h2> <p><strong>Question 1: How can we better control bias?</strong></p> <ul> <li>Can we analytically compute the bias instead of learning it via gradient descent?</li> <li>Can we use a different parameterization to better control neuron activation? For example, \(wx + b \;\to\; w(x + b'),\) where \(b'\) directly controls neuron activation (assuming the input distribution is known). This may allow us to more directly control neuron activity, e.g., by applying weight decay to \(b'\) or explicitly enforcing balancing.</li> </ul> <p><strong>Question 2: Which neurons have better learning dynamics?</strong></p> <ul> <li>SiLU is better than ReLU. Is there something better, gating?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1gSrKOVfEtVNTa7ZCUOpqOI1uTXm0SL9s?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026single-relu-neuron</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Training dynamics of A Single ReLU Neuron}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/single-relu-neuron/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics of AI – How to Begin</title><link href="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/" rel="alternate" type="text/html" title="Physics of AI – How to Begin"/><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-of-ai-recipe/"><![CDATA[<div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-480.webp 480w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-800.webp 800w,/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-of-ai-recipe/physics-of-ai-recipe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In a recent <a href="/blog/2026/physics-of-ai/">blog post</a>, we mentioned that <em>Physics of AI</em> requires a shift in mindset.<br/> This post will explain, from a slightly more technical perspective, <strong>how</strong> to do <em>Physics of AI</em>—I will present a <strong>modular framework</strong>.</p> <p>On the one hand, we need a framework. With a basic framework, the community can share a common language for communication. On the other hand, we must not be constrained by the framework itself. <em>Physics of AI</em> genuinely requires novel, out-of-the-box ideas.</p> <hr/> <h2 id="what-is-physics-of-ai">What Is Physics of AI?</h2> <p><em>Physics of AI</em>, as the name suggests, means studying AI in the same way we study physics.<br/> It aims to answer the following questions (so simple that they may sound trivial):</p> <blockquote> <p><strong>What models, on what data, exhibit what phenomena?</strong><br/> <em>(Bonus: why?)</em></p> </blockquote> <p>There are three core elements here: <strong>models, data, and phenomena</strong>.<br/> To emphasize data and phenomena (which are often overlooked by the community), I deliberately fold many details—such as optimizers and loss functions—into the notion of the <em>model</em>.</p> <p>Below, I elaborate on these three elements.</p> <hr/> <h2 id="models">Models</h2> <ul> <li> <p><strong>Architecture</strong><br/> Examples include MLPs, RNNs, Transformers, DINO, Mamba, KANs, etc.<br/> Architectures consist of various layers such as MLP layers, attention layers, convolutions, etc.</p> </li> <li> <p><strong>Training</strong></p> <ul> <li><strong>Paradigms</strong><br/> Supervised learning, unsupervised learning, self-supervised learning, representation learning, etc. In the LLM era, we further distinguish <em>pre-training</em>, <em>mid-training</em>, and <em>post-training</em>.</li> <li><strong>Optimizers</strong><br/> SGD, Adam, Muon, etc.</li> <li><strong>Loss functions / Rewards</strong><br/> MSE, cross-entropy, accuracy, diffusion loss, etc.<br/> These are often coupled with the training paradigm—for example, contrastive loss in representation learning.</li> <li><strong>Regularization</strong><br/> L2 weight decay, dropout, KL divergence, and so on.</li> </ul> </li> </ul> <hr/> <h2 id="data">Data</h2> <ul> <li><strong>Synthetic (toy) data</strong><br/> We know the data generation process, and have full control over it.</li> <li><strong>Real data</strong><br/> We do not fully know the data generation process.</li> </ul> <hr/> <h2 id="phenomena--observables">Phenomena / Observables</h2> <p>Beyond commonly tracked quantities such as loss curves and accuracy, we also care about:</p> <ul> <li> <p><strong>Biology-like phenomena (often data-dependent)</strong><br/> For example, understanding whether (and how) representations, computations, algorithms, or structures emerge during training.<br/> A well-known example is the <em>induction head</em>.<br/> The field of <em>mechanistic interpretability</em> is closely related here.</p> </li> <li> <p><strong>Physics-like phenomena (often data-independent)</strong><br/> For example, weight matrix spectra, activation subspaces, attention patterns, and so on.</p> </li> </ul> <hr/> <h2 id="what-should-we-measure">What Should We Measure?</h2> <p>A central and difficult question in AI phenomenology is: <strong>what exactly should we measure?</strong><br/> Of course, this depends on the phenomena we want to observe. But in order to observe phenomena, we must first know <em>which quantities to measure</em>—that is, <em>which observables correspond to which phenomena</em>. This creates a classic <strong>chicken-and-egg problem</strong>.</p> <p>We must start somewhere. Based on my experience, common starting points include:</p> <ul> <li> <p><strong>Making abstract ideas concrete</strong>, which leads to observables.<br/> For example, I may care about <em>feature learning</em>, but features are high-dimensional. I therefore define observables that capture certain aspects of feature learning. In this <a href="/blog/2026/feature-learning-1/">blog</a>, I use the <em>number of non-linear neurons</em> as one such observable.</p> </li> <li> <p><strong>Starting from a known phenomenon and discovering new ones.</strong><br/> For instance, some people study <em>grokking</em> and, during reproduction, discover new phenomena such as <em>loss spikes</em>, which then become a subject of study themselves. This leads to <a href="https://arxiv.org/abs/2206.04817">Apple’s slingshot paper</a>.</p> </li> <li> <p><strong>Starting from real data</strong>, and logging common observables during training.<br/> What counts as “common” requires accumulation of experience:<br/> (i) seeing what others in the field measure, and<br/> (ii) learning which observables were useful in your own past experiments.</p> </li> <li> <p><strong>Starting from toy data</strong>, imagining how the model <em>should</em> behave (prompting yourself: <em>“If you were an AI model, what would you do?”</em>), and then designing observables to test whether the model actually behaves that way.</p> </li> </ul> <p>In this way, the original chicken-and-egg problem becomes a <strong>spiral of ascent</strong>:<br/> more phenomena inspire new observables, and new observables lead to the discovery of more phenomena.</p> <hr/> <h2 id="why-does-this-phenomenon-occur">Why Does This Phenomenon Occur?</h2> <p>My personal thinking habit is to first ask:<br/> <strong>Is this phenomenon caused by the data, or by the model?</strong><br/> Here, “model” also includes random seeds: <em>how sensitive is the phenomenon to the random seed?</em></p> <p>The fastest way to get answers is through experiments—changing seeds, changing models, changing data, and checking whether the phenomenon persists. Once these basic experimental results are in hand, we can begin to form <strong>hypotheses</strong>. Hypotheses predict new observables and phenomena, which are then tested by further experiments.</p> <p>This forms a discovery loop:</p> <blockquote> <p><strong>Run experiments → observe phenomena → form hypotheses → design the next experiments</strong></p> </blockquote> <hr/> <h2 id="what-does-it-mean-to-understand-something">What Does It Mean to “Understand” Something?</h2> <p>There are many layers to asking “why” or claiming understanding. How much understanding is <em>enough</em>?</p> <p>My personal criterion is: <strong>when my predictions are broadly consistent with new experimental results, I consider that a success</strong>.<br/> The meaning of “broadly” is subtle. When I am lazy or tolerant, matching trends may be enough. When I am diligent or obsessive, I may want quantitative agreement as well.</p> <p>For example, the discovery of the \(t\sim \gamma^{-1}\) (\(t\): grokking time, \(\gamma\):weight decay) in <a href="https://arxiv.org/abs/2210.01117">our Omnigrok paper</a> came from pushing this obsession further. How far one pushes before stopping largely depends on the researcher’s personality, beliefs, and scientific taste. If we post our findings publicly, the whole community can push the frontier together (we’re all “blind men touching elephants”; different perspectives are always helpful).</p> <hr/> <h2 id="what-makes-a-model-good">What Makes a Model “Good”?</h2> <p>A common debate in the field is: <em>Which model is better?</em><br/> Is this judgment objective or subjective?</p> <p>The <strong>No Free Lunch theorem</strong> already gives the answer: no model is universally better than another across all data.<br/> The real philosophical question is therefore:</p> <blockquote> <p><strong>What kind of data is the world closest to?</strong></p> </blockquote> <p><em>Physics of AI</em> can objectively answer:<br/> <strong>what data, under what models, exhibits what phenomena</strong>—this part is fully objective and uncontested.</p> <p>Once we have a notion of <strong>which phenomena are desirable</strong> (possibly subjective, possibly objective), we can answer:<br/> <strong>what data requires what models</strong>.</p> <p>Going one step further, once we have a notion of <strong>what kind of data the world is closest to</strong> (again, possibly subjective or objective), we can answer:<br/> <strong>which models are good</strong>.</p> <p>The current state of AI development often skips the purely objective stage and jumps directly to subjective claims. As a result, discussions often devolve into “you say yours, I say mine,” lacking the <strong>shared knowledge</strong> required for meaningful communication.</p> <p>The goal of <em>Physics of AI</em> is precisely to construct this shared knowledge.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of AI – How to Begin}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-of-ai-recipe/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Physics of Feature Learning 1 – A Perspective from Nonlinearity</title><link href="https://kindxiaoming.github.io/blog/2026/feature-learning-1/" rel="alternate" type="text/html" title="Physics of Feature Learning 1 – A Perspective from Nonlinearity"/><published>2026-01-01T00:00:00+00:00</published><updated>2026-01-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/feature-learning-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/feature-learning-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Please Read our latest post <a href="/blog/2026/physics-of-ai/">Physics of AI Requires Mindset Shifts</a> for general philosophy.</p> <p>While reading <a href="https://arxiv.org/abs/2509.21519">Yuandong Tian’s work on explaining grokking through feature learning</a>, I found the proposed <em>three-stage dynamics of feature learning</em> particularly intriguing. However, grokking itself has some special characteristics (e.g., the modular addition dataset), which led me to wonder whether feature learning could be studied in a more <em>standard</em> setting. This motivated the low-dimensional regression example explored below.</p> <p>Rather than starting from mathematical derivations, I decided to go straight to experiments. Beyond plotting the loss, we also wanted to define observables that characterize <em>features</em>. One of the most basic observables is the <strong>number of nonlinear neurons</strong>. This measure does not care about the specific form of the features, only whether they are nonlinear.</p> <p>To define neuron activation cleanly, we use <strong>ReLU</strong> activations. A neuron is considered <em>nonlinear</em> if it is activated for some inputs and inactive for others.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>We consider the following regression function:</p> \[y = \sum_{i=1}^{d} \sin^2(f x_i), \quad d = 10,\; f = 10.\] <p>We use an MLP with 10-dimensional input, 100 neurons in the first hidden layer, 100 neurons in the second hidden layer, and a single output. We denote the architecture as ([10, 100, 100, 1]). Inputs are sampled from a standard Gaussian distribution, with a total of 500 samples. The training objective is MSE loss, optimized using <strong>Adam</strong> (learning rate \(10^{-3}\), full batch).</p> <hr/> <h2 id="observation-four-phases-during-training">Observation: Four Phases During Training</h2> <p>We find that all 100 neurons in the first hidden layer remain nonlinear throughout training. In contrast, the number of nonlinear neurons in the second hidden layer exhibits <strong>non-trivial dynamics</strong>. Therefore, all plots below focus on the evolution of nonlinear neurons in the second hidden layer.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss-480.webp 480w,/assets/img/blogs/feature-learning-1/loss-800.webp 800w,/assets/img/blogs/feature-learning-1/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Empirically, the training dynamics appear to consist of four phases:</p> <ul> <li> <p><strong>Phase I (first ~50 steps)</strong><br/> The loss decreases rapidly, while the number of active neurons drops sharply.<br/> <em>Hypothesis</em>: the model is removing irrelevant features.</p> </li> <li> <p><strong>Phase II (50–500 steps)</strong><br/> The loss plateaus around 1, and the number of active neurons remains roughly constant (around 14).<br/> <em>Hypothesis</em>: the model is stuck near a saddle point, leading to slow learning.</p> </li> <li> <p><strong>Phase III (500–1500 steps)</strong><br/> The loss decreases slowly, while the number of active neurons increases.<br/> <em>Hypothesis</em>: this corresponds to genuine feature learning.</p> </li> <li> <p><strong>Phase IV (after ~1500 steps)</strong><br/> The loss decreases rapidly (exponential convergence), while the number of active neurons stays constant.<br/> <em>Hypothesis</em>: the model is fine-tuning features, or fine-tuning the final linear layer.</p> </li> </ul> <p>There are many interesting questions one could study here.</p> <hr/> <h2 id="curiosity-driven-questions">Curiosity-Driven Questions</h2> <ul> <li> <p>Why does Phase I remove so many features?<br/> <em>Speculation</em>: is this related to optimization tricks in LLMs? For example, do weight decay and learning-rate warmup mainly serve to prevent excessive feature removal in this phase?</p> </li> <li> <p>Phase II appears “sticky.” How should we better understand the meaning of this solution?<br/> <em>Speculation</em>: is the loss plateau simply the result of linear regression?</p> </li> <li> <p>How do nonlinear features emerge in Phase III?</p> </li> <li> <p>What determines the exponential convergence rate in Phase IV?<br/> <em>Speculation</em>: once features are learned, does the problem effectively reduce to linear regression, which we know converges exponentially?</p> </li> </ul> <hr/> <h2 id="a-linear-regression-perspective">A Linear Regression Perspective</h2> <p>To better understand the dynamics, we analyze the model from a linear regression viewpoint. We compute three linear regression baselines:</p> \[X \to Y,\quad f_1 \to Y,\quad f_2 \to Y,\] <p>where \(f_1\) and \(f_2\) denote the features from the first and second hidden layers, respectively, which evolve during training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/feature-learning-1/loss_linear_regression-480.webp 480w,/assets/img/blogs/feature-learning-1/loss_linear_regression-800.webp 800w,/assets/img/blogs/feature-learning-1/loss_linear_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/feature-learning-1/loss_linear_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The loss plateau around 1 corresponds closely to the solution of linear regression.</li> <li>In the late stage of training, the blue and orange curves nearly coincide, indicating that the final-layer weights are essentially optimal at every moment, while the features are still being fine-tuned. As a result, the loss continues to decrease.</li> </ul> <p>This is not standard linear regression (where features are fixed and weights evolve), but rather a <strong>dual version</strong>: features evolve while weights remain near their instantaneous optimum. Interestingly, this still leads to exponential decay. A natural next step is to understand the convergence rate quantitatively.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/17gyczGrRuUAuWYuD_NFB7ZlCAA37p1cK?usp=sharing">here</a>.</p> <p>Observing interesting phenomena and asking good questions is already half the battle. We are still working toward explaining more of these effects. If you have any idea regarding how to understand some of the phenomena, please email me at lzmsldmjxm@gmail.com</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026feature-learning-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of Feature Learning 1 -- A Perspective from Nonlinearity}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/feature-learning-1/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming and Zheng, Zhiyun and Qu, Qingyu (January 2026). Physics of Feature Learning 1 – A Perspective from Nonlinearity. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2026/feature-learning-1/</p>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)，Zhiyun Zheng (郑植匀)，Qingyu Qu（屈清宇）]]></summary></entry><entry><title type="html">Physics of AI Requires Mindset Shifts</title><link href="https://kindxiaoming.github.io/blog/2025/physics-of-ai/" rel="alternate" type="text/html" title="Physics of AI Requires Mindset Shifts"/><published>2025-12-31T00:00:00+00:00</published><updated>2025-12-31T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2025/physics-of-ai</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2025/physics-of-ai/"><![CDATA[<p><strong>The “physics of AI” is still far from arriving.</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-of-ai-480.webp 480w,/assets/img/blogs/physics-of-ai-800.webp 800w,/assets/img/blogs/physics-of-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-of-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we use the Tycho–Kepler–Newton analogy, today’s AI development largely remains at the <strong>Tycho stage</strong>—experimentation and observation. Yet even at the level of observation, what we currently have is extremely primitive: most people focus on tuning for a handful of performance-based metrics. This stems from a fundamental difference in goals between physics and AI. Physics aims to <em>transform the world by understanding it</em>, where <em>understanding</em> itself occupies a central position. As a result, the field is very tolerant of work that provides insight even if it is (temporarily) useless. In contrast, AI aims directly to <em>transform the world</em>. The scaling laws of recent years have allowed the field to skip the “understanding” step and jump straight into transforming AI itself. In my view, this constitutes a form of <strong>cognitive debt</strong>—one that will inevitably have to be repaid, sooner or later, if not already.</p> <p>For this reason, it is premature to talk about AI’s “Newtonian mechanics.” Even at the level of basic phenomenology, we are still at a very early stage. Phenomenology can be relatively macroscopic—connecting different models, such as <strong>emergence</strong> and <strong>scaling laws</strong>—or more microscopic—focused on training dynamics, such as <strong>grokking</strong>, <strong>double descent</strong>, or the <strong>edge of stability</strong>. We first need to discover more phenomena; only then will we be motivated to model them and develop theories to study them.</p> <hr/> <p><strong>Why Is AI Phenomenology Hard to Develop?</strong></p> <p>Why is AI phenomenology so difficult to develop? I believe publication culture plays a major role. What is publishable is either work with strong performance gains (in which case phenomenology seems unnecessary), or a compelling story.</p> <p>There are two kinds of “good stories”:</p> <ul> <li><strong>Universality</strong>: the phenomenon must be demonstrated across many settings. <em>Edge of stability</em> is an example. This imposes a very high bar for publications.</li> <li><strong>Surprise</strong>: the phenomenon is striking and unexpected. This is rare and highly unpredictable—<em>grokking</em> being a representative case.</li> </ul> <p>This explains why the list of commonly cited AI phenomenology examples is so short. Given the historical stage of the “physics of AI,” we seem to hold phenomenology to excessively high expectations, which in fact hinders its development.</p> <p><a href="https://physics.allen-zhu.com/">Allen Zeyuan Zhu’s <em>Physics of LLMs</em></a> is excellent work, but from my conversations with friends, the common reaction is that it is interesting yet hard to know where/how to begin if they want to dive into the field. The same applies to our own work <a href="https://openreview.net/forum?id=knPz7gtjPW"><em>Superposition Leads to Robust Neural Scaling</em> (NeurIPS 2025 Best Paper Runner-up)</a>: people are curious about how such a story was conceived. I cannot speak for other researchers in the physics-of-AI space, but from my own experience, I spend an inordinate amount of time packaging a story—”wasting” my own time and increasing the distance between myself and readers.</p> <p>Moreover, phenomena that can be packaged into a story are exceedingly rare. Many phenomena that I personally find fascinating, but cannot turn into a paper, end up casually discarded.</p> <hr/> <p><strong>Toward a More Accessible Phenomenology</strong></p> <p>Therefore, I advocate for a more <strong>accessible and inclusive form of phenomenological research</strong>. This approach would be more tolerant than current AI phenomenology and closer in spirit to phenomenology in physics. It would:</p> <ul> <li>not be oriented toward immediate usefulness;</li> <li>not require being packaged into a complete story;</li> <li>place no restrictions on analytical tools, as long as they are effective in description/prediction.</li> </ul> <p>At the same time, it would emphasize:</p> <ol> <li> <p><strong>Controllability</strong><br/> Use toy models to simplify and abstract real-world settings, such that results can be reproduced with minimal resources (ideally a single notebook plus a CPU is sufficient).</p> </li> <li> <p><strong>Multi-perspective characterization</strong><br/> Describe the object of study from as many angles and metrics as possible—like blind men feeling an elephant.</p> </li> <li> <p><strong>Curiosity / hypothesis-driven exploration</strong><br/> Phenomena should yield new insights—qualitative is sufficient, quantitative is even better.</p> </li> </ol> <p>This kind of accessible phenomenology may not be easy to publish at mainstream AI conferences, but it is extremely valuable for <strong>community building</strong>. Perhaps researcher A discovers a phenomenon (the key is making it public), B connects it to another phenomenon they previously observed, C unifies the two, D develops some theoretical analysis, and E turns the insights into algorithmic improvements. The five of them can then write a paper together.</p> <p>Traditionally, A might only collaborate within a small circle, but my understanding of the physics-of-AI community is that it is still highly fragmented, often divided by application domains. For example, vision researchers tend to collaborate with other vision researchers, and their intuitions are shaped primarily by vision tasks.</p> <hr/> <p><strong>So what can we do?</strong></p> <p><strong>On my end: Starting Blogposts</strong></p> <p>I will start sharing my (our) own “AI phenomenology” research in the form of blog posts. The right expectation for readers is that a colleague is sharing partial results: the work may be incomplete, but the raw data and thought process are presented transparently.</p> <p>The goals are threefold:</p> <ol> <li> <p><strong>To force myself to record observations</strong><br/> As mentioned earlier, phenomena that cannot be turned into papers are often thrown away. This effort is partially inspired by <a href="https://kexue.fm/">Jianlin Su’s blog</a>—his focuses on mathematical principles, whereas mine will emphasize experimental observations (phenomenology), “physical” intuition, and, when necessary, some (semi-)quantitative analysis, providing problems and intuition for future mathematical work.</p> </li> <li> <p><strong>To attract researchers and students who share similar interests</strong><br/> If you are interested in exploring these ideas together, feel free to reach out.</p> </li> <li> <p><strong>Course preparation</strong><br/> I plan to offer a <em>Physics of AI</em> course at Tsinghua University. These blog posts (along with accompanying code) may eventually become course materials.</p> </li> </ol> <p><strong>On your end: How to Get Started</strong></p> <ol> <li> <p><strong>Find questions you care about</strong><br/> For example, studying parameterizations of diffusion-model losses, or reproducing known phenomena such as grokking.</p> </li> <li> <p><strong>Define a simple toy model</strong><br/> For instance, <a href="https://arxiv.org/pdf/2511.13720">Tianhong Li and Kaiming He’s JIT paper</a> uses a 2D spiral dataset to study loss parameterization. The best way to understand grokking is simply to train a modular addition task yourself.</p> </li> <li> <p><strong>Commit to fully understanding the toy model</strong><br/> This is the hardest step. We are often too eager to move from toy models to realistic ones (again, due to publishing culture). Once a toy model produces the desired positive result, we move on. This is a <em>supervised</em> use of toy models. I believe toy models reveal their greatest power when used <em>unsupervised</em>. As the name suggests, it is a toy—approach it with childlike curiosity, play with it, and understand it from every possible angle (like blind men touching elephants).</p> </li> </ol> <p>I cannot guarantee that the insights gained will immediately translate into performance improvements, but I believe that if we, as a field, continue to accumulate such insights, a <strong>percolation-like phase transition</strong> will eventually occur.</p> <hr/> <p>The first phenomenology blogpost is <a href="/blog/2026/feature-learning-1/">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics of AI Requires Mindset Shifts}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{December}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2025/physics-of-ai/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[The “physics of AI” is still far from arriving.]]></summary></entry><entry><title type="html">Achieving AGI Intelligently – Structure, Not Scale</title><link href="https://kindxiaoming.github.io/blog/2025/structuralism-ai/" rel="alternate" type="text/html" title="Achieving AGI Intelligently – Structure, Not Scale"/><published>2025-12-25T00:00:00+00:00</published><updated>2025-12-25T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2025/structuralism-ai</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2025/structuralism-ai/"><![CDATA[<p><strong>TL;DR</strong>: Structuralism AI is the inevitable path beyond scaling — <em>not because scaling is wrong, but because it will eventually hit the energy/data wall.</em></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/structuralist-ai-480.webp 480w,/assets/img/blogs/structuralist-ai-800.webp 800w,/assets/img/blogs/structuralist-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/structuralist-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p>Scaling laws have been the most important guiding principle in AI over the past few years. That much is undeniable. They have driven unprecedented performance gains and largely unified both industry and academia around a single direction. Yet the logic behind scaling laws is surprisingly simple: because AI struggles with Out-of-Distribution (OOD) tasks, the most straightforward solution is to collect more data and train larger models, until everything becomes In-distribution.</p> <p>Scaling laws therefore offer a future that is <strong>robust, but inefficient</strong>.</p> <p>Let me be very clear about my position. If we completely ignore constraints on energy and data, I do not doubt that <strong>pure scaling alone can eventually reach AGI</strong>. I have never doubted this. If compute were infinite and data unlimited, large models could, in principle, cover everything. The problem is precisely that the real world is not like this.</p> <p>So the question becomes:<br/> is there a <strong>more intelligent way</strong> to achieve AGI?</p> <p>I believe there is.<br/> And the answer is not more scale, but <strong>more structure</strong>.</p> <p>It is important that I deliberately use the word <em>structure</em>, not <em>symbol</em>. This distinction is intentional, and I will explain it later.</p> <hr/> <p>Why do we need structure? Because structure enables <strong>compression</strong>. And compression lies at the core of intelligence. As Ilya once put it: <em>Compression is intelligence.</em></p> <p>Consider a simple example. If fractal structure is allowed, the intrinsic complexity of a snowflake is extremely low—it is highly compressible. If structure is disallowed and one must describe it point by point, the apparent complexity of a snowflake is effectively infinite. Today’s scaling laws resemble the latter approach: using ever more parameters and computation to fit massive apparent complexity.</p> <p>A deeper example comes from celestial mechanics. The most direct way to model planetary motion is to store the positions of planets at every moment in time—a lookup table with enormous cost. Kepler achieved the first true compression by realizing that planetary orbits are ellipses, a global structure in time that dramatically reduced complexity. Newton achieved the second compression by discovering local dynamical laws that explained even more with fewer parameters.</p> <p>And where does modern AI stand? Work by Vafa and collaborators shows that Transformers do not naturally learn Newtonian world models. This means that <strong>correct physical structure does not reliably emerge from scale alone</strong>.</p> <p>Our current expectation that “structure will eventually emerge” often resembles primitive humans praying to gods. The only difference is that our sacrifices—data and compute—actually work (to some extent). And precisely because they work, we lack sufficient motivation to search for a more scientific, more intelligent path forward.</p> <hr/> <p>Structure is explicit and everywhere in the natural sciences. In fact, without structure, there would be no natural science at all.</p> <p>If we draw an analogy with the Tycho–Kepler–Newton trajectory, today’s AI still largely lives in the Tycho era: experiment-driven, data-driven, with only the beginnings of a Keplerian phase—empirical laws such as scaling laws. But unlike the history of celestial mechanics, we have turned these empirical laws into articles of faith, aggressively scaling experiments and engineering systems around them, rather than treating them as clues toward a deeper theory—a “Newtonian mechanics of AI.”</p> <p>From an intellectual perspective, this is not progress. It is regression.</p> <hr/> <p>At this point, you might say: “This is just another piece criticizing scaling and foundation models.”<br/> It is not.</p> <p>My stance is clear and neutral. According to the No Free Lunch theorem, every model has its domain of applicability and its limitations. Or, more bluntly: <em>All models are wrong, but some are useful.</em></p> <p>The real issue is not whether to use foundation models, but whether we understand that <strong>different tasks possess fundamentally different structures and compressibility</strong>. From a compression perspective, and by analogy with the natural sciences, tasks fall naturally into categories. Some are <strong>“physics-like”</strong>: highly compressible, with symbolic formulas emerging from continuous data. Some are <strong>“chemistry-like”</strong>: substantially compressible, with clear structure but incomplete or approximate symbols. Others are <strong>“biology-like”</strong>: only weakly compressible, dominated by empirical regularities and statistical induction. Pure noise exists as well, but no model can handle it, so we can safely ignore it.</p> <p>An ideal intelligent system should be able to recognize which kind of task it is facing and apply the appropriate degree of compression.</p> <p>Symbolic models excel at physics-like tasks but fail on chemistry- and biology-like ones. Connectionist models, due to their generality, can in principle handle all types—but precisely because they lack structure, they are extremely inefficient on physics- and chemistry-like problems.</p> <table class="table table-bordered"> <thead> <tr> <th style="border-right: 2px solid #dee2e6;"> <div style="display: flex; justify-content: space-between;"> </div> </th> <th>"Physics-like" tasks</th> <th>"Chemistry-like" tasks</th> <th>"Biology-like" tasks</th> </tr> </thead> <tbody> <tr> <td>**Symbolism AI**</td> <td>Excellent</td> <td>Poor</td> <td>Poor</td> </tr> <tr> <td>**Connectionism AI**</td> <td>Inefficient</td> <td>Inefficient</td> <td>Good</td> </tr> <tr> <td>**Structuralism AI**</td> <td>Good</td> <td>Good</td> <td>Good</td> </tr> </tbody> </table> <hr/> <p>This is why I argue for <strong>Structuralism</strong>.</p> <p>Symbolism starts from physics-like tasks. Connectionism starts from biology-like tasks. A natural question follows: can we build AI starting from chemistry-like tasks? Structuralism, by design, aims to capture this intermediate regime. We want symbols—stricter, more discrete structures—to emerge from structure, and we want empirical regularities—looser structures—to be learned by relaxing structure from data.</p> <p>In supervised learning, this distinction is already quite concrete. Linear regression is symbolic. Multi-layer perceptrons are connectionist. EQL represents a neural–symbolic hybrid. Kolmogorov–Arnold Networks (KANs), by contrast, are structuralist. The representation theory underlying KANs compactly captures the compositional structure of multivariate functions. As a result, KANs are neither structureless like MLPs, nor overconstrained like linear models, nor plagued by instability from neural–symbolic mismatch.</p> <p>Structuralism is not a compromise. It is a unification.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/four-ai-philosophy-480.webp 480w,/assets/img/blogs/four-ai-philosophy-800.webp 800w,/assets/img/blogs/four-ai-philosophy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/four-ai-philosophy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p>But the real world goes far beyond supervised learning. We do not merely learn structure from data—we compare structures, reuse structures, and build “structures of structures” (category theory!). This is <strong>abstraction</strong>.</p> <p>I want to argue explicitly: <strong>abstraction is one of the central bottlenecks to AGI</strong>. This aligns closely with Rich Sutton’s emphasis on abstraction in OaK architecture. Continual learning is fundamentally about preserving abstract invariances across tasks. Adaptivity and fluidity—for example in ARC-AGI—is about in-context abstraction. And many ARC-AGI tasks are, at their core, simplified forms of intuitive physics, an essential element of world models.</p> <p>So how do we enable abstraction? I will be honest: I do not yet have a complete solution.</p> <p>One insight is that abstraction arises from comparing and reusing structures. Attention is also a mechanism for comparison, but it implicitly assumes that structure can be embedded into vector spaces and that similarity can be measured by dot products. In reality, most structures are not isomorphic to vector spaces. We do this largely because it fits GPU computation, not because it is cognitively or scientifically correct.</p> <hr/> <p>I believe current AI development is <strong>secretly structuralist</strong>, but mostly in an extrinsic sense. Reasoning is structured. Agent frameworks are structured. Yet the underlying models remain connectionist. This is extrinsic structuralism, and it relies heavily on Chain-of-Thought-like data to explicitly supervise structure.</p> <p>I am willing to bet that the next wave of progress will come from <strong>intrinsic structuralism</strong>: injecting general-purpose structure into models, or enabling structure to emerge internally, without relying on explicit CoT supervision.</p> <p>From an application perspective, the AGI we actually need must be efficient, adaptive, generalizable, and physically grounded. Structure is essential to all four. The physical world itself is highly structured and highly compressible—compositionality, sparsity, temporal locality. Without these structures appearing in our models, world models will remain out of reach.</p> <hr/> <p><strong>In summary, Structuralism AI represents a path fundamentally different from scaling. It may be harder, but it is also more interesting, richer in opportunity, and far more promising in the long run.</strong></p> <p><strong>In 2026, it’s time to bet/work on something different.</strong></p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025structuralism-ai</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Achieving AGI Intelligently -- Structure, Not Scale}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{December}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2025/structuralism-ai/}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Text citation:</strong></p> <p>Liu, Ziming (December 2025). Achieving AGI Intelligently – Structure, Not Scale. KindXiaoming.github.io. https://KindXiaoming.github.io/blog/2025/structuralism-ai/</p>]]></content><author><name></name></author><category term="AI"/><category term="Structuralism-AI"/><category term="AGI"/><summary type="html"><![CDATA[TL;DR: Structuralism AI is the inevitable path beyond scaling — not because scaling is wrong, but because it will eventually hit the energy/data wall.]]></summary></entry><entry><title type="html">Philosophical thoughts on Kolmogorov-Arnold Networks</title><link href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/" rel="alternate" type="text/html" title="Philosophical thoughts on Kolmogorov-Arnold Networks"/><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/"><![CDATA[<p>Recently, collaborators and I proposed a new type of neural networks called the Kolmogorov-Arnold Networks (KANs), which are somewhat similar to but mostly different from Multi-Layer Perceptrons (MLPs).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/kan-480.webp 480w,/assets/img/blogs/kan-800.webp 800w,/assets/img/blogs/kan-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/kan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The technical differences between MLPs and KANs can be found in our paper and many discussions over the internet. This blogpost does not delve into technicalities, but want to lay out quick philosophical thoughts, open to discussion. I will attempt to answer the follwoing questions:</p> <ul> <li>Q1: Are KANs and MLPs the same?</li> <li>Q2: What’s the philosophical difference between KANs and MLPs?</li> <li>Q3: Which is more aligned with science, KANs or MLPs?</li> </ul> <h2 id="q1-are-kans-and-mlps-the-same">Q1: Are KANs and MLPs the same?</h2> <p>The argument that “KANs and MLPs are the same because they have the same expressive power” is similar to the argument “A human being and a cup are the same because they are both made up of atoms.” It is well accepted in physics that each (energy) level has a physical theory, so even if two systems share the same theory in the most microscopic level, they are not necessarily the same on higher levels because of emeregent phenomenon. Back to the MLPs vs KANs case, even though MLPs and KANs have the same expressive power (which is a foundational aspect), other aspects emerging from it might be quite different. These emerging properties include optimization, generalization, interpretability, etc.</p> <h2 id="q2-whats-the-philosophical-difference-between-kans-and-mlps">Q2: What’s the philosophical difference between KANs and MLPs?</h2> <p><strong>Reductionism vs. Holism</strong> While MLPs are more aligned with holism, KANs are more aligned with reductionism. The design principle of MLPs is “more is different”. In an MLP, each neuron is simple because it has fixed activation functions. However, what matters is the complicated connection patterns among neurons. The magical power of MLPs performing a task is an emergent behavior which is attributed to collective contribution from all neurons. By contrast, in a KAN, each activation function is complicated because it has learnable functions. By sparsification and pruning, we hope the computation graph to be simple. In summary, MLPs have simple ‘atoms’ but complicated ways to combine these atoms; KANs have complicated (diverse) ‘atoms’ but simple ways to combine these atoms. In this sense, MLPs’ expressive power comes from the complicated connection patterns (fully-connected structure), which give rise to emergent bahavior (holism). In contrast, KANs’ expressive power comes from the complexity of fundamental units (learnable activation functions), but the way to decompose whole network to units is simple (reductionsim).</p> <h2 id="q3-which-is-more-aligned-with-science-kans-or-mlps">Q3: Which is more aligned with science, KANs or MLPs?</h2> <p>We ask a crazy question: if our science is written by a neural network, is it more likely to be generated with KANs or with MLPs? I tend to say KANs are more aligned with our science. Note that the way we write science, is mostly based on reductionism. Throughout the history of science, reducntionism has been the standard way of thinking. It was not until very recently did scientists start to realize the importance of holism (“more is different”), however, the study of complex systems is extremely hard and many tools are still based on reductionism. Because our science is mostly reductionism, and KANs are more aligned with redunctionsim than MLPs, KANs are more promising tools to describe science. If you think the above discussion is too abstract, let us just consider a concrete task of compiling a symbolic formula into neural networks. Given the formula of Hooke’s law \(F=kx\), it is unclear how to compile this formula into MLPs with (say) ReLU activations, but it is quite straight-forward to compile it into KANs by leveraging flexiable activation functions. For example \(kx=((k+x)^2-k^2-x^2)/2\) can be represeted as a \([2,2,1]\) KAN; when both \(k, x\) are postive, we even just need a \([2,1,1]\) KAN by leveraging \(kx={\rm exp}({\rm log}k+{\rm log}x)\).</p> <h2 id="closing-remarks">Closing Remarks</h2> <p>A model performs well on a task, when the inductive bias of the model meets the inductive bias of the task. So there is no free lunch – because KANs are good at science, there must be something KANs are not good at. MLPs are not good at science, people have shown their effectivenss on many non-scientific tasks including vision and languages. Trained as a physicist, I tend to think in redunctionsim. I constantly think about how a dataset can be generated from simple primitives (a process my advisor Max calls “braining”, i.e., training with my own brains), because it can inspire the design of better models. However, real world can be too complicated for my tiny brain to fully imagine and understand. Sometimes we just have to get our hands wet before burning out our brains. Combing back to the KAN vs MLP case, although I would love to understand their strengths and limitations with pure reasoning (philosophical thinking is one way), empirical experiments (guided by some reasoning) are probably more effective.</p>]]></content><author><name></name></author><category term="AI"/><category term="Kolmogorov-arnold-networks"/><category term="interpretability"/><category term="AI-for-Science"/><summary type="html"><![CDATA[Recently, collaborators and I proposed a new type of neural networks called the Kolmogorov-Arnold Networks (KANs), which are somewhat similar to but mostly different from Multi-Layer Perceptrons (MLPs).]]></summary></entry></feed>