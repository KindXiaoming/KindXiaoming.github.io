<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-11T06:26:52+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Sparse attention 3 – inefficiency of extracting similar content</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-3/" rel="alternate" type="text/html" title="Sparse attention 3 – inefficiency of extracting similar content"/><published>2026-01-12T00:00:00+00:00</published><updated>2026-01-12T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In <a href="/blog/2026/sparse-attention-1/">Sparse-attention-1</a>, we showed that a single-layer attention model (without positional embeddings) can learn to copy the <strong>current</strong> token. In <a href="/blog/2026/sparse-attention-2/">Sparse-attention-2</a>, we showed that the same model is unable to extract a specific previous token based on <em>position</em>, e.g., the last token. But what about extracting a previous token based on <em>content</em> (semantic meaning)? This is a more natural task, because it is precisely what attention is designed to do—attend to similar content.</p> <p>As we show below, somewhat surprisingly, a single attention layer is very inefficient at performing this extraction task.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to extract the token that is most similar to the current token. We assume tokens have numerical meanings; for example, token \([5]\) represents the number \(5\). Then \([5]\) is closer to \([6]\) than to \([8]\) because \(1=|5-6| &lt; |5-8|=3.\)</p> <p>Taking context length = 4 as an example, the task is to predict \([1][5][10][6] \rightarrow [5]\).<br/> This is because \([5]\) is the closest to \([6]\), than \([1]\) or \([10]\).</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>. The model consists only of an Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers and no positional embeddings.</p> <hr/> <h2 id="failure-even-for-context-length--3-with-small-embedding-dimension">Failure even for context length = 3 (with small embedding dimension)</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e., \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 3 (task: \([1][5][6]\to[5]\)), as we vary the vocabulary size, we consistently observe that perplexity converges to \(V/2\) (or lower, for smaller \(V\)).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_2-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_2-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By visualizing the learned embeddings, we find that they exhibit a continuous structure; that is, numerically closer tokens are embedded closer together:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_final_2D-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_final_2D-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_final_2D-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_final_2D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-vocab-size-v">Dependence on vocab size \(V\)</h2> <p>The existence of structure may explain why the perplexity can fall slightly below \(V/2\): the continuous geometric structure is partially learned and leveraged to reduce the loss. However, the model largely becomes confused (similar to what we observed in the <a href="/blog/2026/sparse-attention-2/">previous blog</a>) and effectively guesses a random token from the previous context. This leads to a perplexity of \(\frac{C-2}{C-1}V\), which qualitatively (though not quantitatively) matches the experimental results:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/perplexity_C-480.webp 480w,/assets/img/blogs/sparse-attention-3/perplexity_C-800.webp 800w,/assets/img/blogs/sparse-attention-3/perplexity_C-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/perplexity_C.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="dependence-on-embedding-dimension">Dependence on embedding dimension</h2> <p>We find that the best achievable perplexity decreases with \(n_{\rm embd}\), in fact faster than \(V/n_{\rm embd}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/n_embd_dependence-480.webp 480w,/assets/img/blogs/sparse-attention-3/n_embd_dependence-800.webp 800w,/assets/img/blogs/sparse-attention-3/n_embd_dependence-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/n_embd_dependence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This may be explained by the increasingly improved geometry of the embeddings (as seen in the first two principal components) as we increase \(n_{\rm embd}\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-3/embd_geometry-480.webp 480w,/assets/img/blogs/sparse-attention-3/embd_geometry-800.webp 800w,/assets/img/blogs/sparse-attention-3/embd_geometry-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-3/embd_geometry.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="questions--ideas">Questions / Ideas</h2> <ul> <li>Our results demonstrate the ineffectiveness of attention in extracting similar content in this setting.</li> <li>One possible fix is to replace the inner product of Q/K with the Euclidean distance between Q and K. Ideally, a 1D embedding would already be sufficient if the kernel computes Euclidean distances and weighs probabilities inversely proportional to distance (similar to <a href="https://arxiv.org/abs/2502.01628v1">harmonic loss</a>). The innner-product in attention probably produces the inefficiency, and we should try other distance measures.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1k8IEg_zc12_8XHOE0SjWv5LyeaZBC5cK?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 3 -- inefficiency of extracting similar content}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Emergence of Induction Head Depends on Learning Rate Schedule</title><link href="https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule/" rel="alternate" type="text/html" title="Emergence of Induction Head Depends on Learning Rate Schedule"/><published>2026-01-11T00:00:00+00:00</published><updated>2026-01-11T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/induction-head-lr-schedule/"><![CDATA[<p><strong>Author: Qingyu Qu (屈清宇), Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Induction heads are crucial for in-context learning abilities. However, the dependence of its emergence on learning rate schedule is fewly discussed. Indeed, at first glance, the emergence of induction heads should depend more, if not exclusively, on <strong>structure of data</strong>. But it makes sense to think of the <strong>learning rate schedule</strong> because such a delicate module like induction head should not use a large learning rate to construct. Below we make an early exploration of the dependence of induction head emergence on learning rate schedule.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We adopt a similar setup as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>, where a two layer transformer(with mlps) is trained on openwebtext data. The details are as follows.</p> <p><strong>Model</strong><br/> The model here is a two layer transformer(with mlps). Embedding dimension is 768, number of heads per layer is 12, these numbers are chosen to be the same as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>. Weight tying between embedding and unembedding is used. The implementation of the model is a modified version of <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>. The train loop is also basically train.py in <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>.</p> <p><strong>Dataset</strong><br/> We use <a href="https://huggingface.co/datasets/Skylion007/openwebtext">openwebtext</a> as the training dataset, because we found that natural language data is important for induction head emergence. We also found many papers arguing that induction heads emerge from synthetic data, but our “induction heads” emerging from synthetic data can rarely perform well on repeated random sequences, which we think is an important ability of induction heads shown in <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>.</p> <p><strong>Induction score</strong> To measure the emergence of induction heads, we adopt a similar strategy as <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>, which they called prefix matching score. We test the model checkpoint on a repeated random sequence, and calculate a head’s attention weight given to the token we expect an induction head to attend to – the token where the prefix matches the present context. We adopt the largest induction score among all heads to be the induction score of the model. In other words, a large induction score means that there is at least one head showing induction attention pattern on repeated random sequences.</p> <hr/> <h2 id="fixed-learning-rate-fails-to-give-rise-to-induction-heads">Fixed learning rate fails to give rise to induction heads.</h2> <p>We train the same model using the same data. The only difference is that the learning rate is among \(\{6\times 10^{-5}, 6\times 10^{-4}, 6\times 10^{-3}\}\). We train the model until the accumulated learning rate reaches threshold 3, i.e. we train the model for \(\{5\times 10^4, 5\times 10^3, 5\times 10^2\}\) steps. The goal of this trick is to balance the time effect introduced by learning rate, i.e. 1 step under 0.1 learning rate is roughly 100 steps under 0.01 learning rate.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/fix_lr-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/fix_lr-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/fix_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/fix_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As shown by the figure, the induction score is low. In fact, in our setup, a random sequence of length 10 is repeated 5 times, so the random guess induction score is around 0.1. One might argue we do not train enough, but below we show that we can get induction heads with these many steps(or accumulated learning rates) using learning rate warmup.</p> <hr/> <h2 id="learning-rate-warmup-to-the-right-plateau-is-important-for-induction-heads-emergence">Learning rate warmup to the right plateau is important for induction heads emergence</h2> <p>In this section, we first warm up the learning rate for 100 steps to a plateau among \(\{6\times 10^{-5}, 6\times 10^{-4}, 6\times 10^{-3}\}\), then the learning rate will be stable until the end of training.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/warmup_lr-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/warmup_lr-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/warmup_lr-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/warmup_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that, in our experiment, the plateau learning rate can be neither too large nor too small. One may doubt that our training in \(6\times 10^{-3}\)case is not enough. The following figure shows that, even though we train 5000 steps in \(6\times 10^{-3}\)case, induction heads still miss.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/verify-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/verify-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/verify-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/verify.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="learning-rate-warmup-iters-affect-the-speed-of-emergence-of-induction-heads">Learning rate warmup iters affect the speed of emergence of induction heads</h2> <p>Since we found that \(6\times 10^{-4}\) is the right plateau for learning rate warmup, we test the learning rate schedule with different warmup iterations to this plateau. The result is as follows.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/warmup_steps-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/warmup_steps-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/warmup_steps-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/warmup_steps.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As shown by the figure above, warmup iter=100 leads to the fastest emergence of induction heads.</p> <hr/> <h2 id="learning-rate-decay-has-little-effect-on-induction-head-emergence">Learning rate decay has little effect on induction head emergence.</h2> <p>This may sound counter-intuitive since learning rate decay is crucial for constraining the model not to overfit the data, which will destroy the generalisation solution like induction head. However, for a small model trained on a big dataset like openwebtext, maybe overfitting can hardly happen, and this is also verified by the train and validation loss are always the same during the course of training. At the same time, learning rate decay has little effect on the final accumulated learning rate since most of the accumulated learning rate is contributed by the peak learning rate. Below, we train the model for 5000 steps and ignore the little difference in final accumulated learning rate.</p> <p>We use a linear warmup of 100 steps and use cosine decay to a min_lr.</p> <p>If the plateau learning rate = \(6\times 10^{-3}\), induction heads will not emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/decay_6e-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If the plateau learning rate = \(6\times 10^{-4}\), induction head will emerge.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-480.webp 480w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-800.webp 800w,/assets/img/blogs/induction-head-lr-schedule/decay_6e-4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/induction-head-lr-schedule/decay_6e-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qu2026induction-head-lr-schedule</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Emergence of Induction Head Depends on Learning Rate Schedule}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Qu, Qingyu; Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/induction-head-lr-schedule/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Qingyu Qu (屈清宇), Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 2 – Unattention head, branching dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/" rel="alternate" type="text/html" title="Sparse attention 2 – Unattention head, branching dynamics"/><published>2026-01-10T00:00:00+00:00</published><updated>2026-01-10T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In the <a href="/blog/2026/sparse-attention-1/">previous blog</a>, we found that a single-layer attention model (without positional embeddings) can learn to copy the <strong>current</strong> token. But what about copying <strong>earlier</strong> tokens? Since positional embeddings are missing, it seems impossible to determine where each token is located in the sequence. Under this reasoning, the task should be unsolvable, and I would expect the loss to be \({\rm log}(V)\), or equivalently the perplexity to be \(V\), where \(V\) is the vocabulary size.</p> <p>As we show below, however, the story is more subtle.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is to copy the previous token. Taking context length = 4 as an example, this corresponds to<br/> \([A][B][C][D] \rightarrow [C]\).<br/> In fact, due to the absence of positional embeddings, and because attention is equivariant under permutations of previous tokens, copying <em>any</em> previous token defines an equivalently difficult task.</p> <p><strong>Model</strong><br/> We stick to the toy model from the <a href="/blog/2026/sparse-attention-1/">previous blog</a>. The model consists only of an Embedding layer, an Unembedding layer, and a single Attention layer, with no MLP layers.</p> <hr/> <h2 id="observation-loss-plateau-embedding-branching-dynamics">Observation: loss plateau, embedding branching dynamics</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 2 (task: \([A][B]\to[A]\)), as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em> (as in the <a href="/blog/2026/sparse-attention-1/">previous blog</a>), where the perplexity is approximately half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/perplexity-embed-480.webp 480w,/assets/img/blogs/sparse-attention-2/perplexity-embed-800.webp 800w,/assets/img/blogs/sparse-attention-2/perplexity-embed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/perplexity-embed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By visualizing the evolution of the embeddings, we find that the dynamics proceed in two stages. In the first stage, there is a dominant direction along which the embeddings expand. Only in the second stage do the embeddings begin to move along the orthogonal direction, eventually forming a circular structure. This second stage also exhibits interesting branching dynamics:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/gif-480.webp 480w,/assets/img/blogs/sparse-attention-2/gif-800.webp 800w,/assets/img/blogs/sparse-attention-2/gif-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/gif.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="whats-happening">What’s happening?</h2> <p>My first reaction was: how could this task possibly be solved? The low final loss clearly indicates that the model must have developed some internal algorithm to accomplish it. The emergence of circular embeddings is particularly suggestive, hinting that the model has learned something nontrivial and elegant.</p> <p>We can inspect the learned \(W_Q/W_K\) matrices (\(W_V\) is unimportant here since we only care about the attention pattern):</p> \[W_Q = \begin{pmatrix} 13.24 &amp; -3.40 \\ -3.28 &amp; -14.7 \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -14.03 &amp; 3.56 \\ 3.19 &amp; 13.99 \\ \end{pmatrix}\] <p>We observe two numerical clusters (ignoring signs): one around 13 (denoted \(a\)), and another around 3 (denoted \(b\)). We therefore approximate them by the following symbolic forms:</p> \[W_Q = \begin{pmatrix} a &amp; -b \\ -b &amp; -a \\ \end{pmatrix}, \quad W_K = \begin{pmatrix} -a &amp; b \\ b &amp; a \\ \end{pmatrix},\] <p>Let the two inputs to the attention layer be \(E_1 = (x_1, y_1)^T\) and \(E_2 = (x_2, y_2)^T\). The attention logit between them is</p> \[E_2 W_K^T W_Q E_1 = (a x_1 - b y_1,\,-b x_1 - a y_1) \cdot (-a x_2 + b y_2,\, b x_2 + a y_2) = -(a^2 + b^2)(x_1 x_2 + y_1 y_2).\] <p>Since both \(E_1\) and \(E_2\) lie on a circle, the term \(x_1 x_2 + y_1 y_2\) is simply their inner product. The self-attention logit is</p> \[E_2 W_K^T W_Q E_2 = -(a^2 + b^2)(x_2^2 + y_2^2) &lt; E_2 W_K^T W_Q E_1.\] <p>This means that the model attends more strongly to the previous token than to the current token. Importantly, however, this effect is achieved not by <em>increasing</em> attention to the previous token, but by <em>suppressing</em> attention to the current token. This distinction is subtle for context length = 2, but becomes crucial for longer contexts.</p> <hr/> <h2 id="longer-context-length">Longer context length</h2> <p>The attention layer has discovered a way to <em>unattend</em> the current token, but it still cannot distinguish among earlier tokens due to permutation equivariance in the absence of positional embeddings. As a result, the model can do no better than guessing: for context length \(C\), it effectively selects one token uniformly at random from the first \(C-1\) tokens, with probability \(\frac{1}{C-1}\) of choosing any particular one. The probability of guessing the wrong token is therefore \(\frac{C-2}{C-1}\).</p> <p>With vocabulary size \(V\), the best achievable perplexity is thus \(\frac{C-2}{C-1} V\), which is better than pure random guessing \(V\). We verify empirically below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-2/best_perplexity-480.webp 480w,/assets/img/blogs/sparse-attention-2/best_perplexity-800.webp 800w,/assets/img/blogs/sparse-attention-2/best_perplexity-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-2/best_perplexity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This suggests that for long sequences, an attention layer without positional embeddings cannot reliably select a specific previous token. Positional embeddings are therefore necessary to implement, for example, a previous-token head. That said, the discovered strategy of <em>unattending the current token</em> is itself an interesting and nontrivial phenomenon.</p> <hr/> <h2 id="acknowledgement">Acknowledgement</h2> <p>Kaiyue Wen suggests that the sticky plateau might be due to the fact that Sub-n-grams are near-stationary Points (<a href="https://arxiv.org/pdf/2508.12837v1">paper</a>).</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/135t06Hz2FCrEKiTVErR8P4_MiPhl4pnE?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 2 -- Unattention head, branching dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 1 – sticky plateau and rank collapse</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/" rel="alternate" type="text/html" title="Sparse attention 1 – sticky plateau and rank collapse"/><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>The idea of attention is to select what we want from a collection of items. Token embeddings select based on content itself, while positional embeddings select based on position.</p> <p>In this article, we examine attention’s ability to select based on content. Therefore, for simplicity, we ignore positional embeddings.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p><strong>Dataset</strong><br/> The task is: select the current token. Taking context length = 4 as an example, this is<br/> \([A][B][C][D] \rightarrow [D]\).</p> <p><strong>Model</strong><br/> The model consists of only Embedding, Unembedding, and a single Attention layer, with no MLP layers. The main tunable parameters are the vocabulary size, embedding dimension, and context length.</p> <hr/> <h2 id="observation-sticky-plateau">Observation: sticky plateau</h2> <p>We examine the loss curves (in practice, we plot perplexity, i.e. \({\rm exp(loss)}\)). For embedding dimension = 2 and context length = 4, as we vary the vocabulary size, we consistently observe a <em>sticky plateau</em>, where the perplexity corresponds to half of the vocabulary size.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/sticky-plateau-480.webp 480w,/assets/img/blogs/sparse-attention-1/sticky-plateau-800.webp 800w,/assets/img/blogs/sparse-attention-1/sticky-plateau-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/sticky-plateau.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Explanation</strong><br/> We visualize the evolution of the embeddings (vocab size = 4) and find that there are two stages. In the first stage, the blue and yellow directions coincide, and the green and red directions coincide. As a result, the model cannot distinguish blue from yellow, or green from red, but it can distinguish which group a token belongs to. In the second stage, the elements within each group are finally separated.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/branching-480.webp 480w,/assets/img/blogs/sparse-attention-1/branching-800.webp 800w,/assets/img/blogs/sparse-attention-1/branching-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/branching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="how-general-is-the-sticky-plateau">How general is the sticky plateau?</h2> <p>Although the sticky plateau phenomenon is interesting, how general is it? When the embedding dimension increases, there is enough resolution to distinguish different tokens, and the plateau may disappear. We indeed observe that this is the case. However, if we increase the context length, the sticky plateau comes back again. The picture, therefore, is that packing as many items as the context length into an embedding space of a given dimension leads to a situation where the presence or absence of a sticky plateau seems to correspond to a percolation phase transition.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/generality-480.webp 480w,/assets/img/blogs/sparse-attention-1/generality-800.webp 800w,/assets/img/blogs/sparse-attention-1/generality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/generality.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="rank-collapse">Rank collapse</h2> <p>From the embedding visualizations above, we find that the embeddings initially expand along a single direction, and only later expand along another direction. Therefore, we expect that the effective dimensionality of the embeddings (defined in the same way as in the previous <a href="/blog/2026/unigram-toy-1/">blog post</a>), as well as the effective rank of the Q/K/V matrices, should exhibit corresponding dynamics.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-1/eff_rank-480.webp 480w,/assets/img/blogs/sparse-attention-1/eff_rank-800.webp 800w,/assets/img/blogs/sparse-attention-1/eff_rank-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-1/eff_rank.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Left: Q/K also appear very sticky during the loss plateau, which may indicate that the dynamics of Q/K are the primary cause of the loss plateau.</p> <p>Right: when all parameters are scaled up, Q/K become less sticky. Another observation that differs from the low-dimensional case is that when the loss decreases, the rank also decreases; when the loss plateaus, the rank increases; and when the loss decreases again, the rank decreases accordingly. The picture is that when the model is very clear about which features are useful, it aggressively exploits (grows) those features while (relatively) suppressing others, leading to a decrease in rank. When the loss reaches a plateau, the model explores which new features should grow, leading to an increase in rank. Once a useful new feature emerges, the model again exploits it, causing the rank to decrease.</p> <hr/> <h2 id="test-on-large-models">Test on large models</h2> <p>The sticky plateau phenomenon predicts a loss plateau at \({\rm Log}(V/2)\), about 0.3 nats below \({\rm Log}(V)\). To the best of my knowledge, this plateau has never been observed in LLMs, maybe due to</p> <ul> <li>learning rate warmup has already nicely handled this problem</li> <li>we did not zoom in enough to see the plateau</li> <li>the sticky plateau is simply less relevant for large models</li> </ul> <p>Either way, no matter if the sticky plateua is relevant for LLMs, we shoud better understand/control representation/rank collapses and the percolation phase transition.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1lXsAuzQ2nw009an8lNt9PYv_HqR1GhoV?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 1 -- sticky plateau and rank collapse}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Unigram toy model is surprisingly rich – representation collapse, scaling laws, learning rate schedule</title><link href="https://kindxiaoming.github.io/blog/2026/unigram-toy-1/" rel="alternate" type="text/html" title="Unigram toy model is surprisingly rich – representation collapse, scaling laws, learning rate schedule"/><published>2026-01-08T00:00:00+00:00</published><updated>2026-01-08T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/unigram-toy-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/unigram-toy-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Many phenomena have been observed during the training of LLMs—such as scaling laws and (early-stage) representation collapse. Empirically, there are also many training tricks—for example, carefully designing the learning rate schedule, including early LR warmup and late-stage LR decay. Are these phenomena and tricks <strong>specific to LLMs</strong>? Can we reproduce them in <strong>much simpler toy models</strong>?</p> <p>The goal of this blog post is to study an extremely simple <strong>Unigram model</strong>, and surprisingly, we find that many of the phenomena observed in LLMs already appear in this toy setting. Thanks to the simplicity of the toy model, we can more systematically investigate the <strong>origins</strong>, <strong>conditions</strong>, and <strong>control mechanisms</strong> of these phenomena.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider the case with context length equal to 1, i.e., predicting the next token \([B]\) from the current token \([A]\).</p> <p><strong>Dataset</strong><br/> We consider a Unigram dataset—\([A]\) and \([B]\) are independent and are independently drawn from a discrete distribution. Let the vocabulary size be \(V\), and the discrete distribution be<br/> \({\{p_i; \, p_i \ge 0, \sum_{i=1}^V p_i = 1\}}.\)</p> <p>Note that this dataset is even simpler than a Bigram dataset, because here the output \([B]\) does <strong>not</strong> depend on the input \([A]\) at all. The model’s task is simply to learn the token frequency distribution. One may ask: why study such a simple dataset, and does it have any relevance to natural language?</p> <ul> <li>First, the Zipf (heavy-tailed) frequency distribution is a characteristic feature of natural language. Although natural language has many other properties, here we focus exclusively on how <strong>single-token frequency distributions</strong> affect training dynamics.</li> <li>Second, in the learning rate warmup section below, we will provide evidence—namely a <em>sticky plateau</em>—suggesting that LLMs indeed exhibit signs of learning Unigram statistics during early training.</li> </ul> <p>For this dataset, the minimum achievable loss is simply the entropy of the frequency distribution: \(L_{\rm min} = S(\{p_i\}) = - \sum_{i=1}^V p_i \log p_i.\) We further assume that the frequencies follow a power law: \(p_i \sim i^{-\alpha}, \quad i = 1, 2, \dots, V,\) where \(\alpha\) controls the degree of heavy-tailedness. The case \(\alpha = 1\) corresponds to the standard Zipf distribution.</p> <p><strong>Model</strong><br/> We consider a very simple model with only an embedding layer and an unembedding layer. For token \(i\), the embedding vector is \(E_i \in \mathbb{R}^D\) and the unembedding vector is \(U_i \in \mathbb{R}^D\). We usually tie the two weights, i.e., \(E_i = U_i.\)</p> <hr/> <h2 id="1d-embedding-is-sufficient">1D embedding is sufficient</h2> <p>We take \(V = 10\), \(\alpha=1\), and embedding dimension \(D = 1\). Training uses batch size 128, while evaluation uses batch size 2,000,000. All data are drawn online. We find that training converges to the entropy (left and middle plots), and the learned embeddings show that tokens with higher frequency have larger values.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/1d_embedding-480.webp 480w,/assets/img/blogs/unigram-toy-1/1d_embedding-800.webp 800w,/assets/img/blogs/unigram-toy-1/1d_embedding-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/1d_embedding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here is a small detail worth pointing out: if we <strong>do not tie</strong> \(U\) and \(E\), the model has more degrees of freedom and converges faster.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/1d_embedding_untie-480.webp 480w,/assets/img/blogs/unigram-toy-1/1d_embedding_untie-800.webp 800w,/assets/img/blogs/unigram-toy-1/1d_embedding_untie-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/1d_embedding_untie.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Mathematical construction</strong><br/> Why emphasize this detail? When \(E\) and \(U\) are not tied, we can easily construct \(E_i = 1, \quad U_i = \log p_i,\) which exactly realizes the target distribution. However, when \(E\) and \(U\) share parameters, we require the logits \(E_i U\) and \(E_j U\) to induce the same probability distribution, which implies \(E_i = E_j\) and hence \(U_i = U_j\). This would lead to a uniform distribution, contradicting the heavy-tailed distribution we want.</p> <p>In practice, the model seems to find a sufficiently good trade-off: embeddings for different tokens have similar magnitudes, yet remain clearly distinguishable. Since NanoGPT uses weight tying, we will also continue to use weight tying in what follows.</p> <hr/> <h2 id="representation-collapse">Representation collapse</h2> <p>Since a 1D embedding is already sufficient, an extreme case with higher-dimensional embeddings is that different embedding dimensions behave identically. In that case, the effective subspace dimension of the embeddings is actually 1. While reality may not be so extreme, this motivates us to measure the <strong>effective dimensionality</strong> of representations, which may be related to representation collapse observed in LLMs.</p> <p>For an embedding matrix, we perform principal component analysis (PCA), treat the explained variance ratios as a probability distribution, compute its entropy \(S\), and define the effective dimension as \(n_{\rm eff} = \exp(S).\)</p> <p>Taking \(V = 100\) and \(D = 100\), we indeed observe representation collapse, which becomes more pronounced for larger learning rates.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/repr_collapse-480.webp 480w,/assets/img/blogs/unigram-toy-1/repr_collapse-800.webp 800w,/assets/img/blogs/unigram-toy-1/repr_collapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/repr_collapse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="loss-scaling-laws">Loss scaling laws</h2> <p>We study temporal scaling of the form \(L - L_{\rm min} \propto t^{-\beta},\) where \(\beta\) measures the speed of loss decay. We vary \(\alpha\) and \(V\) and examine how \(\beta\) depends on them. We fix \(D = 100\) and learning rate \(\text{lr} = 0.001\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/scaling_law-480.webp 480w,/assets/img/blogs/unigram-toy-1/scaling_law-800.webp 800w,/assets/img/blogs/unigram-toy-1/scaling_law-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/scaling_law.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that:</p> <ul> <li>larger \(V\) leads to smaller \(\beta\);</li> <li>larger \(\alpha\) leads to larger \(\beta\).</li> </ul> <p>Both trends are intuitive: larger \(V\) or smaller \(\alpha\) corresponds to a harder task.</p> <hr/> <h2 id="learning-rate-decay">Learning rate decay</h2> <p>We next ask whether late-stage learning rate decay is helpful in this toy setup. We find that learning rate decay indeed helps, and the improvement becomes larger as the system size increases (i.e., larger embedding dimension \(D\)). We train for 2000 steps with \(\text{lr} = 0.01\), then linearly decay the learning rate from 0.01 to 0.001 over the final 1000 steps. We fix \(V = 100\) and \(\alpha = 1\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/lr_decay-480.webp 480w,/assets/img/blogs/unigram-toy-1/lr_decay-800.webp 800w,/assets/img/blogs/unigram-toy-1/lr_decay-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/lr_decay.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>How can we understand this? When the system has many degrees of freedom, and each update is normalized, many degrees of freedom may update in similar directions. Their combined effect can be too large, causing overshoot. In this case, reducing the learning rate is necessary to avoid overshooting.</p> <hr/> <h2 id="learning-rate-warmup">Learning rate warmup</h2> <p>Can our experiments shed light on the necessity of learning rate warmup? In the representation collapse section, we showed that large learning rates lead to more severe representation collapse—this is one possible explanation.</p> <p>Here we provide another observation. In <em>Analyzing &amp; Reducing Learning Rate Warmup</em> (<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/059445c2d5b3ef918079851628fef1d6-Paper-Conference.pdf">paper</a>), the authors found that without warmup, training descends faster initially but becomes slightly <em>sticky</em> around loss ≈ 7.5, after which it becomes slower than training with warmup:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 30%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/unigram-toy-1/sticky_7d5-480.webp 480w,/assets/img/blogs/unigram-toy-1/sticky_7d5-800.webp 800w,/assets/img/blogs/unigram-toy-1/sticky_7d5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/unigram-toy-1/sticky_7d5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>How should we interpret this value 7.5? Using our Unigram model, if we take \(V = 50304\) (NanoGPT) and \(\alpha = 1\) (Zipfian), we can compute the entropy to be 7.57. This may not be a coincidence.</p> <p>With a relatively large initial learning rate, the model may overly exploit Unigram token frequencies to reduce loss quickly, but because the steps are too large, it fails to capture more subtle structures in the data (Bigram, Trigram, or longer-range correlations). As a result, the model is attracted to a Unigram “saddle-point solution,” and escaping from it requires more time. Of course, this is only a hypothesis, and more complex toy models are needed to test it.</p> <p>Finally, from the perspective of neuron activations, there is another possible explanation: excessively large early learning rates may cause neurons to die more quickly. We discussed this possibility in a previous <a href="(/blog/2025/feature-learning-1/)">blog post</a>.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1aVARfgLyf13arPbXw2tODPWacCgWMhBi?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026fine-unigram-toy-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Unigram toy model is surprisingly rich -- representation collapse, scaling laws, learning rate schedule}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/unigram-toy-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Fine-tuning with sparse updates? A toy teacher-student Setup</title><link href="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/" rel="alternate" type="text/html" title="Fine-tuning with sparse updates? A toy teacher-student Setup"/><published>2026-01-07T00:00:00+00:00</published><updated>2026-01-07T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/fine-tuning-sparsity/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>When we fine-tune a model, do we really need to update <strong>all</strong> layers?<br/> For language models, my mental picture is that the model can be divided along depth into three parts:</p> <ul> <li>early layers: mapping <strong>tokens → latent space</strong></li> <li>middle layers: performing <strong>reasoning in latent space</strong></li> <li>late layers: mapping <strong>latent space → tokens</strong></li> </ul> <p>As long as a task is still expressed in natural language, the mapping between language space and latent space should be largely fixed. Therefore, during fine-tuning, perhaps it is sufficient to fine-tune only the <strong>middle layers</strong>. How can we test this hypothesis?</p> <p>There are three possible approaches:</p> <ul> <li> <p><strong>Method 1</strong>: Fine-tune only the middle layers and check whether it works.<br/> However, to judge effectiveness, we need baselines (e.g., only fine-tuning early layers, or only fine-tuning late layers). The number of baselines grows exponentially, since each layer can either be frozen or fine-tuned.</p> </li> <li> <p><strong>Method 2</strong>: Train normally and examine how the magnitude of parameter updates varies across layers.<br/> The problem is that update magnitudes are highly influenced by the optimizer. For example, even if some layers have very small gradients (and arguably do not need to change), adaptive optimizers may still update them.</p> </li> <li> <p><strong>Method 3</strong>: Train normally, but impose sparsity on the updates (by adding L1 regularization), and then examine how update magnitudes vary across layers.<br/> This approach may avoid the issue in Method 2.</p> </li> </ul> <p>In this article, we use a <strong>teacher–student model</strong> to explore Methods 2 and 3. We find that Method 2 indeed suffers from the suspected issue, while Method 3 can effectively resolve it.</p> <hr/> <h2 id="teacherstudent-setup">Teacher–student setup</h2> <p>We adopt a teacher–student setup: the <strong>Teacher Network</strong> and the <strong>Student Network</strong> are MLPs with identical architectures.<br/> The teacher network is randomly initialized and generates input–output pairs. The student network is trained in a supervised manner to minimize the MSE loss of output predictions.</p> <p>Some layers of the student network are copied from the teacher network, while the remaining layers are randomly initialized. Motivated by the discussion above, we consider a <strong>three-layer MLP</strong>. At initialization:</p> <ul> <li>the first and third layers of the student network are identical to those of the teacher network,</li> <li>the second layer is randomly initialized (and thus different from the teacher network).</li> </ul> <p>During training, we additionally compute the <strong>L1 distance</strong> between the student network at training time and its initialization, and use this as a regularization term to encourage sparse updates. The strength of this regularization is denoted by \(\lambda\).</p> <p>The student network can be interpreted as a <strong>pretrained model</strong>, while the teacher network serves as a <strong>fine-tuned data generator</strong>. We ask whether the student network can “realize” that it actually does <strong>not</strong> need to update the first and third layers.</p> <p>We define two sets of observables to characterize the training dynamics:</p> <ul> <li>the distance between the student network during training and its initialization,</li> <li>the distance between the student network during training and the teacher network.</li> </ul> <p>Specifically, we compute the L1 distances for \(W_1, W_2, W_3, b_1, b_2, b_3\).</p> <hr/> <h3 id="normal-training-lambda--0">Normal training (\(\lambda = 0\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the update magnitude of \(W_2\) is actually <strong>smaller</strong> than that of \(W_1\) and \(W_3\). This shows that Method 2 is unreliable.</p> <p><strong>Additional observation.</strong> Each spike in the loss curve corresponds to a “step” in the weights. This phenomenon may be related to the <a href="https://arxiv.org/abs/2206.04817">slingshot mechanism</a>.</p> <hr/> <h3 id="sparse-updates-lambda--0001">Sparse updates (\(\lambda = 0.001\))</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 70%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/fine-tune-sparsity/lambda_0d001-480.webp 480w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-800.webp 800w,/assets/img/blogs/fine-tune-sparsity/lambda_0d001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/fine-tune-sparsity/lambda_0d001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that only \(W_2\) has a significantly non-zero update, while the updates of all other weights eventually approach zero (although they may move substantially at the beginning, which is likely an artifact of Adam). This indicates that <strong>sparse updates can reveal the intrinsic structure of the data</strong>—namely, that only the middle layer needs to be fine-tuned.</p> <p><strong>Additional observation.</strong> The distance between the trained student network and the teacher network for \(W_2\) does not converge to zero (and is in fact quite large). This may be because \(W_2\), acting on sparse inputs, is effectively low-rank, so the optimal \(W_2\) is not unique.</p> <hr/> <h2 id="questions--ideas">Questions / Ideas</h2> <ul> <li> <p>Fine-tuning with sparse updates can serve as an <strong>interpretability tool</strong>—revealing layer-wise similarity between two datasets.<br/> Given a model pretrained on dataset A and fine-tuned on dataset B with sparse updates, it would be interesting to see which layers undergo large updates and which remain nearly unchanged.</p> </li> <li> <p>L1 regularization could be replaced by other metrics (e.g., the <strong>nuclear norm</strong> to encourage low-rank structure, to accompany LORA).</p> </li> <li> <p>Can this method be scaled up, and if so, how?</p> </li> <li> <p>Could this approach help with <strong>continual learning</strong>?<br/> Sparse updates may mitigate catastrophic forgetting.</p> </li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1NsviyXmeuCppxr1wuL53Q9PzK_5BQWnB?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026fine-tuning-sparsity</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Fine-tuning with sparse updates? A toy teacher-student Setup}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/fine-tuning-sparsity/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Multi-Head Cross Entropy Loss</title><link href="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/" rel="alternate" type="text/html" title="Multi-Head Cross Entropy Loss"/><published>2026-01-06T00:00:00+00:00</published><updated>2026-01-06T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/multi-head-cross-entropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In LLMs, next-token prediction is performed by passing the final-layer hidden representation through the LM head to produce logits, thereby projecting into the token space. The definition of cross-entropy implies that only the correct token receives a low loss, while <em>any</em> other token incurs a high loss. For example, if the correct token is “red,” predicting either “green” or “rabbit” results in a large loss. However, it is clear that <em>green</em> is much closer to <em>red</em> than <em>rabbit</em> is, since both are adjectives and both denote colors.</p> <p>This motivates the following question: when designing the loss, should we take <em>similarity between tokens</em> into account? If token A and token B are similar in some respects, then if the model mistakenly predicts token B instead of token A, should it really be penalized as heavily as predicting a completely unrelated token C?</p> <p>This idea is still quite abstract, and there are likely many concrete ways to implement it. In this post, we explore one possibility: <strong>multi-head cross-entropy loss</strong>. Inspired by multi-head attention—where different heads attend to different semantic aspects—multi-head cross-entropy aims to capture token similarity from multiple semantic perspectives.</p> <hr/> <h2 id="definition">Definition</h2> <p><strong>Standard Cross-Entropy</strong></p> <p>Let the input representation to the LM head be \(x \in \mathbb{R}^D\) and the LM head weights be \(W \in \mathbb{R}^{V \times D}.\) The LM head output (i.e., the logits) is \(y = W x \in \mathbb{R}^V,\) where \(V\) is the vocabulary size. Standard cross-entropy computes the loss between $$y$ and the ground-truth label.</p> <p><strong>Multi-Head Cross-Entropy</strong></p> <p>We split \(x\) into \(H\) heads: \(x = [x_1; x_2; \cdots; x_H], x_i \in \mathbb{R}^{D/H}\). Similarly, we split \(W\) into \(H\) parts: \(W_i \equiv W[:,i\frac{D}{H}:(i+1)\frac{D}{H}] \in \mathbb{R}^{V\times D/H}.\) The logits for the \(i\)-th head are \(y_i = W_i x_i \in \mathbb{R}^V.\) After applying Softmax, we define a probability distribution \(p_{i,j} = \frac{\exp(y_{i,j})}{\sum_{j=1}^{V} \exp(y_{i,j})},\) which represents the probability of token \(j\) under the \(i\)-th semantic head.</p> <p>How should we aggregate different heads? Here we simply sum the probabilities (without a rigorous justification, and many other choices are possible): \(p_j = \sum_{i=1}^{H} p_{i,j}.\) The corresponding aggregated logit is \(y_j = \log(p_j).\)</p> <hr/> <h2 id="example-toy-language">Example: Toy Language</h2> <p><strong>Dataset</strong></p> <p>We assume each token has two features:</p> <ul> <li><strong>Part of speech</strong>: noun or verb</li> <li><strong>Topic</strong>: mathematics or sports</li> </ul> <p><strong>Grammar:</strong> nouns and verbs alternate: \(\text{noun} \rightarrow \text{verb} \rightarrow \text{noun} \rightarrow \text{verb} \rightarrow \cdots\)</p> <p><strong>Topic:</strong> within a sentence, the topic is consistent—either mathematics or sports.</p> <p>Thus, we have four classes of tokens:</p> <ul> <li><strong>A</strong>: math nouns</li> <li><strong>B</strong>: math verbs</li> <li><strong>C</strong>: sports nouns</li> <li><strong>D</strong>: sports verbs</li> </ul> <p>Each class contains 10 tokens (randomly chosen). Valid sentences look like:</p> \[[A8] \rightarrow [B2] \rightarrow [A5] \rightarrow [B6] \rightarrow [A1] \rightarrow [B7] \rightarrow \cdots\] \[[C2] \rightarrow [D1] \rightarrow [C6] \rightarrow [D2] \rightarrow [C10] \rightarrow [D3] \rightarrow \cdots\] <p><strong>Model</strong></p> <p>We consider a <strong>bi-gram model</strong>, which predicts the next token based on the previous token. The model uses an MLP, with weight tying between the embedding layer and the LM head.</p> <hr/> <h2 id="results">Results</h2> <p>We perform PCA on the embedding space and project each token embedding onto PC1/PC2. We find that as the number of heads increases:</p> <ol> <li>Clustering improves.</li> <li>The explained variance increases (i.e., the representation becomes more low-dimensional).</li> </ol> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/multi-head-cross-entropy/toy-language-480.webp 480w,/assets/img/blogs/multi-head-cross-entropy/toy-language-800.webp 800w,/assets/img/blogs/multi-head-cross-entropy/toy-language-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/multi-head-cross-entropy/toy-language.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="grokking">Grokking</h2> <p>I also randomly tried a grokking (modular addition) setup and found that multi-head cross-entropy does not significantly accelerate grokking. This was not particularly surprising—I did not have a strong prior for why it should help. This result is reasonable because, in modular addition, tokens essentially have only a single semantic dimension (numerical value), so there is no meaningful notion of multiple semantics for different heads to exploit.</p> <hr/> <h2 id="questions">Questions</h2> <ul> <li>Can we design a more suitable toy dataset that enables more mechanistic interpretability?</li> <li>Does it make sense to apply multi-head cross-entropy loss to large language models?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1QviExbkM6yCz_-T7UfSmCVL89ZGy7Gj_?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026multi-head-cross-entropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Multi-Head Cross Entropy Loss}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/multi-head-cross-entropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI,"/><category term="New-Model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">What’s the difference – (physics of) AI, physics, math and interpretability</title><link href="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/" rel="alternate" type="text/html" title="What’s the difference – (physics of) AI, physics, math and interpretability"/><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/ai-physics-interpretability/"><![CDATA[<p>In a previous <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>, I discussed how to conduct <em>Physics of AI</em> research. Some friends later asked: <strong>Can AI and physics really be fully analogous?</strong> And <strong>what is the difference between Physics of AI and interpretability?</strong></p> <p>This article will discuss two points:</p> <ol> <li><strong>AI and physics are not fully analogous</strong>, but that does not prevent us from borrowing methodologies from physics. In fact, <em>Physics of AI</em> is technically a <strong>simpler game than physics</strong>. Its main obstacles lie in <strong>publication culture</strong> (as discussed in a previous <a href="/blog/2026/physics-of-ai/">blog post</a>), not in the intrinsic difficulty of the subject.</li> <li><strong>(As I define) Physics of AI is not the same as (what people usually define) interpretability</strong>. The key reason is that I see some genuinely new research perspectives that deserve a new name. What that name is does not really matter. If, in the future, the community expands the definition of interpretability, I would also be happy to simply call it interpretability.</li> </ol> <hr/> <p>First, what’s the difference between physics and (physics of) AI?</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-480.webp 480w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-800.webp 800w,/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/ai-physics-interpretability/physics-vs-physics-of-ai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="physics">Physics</h2> <p>If we count from the time of Newton, physics has taken about <strong>400 years</strong> to develop to its current state. Compared with the development of AI, this is extremely slow. This slowness comes from both <strong>practical constraints</strong> and <strong>philosophical constraints</strong>.</p> <p><strong>Practical constraints — physics research has strong directionality</strong></p> <ul> <li><strong>Experiment-driven</strong>: theories emerge from experimental observations.</li> <li><strong>“Human-centered” scales</strong>: starting from scales directly accessible to human perception, then extending in both directions—toward the microscopic (atoms, quarks) and the macroscopic (celestial bodies, the universe).</li> </ul> <p>Both directions are constrained by experimental and observational bottlenecks. It takes a long time to build microscopes and telescopes in the physical world.</p> <p><strong>Philosophical constraints</strong></p> <p>We do not actually know how the “creator” (physical laws) truly runs the universe. We can only infer laws from phenomena.<br/> <em>All models are wrong, but some are useful.</em></p> <p>Even the most committed reductionists are troubled by two questions:</p> <ul> <li><strong>Minimum and maximum scales</strong>: Are fundamental particles truly fundamental, e.g., can quarks or electrons be further divided? What lies beyond the universe?</li> <li><strong>Cross-scale emergence</strong>: How do phenomena at different levels “emerge” from one another?</li> </ul> <hr/> <h2 id="physics-of-ai">(Physics of) AI</h2> <p>For AI, <strong>none of the above constraints really exist</strong>.</p> <p><strong>No practical constraints</strong></p> <p>Apart from computational limits that prevent us from reaching arbitrarily large scales, we have <strong>no observational limitations</strong>. In principle, we can study the evolution of <strong>all weights and all neurons</strong>.</p> <p><strong>No philosophical constraints</strong></p> <p>We fully understand the “fundamental particles” of AI (neurons, weights, gradient descent), and we know the “boundary of the universe” (the entire neural network). <strong>We are the creator.</strong></p> <p>In principle, we can observe phenomena at any level at any time. There is no inherent directionality and no “human-centered” scale. AI systems are also <strong>closed systems</strong>—we know exactly how they evolve because we train them ourselves. Therefore, in principle, it must always be possible to explain large-scale phenomena using small-scale mechanisms, even if such explanations are not always useful.</p> <p>Physics is different: the continual discovery of “new physics” shows that its boundaries are still being broken.</p> <hr/> <h2 id="mathematics">Mathematics</h2> <p>At this point, it is tempting to equate the physics of AI with mathematics, since both have clearly defined “fundamental particles”—called <strong>axioms</strong> in mathematics. But mathematics is clearly more difficult, for several reasons:</p> <ul> <li><strong>Mathematics is also largely directional</strong>: starting from axioms and deriving results is a process from “microscopic” to “macroscopic.” In contrast, AI phenomenology can be studied simultaneously at multiple levels.</li> <li><strong>Symbolic spaces make the definition of scales/levels and observations difficult</strong>: although backward reasoning (induction, abduction) exists in mathematics, it is often hard to define what “intermediate reasoning” even means, because symbolic spaces can be infinite. Neural networks (current AI systems), by contrast, have bounded topologies — no smaller than a neuron and no larger than the entire network.</li> <li><strong>Cultural emphasis on rigor</strong>: mathematical rigor often comes at the cost of faithfulness to reality. AI research, in contrast, emphasizes practicality and is far less obsessed with rigor. Physics lies somewhere in between mathematics and AI in terms of rigor.</li> </ul> <hr/> <h2 id="does-the-physics-of-ai-have-no-difficulties-then">Does the Physics of AI Have No Difficulties, Then?</h2> <p>Of course not.</p> <p>Philosophically, the physics of AI does not suffer from reductionism problems (we know the minimum and maximum scales, and cross-scale explanations are possible in principle). However, it may still face <strong>technical challenges</strong>. Even so, I remain optimistic. Two major technical questions are:</p> <ul> <li><strong>How do we define levels and corresponding observations?</strong> Neurons, weights, and representations are clear, but how should we define circuits or modules? Once levels are defined, what observables should we introduce, and what phenomena should we observe? I gave partial answers in this <a href="/blog/2026/physics-of-ai-recipe/">blog post</a>.</li> <li><strong>How do we characterize the connections (emergence) between levels?</strong> In principle, because AI systems are closed, lower-level phenomena should always be able to explain higher-level ones. In practice, however, finding explanations that are <em>simple</em> and <em>useful</em> is still a non-trivial problem. To be honest, I still have no clue how to answer this question. But more hands-on experiments will give the answer.</li> </ul> <hr/> <h2 id="response-to-critiques-from-the-scaling-camp">Response to Critiques from the Scaling Camp</h2> <p>The issue of emergence is the main line of attack from the <strong>scaling camp</strong> against the <strong>research camp</strong>:<br/> <em>How can phenomena observed in small models transfer to large models?</em></p> <p>Below is my response/attitude/belief:</p> <p><strong>Philosophical level</strong></p> <ul> <li>In natural science, analogies of emergence do not straightforwardly apply here (as discussed above). The ineffectiveness of reductionism in explaining emergence in nature (e.g., <a href="https://arxiv.org/abs/2503.01800">Hilbert’s sixth problem</a> is a hard problem) does not imply the same ineffectiveness in AI. Moreover, I personally do not like the word “emergence” since it seems to imply something mysterious (which might be appropriate for natural science, but not AI).</li> <li>Believe in <strong>shared explanations</strong>. Phenomenon A in small models and phenomenon B in large models may originate from the same cause. The absence of A in large models does not invalidate the value of studying it.</li> </ul> <p><strong>Methodological level</strong></p> <ul> <li><strong>Be pragmatic</strong>. There are many concrete things we can do that have not yet been done. Only by doing them can we know whether they are useful.</li> <li>Be specific. Cross-scale phenomena usually fall into three categories: <ul> <li><strong>Expansion</strong>: phenomenon A in small models becomes more pronounced in large models.</li> <li><strong>Shrinkage</strong>: phenomenon A in small models becomes less visible in large models.</li> <li><strong>Transformation</strong>: phenomenon A in small models turns into phenomenon B in large models.</li> </ul> <p>The scaling critique focuses on <em>shrinkage</em>. We are betting on <em>expansion</em> and <em>transformation</em>. Even if everything turns out to be shrinkage, our efforts are still not meaningless—at that point, I would more firmly side with the scaling camp.</p> </li> <li>We must acknowledge that academia cannot afford to train the largest models. We should call on large-model companies to open-source models—or at least open-source <em>phenomena</em>. Of course, academia must first identify interesting observables in toy and small models, so that industry knows <em>what</em> to observe.</li> </ul> <hr/> <h2 id="interpretability">Interpretability</h2> <p>In a broad sense, interpretability includes everything—one could even say that <em>physics itself is about interpreting the universe</em>. In that sense, the physics of AI certainly belongs to interpretability.</p> <p>However, what people usually mean by interpretability refers narrowly to <strong>interpretability research in AI</strong>, and their understanding is constrained by past work:</p> <ul> <li>Some believe interpretability is just storytelling—pleasant but useless.</li> <li>Others believe interpretability is mathematics—rigorous but useless.</li> <li>The characterizations are often too coarse, making it unclear whether we are studying causality or correlation. This is precisely what <em>mechanistic interpretability</em> seeks to break away from.</li> <li>There is a lack of methodology: <em>at what level does an explanation count as complete?</em></li> </ul> <p>Physics of AI differs from interpretability in all aspects:</p> <ul> <li>Starts with <strong>phenomenology</strong>, emphasizing faithful recording of phenomena and reducing the “storytelling” aspect (see previous <a href="/blog/2025/physics-of-ai/">blog post</a>).</li> <li>Is <strong>not mathematics</strong> (as discussed above). But we can adopt mathematical tools when they are useful.</li> <li>Characterizes phenomena at <strong>multiple levels</strong>, although I personally perfer startting from smallest “toy” models. You might have noticed that my recent technical blogs all study very simple models. The toy models already demonstrate very rich phenomena and I believe that the phenomena in toy models can transform (although maybe not directly transfer) to phenomena in larger models.</li> <li>Gradually builds a <strong>methodology</strong> to describe the connections between phenomena across levels.</li> </ul> <p>Therefore, we deserve a <strong>new name</strong> for what we are doing. <em>Physics of AI</em> is the name I chose.<br/> Again, the name itself is not important. What matters is that it signals <strong>new territory and new treasures</strong>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025physics-of-ai-recipe</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{What's the difference -- (physics of) AI, physics, math and interpretability}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/ai-physics-interpretability/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[In a previous blog post, I discussed how to conduct Physics of AI research. Some friends later asked: Can AI and physics really be fully analogous? And what is the difference between Physics of AI and interpretability?]]></summary></entry><entry><title type="html">Representation anisotropy from nonlinear functions</title><link href="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/" rel="alternate" type="text/html" title="Representation anisotropy from nonlinear functions"/><published>2026-01-04T00:00:00+00:00</published><updated>2026-01-04T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/activation-anisotropy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/activation-anisotropy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Neural representations in deep neural networks are usually anisotrophic. This article aims to understand how nonlinear activation functions lead to representation anisotropy.</p> <hr/> <h2 id="toy-model">Toy model</h2> <p>We consider an LNL (linear-nonlinear) model, which is basically a two-layer MLP excluding the down projection layer: \(h_{\rm pre} = Wx, h_{\rm post} = \sigma(h_{\rm pre})\) where \(\sigma(\cdot)\) is the activation function. We set \(x\in\mathbb{R}^d\) to be isotropic – \(N\) samples are drawn from standard Gaussian distribution. \(W\in\mathbb{R}^{d\times D}\) is randomly initialized. We set \(d=100, D=400, N=10000\). We won’t train the model, and will only be interested in characterizing the anisotropy of \(h_{\rm post}\). We can stack \(h_{\rm post}\) of all \(N\) samples into a matrix \(H\in\mathbb{R}^{N\times D}\).</p> <p>To measure anisotropy, we apply singular value decomposition (SVD) to \(H\) to obtain singular values. The singular value distribution characterizes anisotropy of representations. We normalize singular values so that they sum up to 1.</p> <hr/> <h2 id="observation-1-relu-activation-leads-to-massive-sigma_1-while-linear-activation-does-not">Observation 1: ReLU activation leads to massive \(\sigma_1\), while linear activation does not.</h2> <p>ReLU function make \(\sigma_1\) stand out, but linear function does not:</p> <p>ReLU and linear:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/relu-linear-480.webp 480w,/assets/img/blogs/activation-anisotropy/relu-linear-800.webp 800w,/assets/img/blogs/activation-anisotropy/relu-linear-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/relu-linear.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SiLU is qualitatively similar to ReLU:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 45%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/silu-480.webp 480w,/assets/img/blogs/activation-anisotropy/silu-800.webp 800w,/assets/img/blogs/activation-anisotropy/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This begs the question: why? Before answering why, we first do an interpolation experiment – leaky relu interpolates between ReLU (\(p=0\)) and linear (\(p=1\)). We will use the ratio \(\sigma_1/\sigma_2\) to measure how “standing out’’ \(\sigma_1\) is.</p> <hr/> <h2 id="observation-2-first-order-phase-transition-of-leaky-relu">Observation 2: First order phase transition of Leaky ReLU</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/leaky_relu-480.webp 480w,/assets/img/blogs/activation-anisotropy/leaky_relu-800.webp 800w,/assets/img/blogs/activation-anisotropy/leaky_relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/leaky_relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Although I expect that \(\sigma_1/\sigma_2\) would smoothly interpolate between two extremes, there is a first order phase transition around \(p_c\approx 0.8\). For \(p&gt;p_c\), the ratio remains close to 1. For \(p&lt;p_c\), the ratio grows exponentially (the y axis is log-scale) as \(p\) decreases. I don’t understand why.</p> <hr/> <h2 id="explanation-relu-polarizes-activations">Explanation: ReLU polarizes activations</h2> <p>The intuition: because ReLU maps negative values to zero, this creates a notion of polarity. Indeed, we find the first eigenvector to be \([1, 1, 1, \cdots, 1]\).</p> <p>To better see this, we even drop the linear matrix and only keep the nonlinearity. For isotropic pre-activations, the spectrum for the post-activations look like this:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/isotropic-preact-480.webp 480w,/assets/img/blogs/activation-anisotropy/isotropic-preact-800.webp 800w,/assets/img/blogs/activation-anisotropy/isotropic-preact-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/isotropic-preact.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where \(\sigma_1\) stands out, while all rest singular values are small and almost the same.</p> <p>In hindsight, this explantion is almost trivial, but the consequence (large \(\sigma_1\)) is something I don’t think I had appreciated enough.</p> <hr/> <h2 id="observation-3-tanh-doesnt-make-sigma_1-stand-out">Observation 3: Tanh doesn’t make \(\sigma_1\) stand out</h2> <p>Based on the above explantion, non-polarized activation functions (like Tanh) don’t make \(\sigma_1\) stand out, which is indeed the case:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/activation-anisotropy/tanh-480.webp 480w,/assets/img/blogs/activation-anisotropy/tanh-800.webp 800w,/assets/img/blogs/activation-anisotropy/tanh-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/activation-anisotropy/tanh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="question">Question</h2> <ul> <li>Is this a feature or a bug? Right now I tend to think this is a bug. Anisotropy could be the reason for training inefficiency.</li> <li>If this is a bug, how can we avoid this? Can we simply try to find better activation functions, or do we need extra processing?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1k-vibPZIgB9e5--i87_zG5IfLDH_elLQ?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026activation-anisotropy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Representation anisotropy from nonlinear functions}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/activation-anisotropy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Training dynamics of A Single ReLU Neuron</title><link href="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/" rel="alternate" type="text/html" title="Training dynamics of A Single ReLU Neuron"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/single-relu-neuron</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/single-relu-neuron/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a previous <a href="/blog/2026/feature-learning-1/">blog post</a>, we studied feature learning in shallow, wide MLPs. In this article, we consider an even simpler setting: an MLP with only <strong>one hidden layer</strong>, <strong>a single ReLU neuron</strong>, and a <strong>self-generated target function</strong> (a teacher network).</p> <p>In this setting, we know that there exists a set of weights for which the loss is exactly zero. But will gradient-based optimization run into difficulties? For example, if the neuron is initialized to be overly <strong>active</strong> (positive pre-activation for all inputs), or overly <strong>inactive</strong> (negative pre-activation for all inputs), will optimization fail? Even if the initialization is reasonable, can the training dynamics drive the neuron into a bad state (too active or too inactive)?</p> <p>For convenience, we define three states of a neuron:</p> <ul> <li><strong>Hyperactive</strong>: activated for all inputs (pre-activation always positive)</li> <li><strong>Inactive / Dead</strong>: never activated (pre-activation always negative)</li> <li><strong>Balanced</strong>: activated for some inputs and inactive for others</li> </ul> <p>Although this setup is extremely simple, our ultimate goal is to gain insights relevant to LLM training. One intuition is that many tricks used in LLM training—such as LR warmup, LR decay, weight decay, and MoE balancing—may implicitly control neuron activity levels, thereby influencing feature learning.</p> <hr/> <h2 id="problem-setup">Problem Setup</h2> <p>An MLP with one hidden layer and one neuron can be written as (for simplicity, the input is also 1D):</p> \[f(x; w_1, b_1, w_2, b_2) = w_2\sigma(w_1x+b_1)+b_2, \quad \sigma(x) ={\rm ReLU}(x) \equiv {\rm max}(0,x)\] <p>For the teacher network, we set \(w_1^T = w_2^T = 1, \quad b_1^T = b_2^T = 0\), and so \(f(x)\equiv {\rm ReLU}(x)\). We take the input domain to be \(x \in [-1,1].\)</p> <p>For the student network, we initialize \(w_1^S = w_2^S = 1,\) and focus on varying the initializations of \(b_1^S\) and \(b_2^S\). Training uses MSE loss and the Adam optimizer (default LR = 0.01). Below, we only discuss the student’s weights, so we omit the superscript \(S\).</p> <hr/> <h2 id="observation-0-large-b_1-leads-to-local-minima">Observation 0: Large \(|b_1|\) Leads to Local Minima</h2> <p>We fix the initialization \(b_2 = 0\).</p> <ul> <li>When \(b_1 &gt; 1\), the neuron is initialized in the <strong>hyperactive</strong> state, and the loss gets stuck around 0.02 (corresponding to approximating ReLU with linear regression).</li> <li>When \(b_1 &lt; -1\), the neuron is initialized in the <strong>dead</strong> state, and the loss gets stuck around 0.1 (corresponding to approximating ReLU with a constant function).</li> </ul> <p>This may help explain why some initialization schemes (e.g., Kaiming initialization) set the bias to zero.</p> <hr/> <h2 id="observation-1-large-b_2-can-kill-the-neuron">Observation 1: Large \(|b_2|\) Can Kill the Neuron</h2> <p>We fix \(b_1 = 0\), so the neuron is <strong>balanced</strong> at initialization. Is everything fine then? Not quite. We find that when \(b_2 = -1.1\), the loss can go to zero, but when \(b_2 = -1.2\), the loss gets stuck around 0.02 (again corresponding to approximating ReLU with linear regression).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/loss-480.webp 480w,/assets/img/blogs/single-relu-neuron/loss-800.webp 800w,/assets/img/blogs/single-relu-neuron/loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By closely examining the training dynamics, we find that the turning point \(x_t \equiv - b_1 / w_1\) moves left during training (starting from 0). When it moves past -1, the neuron transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>The local minimum for \(b_2 = -1.2\) corresponds to the linear regression solution (the neuron is in the hyperactive state):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/large_b2_init-480.webp 480w,/assets/img/blogs/single-relu-neuron/large_b2_init-800.webp 800w,/assets/img/blogs/single-relu-neuron/large_b2_init-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/large_b2_init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Conversely, if we increase the bias of the target function, the neuron also transitions from <strong>balanced</strong> to <strong>hyperactive</strong>.</p> <p>This observation connects to two empirical practices:</p> <ul> <li><strong>Model bias</strong>: In some LLM training setups, bias terms are disabled. The observation above suggests that bias dynamics can drive neurons away from the balanced state.</li> <li><strong>Data bias</strong>: Whitening inputs and normalizing intermediate representations are common practices.</li> </ul> <hr/> <h2 id="observation-2-too-large-a-learning-rate-can-also-kill-the-neuron">Observation 2: Too Large a Learning Rate Can Also Kill the Neuron</h2> <p>We fix \(b_1 = 0\) and \(b_2 = -1\).</p> <ul> <li>When LR = 0.2, the model’s loss can be optimized to zero.</li> <li>When LR = 0.4, the loss only reaches 0.1 (again corresponding to fitting ReLU with a constant function), because the neuron becomes <strong>dead</strong>.</li> </ul> <p>This may be related to LR warmup in LLM training: if the initial learning rate is too large, the loss may decrease quickly, but at the cost of killing some neurons. Once dead, these neurons are hard (or impossible) to bring back to a balanced state.</p> <hr/> <h2 id="observation-3-silu-eventually-recovers-but-gets-stuck-at-a-saddle-point-for-a-long-time">Observation 3: SiLU Eventually Recovers, but Gets Stuck at a Saddle Point for a Long Time</h2> <p>For SiLU, we observe that the loss can eventually reach zero (up to machine precision), but training gets stuck at saddle points for very long time, reducing training efficiency.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/single-relu-neuron/silu-480.webp 480w,/assets/img/blogs/single-relu-neuron/silu-800.webp 800w,/assets/img/blogs/single-relu-neuron/silu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/single-relu-neuron/silu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="takeaway">Takeaway</h2> <p><strong>Irreversibility</strong>: once a neuron leaves the balanced state, it is very hard to return. Bias plays a crucial role. This might be why we have various ugly tricks for LLM (learning rate schedule, weight decay, etc).</p> <hr/> <h2 id="questionsideas">Questions/Ideas</h2> <p><strong>Question 1: How can we better control bias?</strong></p> <ul> <li>Can we analytically compute the bias instead of learning it via gradient descent?</li> <li>Can we use a different parameterization to better control neuron activation? For example, \(wx + b \;\to\; w(x + b'),\) where \(b'\) directly controls neuron activation (assuming the input distribution is known). This may allow us to more directly control neuron activity, e.g., by applying weight decay to \(b'\) or explicitly enforcing balancing.</li> </ul> <p><strong>Question 2: Which neurons have better learning dynamics?</strong></p> <ul> <li>SiLU is better than ReLU. Is there something better, gating?</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1gSrKOVfEtVNTa7ZCUOpqOI1uTXm0SL9s?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026single-relu-neuron</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Training dynamics of A Single ReLU Neuron}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/single-relu-neuron/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>