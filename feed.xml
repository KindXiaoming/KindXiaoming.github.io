<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kindxiaoming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kindxiaoming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-09T20:19:18+00:00</updated><id>https://kindxiaoming.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Memory 1 – How much do linear layers memorize?</title><link href="https://kindxiaoming.github.io/blog/2026/memory-1/" rel="alternate" type="text/html" title="Memory 1 – How much do linear layers memorize?"/><published>2026-02-09T00:00:00+00:00</published><updated>2026-02-09T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/memory-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/memory-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I am deeply inspired by the perspective that a language model (or at least some components of it) may behave like a <strong>memorization machine</strong>. In particular, John Morris and colleagues at Meta attempted to quantify model capacity using random data in the paper <a href="https://arxiv.org/abs/2505.24832">“How much do language models memorize?”</a>. They estimate that GPT-style models have a capacity of roughly <strong>3.6 bits per parameter</strong>.</p> <p>In the spirit of the <em>physics of AI</em>, I want to study neural network memorization in a highly simplified setting and aim to understand the behavior as completely as possible. A natural starting point is the simplest nontrivial model: <strong>a linear layer</strong>.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider a linear layer that maps an input \(x \in \mathbb{R}^n\) to logits \(\ell \in \mathbb{R}^V\). We construct a random-label dataset with \(N\) samples: each input \(x\) is drawn from a standard Gaussian distribution, and each label is a randomly chosen one-hot vector.</p> <p>If the network predicts a sample’s label correctly, we say that the sample is <strong>memorized</strong>. Let \(C(N)\) denote the number of memorized samples. We train the linear layer using the cross-entropy loss and the Adam optimizer.</p> <hr/> <h2 id="finding-1-cn-for-different-n">Finding 1: \(C(N)\) for different \(n\)</h2> <p>We fix \(V = 40\) and sweep the input dimension \(n = 10, 20, 30, 40\), as well as the dataset size \(N = 5, 10, \ldots, 400\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/memory-1/C_N-480.webp 480w,/assets/img/blogs/memory-1/C_N-800.webp 800w,/assets/img/blogs/memory-1/C_N-1400.webp 1400w,/assets/img/blogs/memory-1/C_N-1920.webp 1920w,/assets/img/blogs/memory-1/C_N-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/memory-1/C_N.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that for each \(n\), \(C(N)\) initially grows linearly—corresponding to memorizing all samples—and then saturates. This behavior is also reported in <a href="https://arxiv.org/abs/2505.24832">“How much do language models memorize?”</a>. However, there are two subtle differences:</p> <ul> <li>For input_dim = 10, the “saturation” regime still exhibits a positive slope.</li> <li>For input_dim = 40, performance in the saturation regime is worse than at the peak of the linear-growth regime.</li> </ul> <p>We find that both effects depend on the learning rate. Since we use a fixed learning rate of 0.01 across all input dimensions, this value is <strong>too small</strong> for input_dim = 10 and <strong>too large</strong> for input_dim = 40.</p> <p>Additionally, the effective capacity appears to scale linearly with \(n\): for \(n = 10, 20, 30, 40\), the capacity is roughly \(40, 80, 120, 160\), respectively.</p> <hr/> <h2 id="finding-2-memorization-sharpness-and-jamming">Finding 2: Memorization, sharpness, and jamming</h2> <p>For input_dim = 40, we find that the saturation phase performs slightly worse than the best point in the linear-growth phase. We conjecture that this is due to <strong>gradient conflicts</strong> when the number of samples becomes large. Such conflicts can create narrow “valley directions” in the loss landscape (see, for example, <a href="https://arxiv.org/abs/2410.05192">“Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective”</a>), where smaller learning rates are needed.</p> <p>This intuition is closely related to the <strong>jamming phase transition</strong>: placing a single ball into an empty box is easy, but adding a new ball into an already crowded box requires much more care—analogous to using a smaller learning rate.</p> <p>We define <strong>sharpness</strong> as the largest eigenvalue of the Hessian of the loss function. Indeed, as the number of data points increases, we observe a sharp phase transition near the memorization capacity. Beyond this point, the loss landscape becomes significantly sharper, requiring smaller learning rates for stable optimization.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/memory-1/jamming-480.webp 480w,/assets/img/blogs/memory-1/jamming-800.webp 800w,/assets/img/blogs/memory-1/jamming-1400.webp 1400w,/assets/img/blogs/memory-1/jamming-1920.webp 1920w,/assets/img/blogs/memory-1/jamming-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/memory-1/jamming.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="finding-3-which-samples-are-memorized">Finding 3: Which samples are memorized?</h2> <p>A natural hypothesis is that samples with low loss at initialization are more likely to be memorized. However, this does not appear to be the case: we find no clear correlation between a sample’s initial loss and its final loss.</p> <p>Instead, the model appears to exploit <strong>statistical biases</strong> in the finite dataset. Although the underlying data distribution is random and symmetric in expectation, a finite sample can exhibit incidental biases that the network can learn and exploit. Samples that align with these emergent patterns are more likely to be predicted correctly.</p> <p>As a simple analogy, consider flipping a fair coin 100 times and obtaining 52 heads and 48 tails. To minimize loss (or maximize accuracy), a model would predict “heads,” even if it were initially biased toward predicting “tails.”</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1PwR3YpNHaiIMAtSTq12q63QkEHMysrFq?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026memory-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Memory 1 -- How much do linear layers memorize?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/memory-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Memory"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Transformers don’t learn Newton’s laws? They learn Kepler’s laws!</title><link href="https://kindxiaoming.github.io/blog/2026/kepler-newton/" rel="alternate" type="text/html" title="Transformers don’t learn Newton’s laws? They learn Kepler’s laws!"/><published>2026-02-08T00:00:00+00:00</published><updated>2026-02-08T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/kepler-newton</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/kepler-newton/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <p>The blog explains the philosophy behind our new paper <a href="https://arxiv.org/pdf/2602.06923">“From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers”</a>.</p> <hr/> <p><strong>Must world models know Newtonian physics?</strong></p> <p>The answer is <strong>no</strong>, if we consider humans to be valid “world models.” Not everyone has a physics background or can effortlessly write down Newton’s equations, yet anyone can easily predict the trajectory of a flying ball. The computation performed by the human brain is unlikely to involve explicitly integrating Newtonian equations; instead, it relies on <em>intuitive physics</em>—for example, extrapolating a parabolic trajectory. Although this approach may be less fundamental, it is just as accurate and far more efficient for predicting the motion of a flying ball.</p> <p><strong>The key lesson: world models are not unique.</strong></p> <p>In science, we constantly encounter situations where multiple theories explain the same phenomenon equally well, and can only be distinguished once more experimental data become available under more diverse conditions. For instance, Newtonian kinetic energy, \(E_N = \frac{1}{2}mv^2,\) works extremely well in the classical regime, where velocities are much smaller than the speed of light \(c\). It only breaks down—and is superseded by the relativistic expression \(E_R = \frac{mc^2}{\sqrt{1 - v^2/c^2}}\) —when \(v\) becomes comparable to \(c\). After all, <em>all models are wrong, but some are useful</em>.</p> <p><strong>Transformers can’t learn Newton’s laws? They’re learning Kepler’s laws.</strong></p> <p><a href="https://arxiv.org/abs/2507.06952">Vafa et al.</a> showed that a transformer can predict planetary motion (e.g., Earth orbiting the Sun) remarkably well, even though its internal representations do not explicitly encode gravitational forces. Does this mean transformers are not valid world models? <strong>No.</strong> In fact, our new paper shows that the transformer is learning a <em>Keplerian world model</em>, rather than a <em>Newtonian world model</em>. Note that I swept tokenization details under the rug; see <a href="https://arxiv.org/pdf/2602.06923">our paper</a>. This closely parallels the flying-ball example above:</p> <ul> <li>The <strong>Newtonian model</strong> is force-based and involves simulating differential equations.</li> <li>The <strong>Keplerian model</strong> is geometric, focusing on the continuation of elliptical trajectories.</li> </ul> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/world-newton/kepler-newton-480.webp 480w,/assets/img/blogs/world-newton/kepler-newton-800.webp 800w,/assets/img/blogs/world-newton/kepler-newton-1400.webp 1400w,/assets/img/blogs/world-newton/kepler-newton-1920.webp 1920w,/assets/img/blogs/world-newton/kepler-newton-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/world-newton/kepler-newton.png?18ecb0c94e3c4d57a14937b79988cdd5" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Even more intriguingly, when we replace full-context attention with <em>local attention</em> (i.e., reduce the context length), the transformer begins to behave like a <strong>Newtonian world model</strong>. As the context length is varied, we observe a sharp and fascinating <em>phase transition</em> between these two types of world models.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/world-newton/phase-change-480.webp 480w,/assets/img/blogs/world-newton/phase-change-800.webp 800w,/assets/img/blogs/world-newton/phase-change-1400.webp 1400w,/assets/img/blogs/world-newton/phase-change-1920.webp 1920w,/assets/img/blogs/world-newton/phase-change-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/world-newton/phase-change.png?9a5dc77ba69d5477c1273373fcd8fd0d" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p><strong>Implications for world models</strong></p> <p>When task diversity is limited—in our case, a single task of predicting planetary motion—there may exist many different world models that all perform equally well. We cannot rule out the possibility that, in a parallel universe governed by the same physical laws, humans might develop a fundamentally different physical theory. Perhaps AI or world models have already done so. The real challenge is: <em>how do we extract these internal theories and make sense of them?</em></p> <p>If, however, we insist that AI models should know Newtonian physics, a few promising directions emerge:</p> <ul> <li> <p><strong>(1) Increase task diversity.</strong><br/> A purely geometric approach works well for planetary motion but fails for chaotic dynamics. Newtonian physics, by contrast, applies broadly to all classical systems, including both planetary motion and chaos.</p> </li> <li> <p><strong>(2) Introduce architectural inductive biases.</strong><br/> Transformers should encourage at least some attention heads to operate locally, enabling the construction of differential operators (as in Newton’s second law). At the same time, hard constraints on context length are undesirable for long-term consistency (e.g., in video generation). Semantics are often global in time, while forces are local in time; a good architecture should accommodate both.</p> </li> <li> <p><strong>(3) Differential programming.</strong><br/> For example, the Taichi system developed by <a href="https://yuanming.taichi.graphics/">Yuanming Hu</a>. If the first two directions aim to make neural models more <em>symbolic</em>, differential programming pursues the opposite paradigm: making <em>symbolic systems more neural</em>.</p> </li> </ul> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026world-newton</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Transformers don't learn Newton's laws? They learn Kepler's laws!}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/kepler-newton/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="World-model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">When I say “toy models”, what do I mean?</title><link href="https://kindxiaoming.github.io/blog/2026/toy/" rel="alternate" type="text/html" title="When I say “toy models”, what do I mean?"/><published>2026-02-07T00:00:00+00:00</published><updated>2026-02-07T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/toy</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/toy/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/methodology-toy/toy_teaser-480.webp 480w,/assets/img/blogs/methodology-toy/toy_teaser-800.webp 800w,/assets/img/blogs/methodology-toy/toy_teaser-1400.webp 1400w,/assets/img/blogs/methodology-toy/toy_teaser-1920.webp 1920w,/assets/img/blogs/methodology-toy/toy_teaser-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/methodology-toy/toy_teaser.png?ef7be79166415858a39dd99991395bcf" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="motivation">Motivation</h2> <p>When I talk with colleagues and students, they are often confused by what I call “toy models.” The confusion usually comes from two questions:<br/> (1) What exactly is a toy model, and how does one come up with it?<br/> (2) Toy models sound useless—what are they good for?</p> <p>The second question is essentially a special case of the first: only after you become familiar with a tool do you understand what it is useful for.</p> <p>The goal of this blog is therefore twofold: to clearly explain what a toy model is (for AI researchers), and to provide a <strong>methodology</strong>—how to <em>construct</em> toy models.</p> <hr/> <h2 id="what-does-a-toy-model-mean-in-physics">What does a toy model mean in physics?</h2> <p>The term <em>toy model</em> comes from physics—at least, that is where I first encountered and grew fond of it during my physics training. It precisely captures a physicist’s research aesthetic: striking a delicate balance between <strong>simplicity</strong> and <strong>relevance</strong>.</p> <p>For example, magnetic materials exhibit a critical temperature (the Curie temperature): above it they are paramagnetic, below it ferromagnetic. To understand this phenomenon, Ernest Ising proposed the Ising model. The simplest Ising model, which only considers interactions between neighboring atoms, is already sufficient to produce this phase transition and even predict Curie temperatures for some materials.</p> <p>As another example, although animal shapes vary enormously, scientists have found that heat dissipation and energy consumption depend primarily on an animal’s <em>size</em>, not its detailed shape. For studying heat dissipation, a “spherical cow” (or “spherical chicken”) is therefore an excellent toy model.</p> <p>So what is the philosophy behind toy models in physics?</p> <blockquote> <p><strong>Einstein:</strong> <em>Everything should be made as simple as possible, but not simpler.</em></p> </blockquote> <blockquote> <p><strong>Max Tegmark</strong> (my PhD advisor):<br/> <em>If we don’t understand something, we should resort to a simpler thing that we still don’t understand… until we get to something we can start to understand.</em></p> </blockquote> <p>You might argue that constructing toy models sounds like it requires genius. For instance, how did Ising come up with the Ising model? My argument is that building toy models requires at most <strong>10% talent</strong> and <strong>90% hard work</strong>—provided you have the <em>right methodology</em>. With the right methodology, anyone with 90% effort can outperform a “genius” relying on 10% talent alone. This is precisely the purpose of this article: to teach a methodology for constructing toy models.</p> <p>Before doing so, however, I must emphasize that toy models from physics cannot be transplanted wholesale into AI. We need to <strong>redefine</strong> what a toy model means in AI.</p> <hr/> <h2 id="toy-model-in-ai-model--network--data">Toy model in AI? Model = Network + Data</h2> <p>In AI, <em>models</em> and <em>data</em> must be clearly distinguished.</p> <p>In physics, a toy model is just a <em>model</em> whose purpose is to explain <em>data</em>. Here, “data” refers to physical phenomena or laws, which are fixed in our universe and cannot be arbitrarily controlled.</p> <p>In AI, however, a “toy model” can refer either to the <strong>model (neural network)</strong> or to the <strong>data</strong>—and these two factors can be controlled independently. In particular, we have the superpower of <em>designing and manipulating data</em>, something that physics does not allow. To avoid ambiguity, when I specifically mean a neural network, I will use the word <em>Network</em> rather than <em>Model</em>. In short:</p> <blockquote> <p><strong>Model = Network + Data</strong></p> </blockquote> <p>This gives rise to four regimes: the network can be simple or complex, and the data can be simple or complex. The four regimes correspond to existing research paradigms:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/methodology-toy/four_zone_today-480.webp 480w,/assets/img/blogs/methodology-toy/four_zone_today-800.webp 800w,/assets/img/blogs/methodology-toy/four_zone_today-1400.webp 1400w,/assets/img/blogs/methodology-toy/four_zone_today-1920.webp 1920w,/assets/img/blogs/methodology-toy/four_zone_today-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/methodology-toy/four_zone_today.png?611cf3887938056532ecae7ea3e23805" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Zone 1 — simple data, simple network.</strong><br/> This is where much of ML theory lives. To prove theorems, both the data and the model must satisfy strong simplifying assumptions.</li> <li><strong>Zone 2 — simple data, complex network.</strong><br/> This is the regime of Zeyuan Zhu’s <em>Physics of LLMs</em>: carefully designed, controlled tasks paired with real-world LLMs.</li> <li><strong>Zone 3 — complex data, simple network.</strong><br/> This is the vibe of traditional science. Given complex observed data, scientists propose simple models that can generate it.</li> <li><strong>Zone 4 — complex data, complex network.</strong><br/> This is mainstream modern AI.</li> </ul> <p><strong>Definition of a Toy Model</strong><br/> Anything outside Zone 4—i.e., Zones 1, 2, and 3—belongs to the realm of <em>toy models</em>. Which toy model one should study depends entirely on one’s goal. Here are a few concrete examples:</p> <ul> <li><strong>Zone 1:</strong> I want to study the emergence of sparse attention and gain as much quantitative, possibly analytic, understanding as possible. Both the network and the data should be as simple as possible.<br/> See: <a href="https://arxiv.org/pdf/2505.17863">“The emergence of sparse attention: impact of data distribution and benefits of repetition”</a>.</li> <li><strong>Zone 2:</strong> I want to decompose LLM capabilities by studying their behavior on simpler, more controllable tasks.<br/> See Zeyuan Zhu’s <a href="https://physics.allen-zhu.com/">Physics of LLM</a>.</li> <li><strong>Zone 3:</strong> I want to test whether data possesses a certain structure. For example, if an equivariant network fits the data well, the data likely has that symmetry; if not, it probably doesn’t. This closely resembles hypothesis testing in traditional science.<br/> See: <a href="https://arxiv.org/abs/2109.13901">Physics-augmented learning</a>, <a href="https://arxiv.org/abs/1905.11481">AI Feynman</a>.</li> </ul> <hr/> <h2 id="an-important-mindset-interpolation">An important mindset: interpolation</h2> <p>A common criticism of toy models is:</p> <blockquote> <p>What works in a toy model may not work in a real model—and even if it does, it may work for entirely different reasons.</p> </blockquote> <p>At its core, this criticism argues that toy models and real models feel completely disconnected, making transfer nontrivial.</p> <p><strong>The solution: interpolation.</strong><br/> We should find a <em>path</em> connecting toy models and real models—akin to the notion of <em>homotopy</em> in mathematics.</p> <h3 id="why-interpolate">Why interpolate?</h3> <p><strong>Knowledge is created at the boundary between the known and the unknown.</strong> If we fully understand a toy model and the real model behaves differently, there must be a phase change (sharp or smooth) along the path connecting them. Understanding the phase change is key to understanding the difference between the toy setup and the real setup. Starting from the toy model (known) and gradually moving toward the real model (unknown) mirrors how humans learn. Fully known is trivial; fully unknown is confusing. The sweet spot—half-known, half-unknown—is where information gain is maximized. Research is, at its core, an information-gathering game.</p> <h3 id="how-to-interpolate">How to interpolate?</h3> <ul> <li><strong>Directionality:</strong> move from simple to complex, or from complex to simple.</li> <li><strong>Locality:</strong> change one feature at a time; avoid overly large jumps.</li> </ul> <p>Here are a few (abstracted) examples from projects I’ve supervised:</p> <ul> <li>A complex model shows strange behavior on real data (Zone 4). We hypothesize similar behavior might appear on simpler data. A student constructs a simple dataset (Zone 2), but the behavior differs. After failing to “complexify” the simple data to match reality, we realize the real data itself has a tunable parameter that can simplify it.<br/> <strong>Lesson:</strong> not only can you make simple data more complex—you can also make complex data simpler.</li> <li>A diffusion model exhibits an intriguing phenomenon on CIFAR-10 (Zone 4). A student builds a toy setup (Zone 1): simple 2D data and a simple MLP. The phenomenon appears, but it’s unclear whether the mechanisms match. The toy metric doesn’t transfer. I then emphasize interpolation: network and data complexity can be adjusted independently. For example, keep a complex network (U-Net or DiT) but simplify the data to one prototype image per CIFAR class—only ten images total. This yields complex networks with simple data (Zone 2).<br/> <strong>Lesson:</strong> maintain an interpolation mindset. Simplicity vs. complexity is not binary—it is continuous and multidimensional.</li> </ul> <hr/> <h2 id="the-map-of-my-physics-of-ai">The map of my “physics of AI”</h2> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/methodology-toy/four_zone_physics_of_ai-480.webp 480w,/assets/img/blogs/methodology-toy/four_zone_physics_of_ai-800.webp 800w,/assets/img/blogs/methodology-toy/four_zone_physics_of_ai-1400.webp 1400w,/assets/img/blogs/methodology-toy/four_zone_physics_of_ai-1920.webp 1920w,/assets/img/blogs/methodology-toy/four_zone_physics_of_ai-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/methodology-toy/four_zone_physics_of_ai.png?4bc46a63a6b02dc90cd64c2a100b0b84" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, how does the Zone 1–4 framework guide my <em>Physics of AI</em> program?</p> <p>The ultimate goal is, of course, Zone 4. But I argue that we should start from foundations in Zone 1—while distinguishing this effort from ML theory. ML theory is theory-driven, prioritizing rigor and carefully “feeling the elephant’s trunk.” Physics of AI is experiment- and phenomenon-driven, prioritizing intuition and breadth—touching many sides of the elephant and assembling a coherent picture.</p> <p>I view Physics of AI as progressing through three stages:</p> <ul> <li><strong>Stage 1:</strong> Study training dynamics on simple data and simple models. This may sound trivial, but it isn’t—there is a rich landscape of dynamics and many failure modes, some of which appear connected to failures in LLMs. The goal is to catalog phenomena (e.g., grokking). In physics terms: <em>what are the behaviors of the fundamental particles?</em> In AI, the “particles” (attention, MLPs, etc.) are already given; we only need to characterize them.</li> <li><strong>Stage 2:</strong> Study training dynamics on simple data but complex models. This is actually almost trivial once Stage 1 is done: we combine well-understood modules and add corrections to capture their interactions. At this stage, we begin to build systematic recipes for architectures and optimizers. Physics analogy: <em>what are the interactions between particles?</em></li> <li><strong>Stage 3:</strong> The hardest step—moving from Stage 2 to Stage 3—requires understanding the data itself and therefore draws on domain expertise across fields. Physics analogy: <em>what is the Standard Model of AI?</em></li> </ul> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026methodology-toy</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{When I say "toy models", what do I mean?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/toy/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Methodology"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">On the physical interpretation of drifting generative models</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-3/" rel="alternate" type="text/html" title="On the physical interpretation of drifting generative models"/><published>2026-02-06T00:00:00+00:00</published><updated>2026-02-06T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading with interest the paper <a href="https://arxiv.org/pdf/2602.04770">“Generative Modeling via Drifting”</a>, as its use of attraction and repulsion is closely related to our earlier work on <a href="https://arxiv.org/abs/2209.11178">PFGM</a> and <a href="https://arxiv.org/abs/2302.04265">PFGM++</a>.</p> <p>The goals of this blog are to:</p> <ul> <li>provide a clear <strong>physical interpretation</strong> of drifting generative models;</li> <li>present <strong>visualizations across many settings</strong>, offering guidance on kernel choice and highlighting potential failure modes.</li> </ul> <hr/> <h2 id="physical-interpretation">Physical interpretation</h2> <p>The involvement of neural networks can obscure the intuition, so we first remove them from the story.</p> <p>The physical picture is straightforward:</p> <ul> <li>There are \(N\) positively charged particles (the data), fixed in space.</li> <li>There are \(N\) negatively charged particles, randomly initialized.</li> <li>The negative particles are released and move according to a force field determined by interactions with positive particles (\(V^+\)) and with other negative particles (\(V^-\)). For any pair of particles, the force magnitude is \(f(r)\) (corresponding to their kernel \(K\)). The paper uses \(K(r) = \exp\!\left(-\frac{r}{\tau}\right),\) which corresponds to a Yukawa-type potential. Negative charges are attracted to positive charges and repelled by other negative charges.</li> <li>Intuitively, the final equilibrium should consist of one-to-one positive–negative pairs, which appear neutral to the outside world, yielding a net force \(V = 0\).</li> </ul> <p>Importantly, the evolution of the negative charges is entirely determined by the force field and does <strong>not</strong> require a neural network. The role of the network is simply to <em>learn and approximate</em> this evolution, so that it can generalize to unseen configurations at inference time.</p> <p>In what follows, I focus purely on visualizing the forward dynamics, without training any neural network.</p> <hr/> <h2 id="dependence-on-tau">Dependence on \(\tau\)</h2> <p>The paper uses the force \(f(r) = r \exp\!\left(-\frac{r}{\tau}\right).\) We first illustrate how the dynamics depend on the parameter \(\tau\).</p> <h3 id="tau02">\(\tau=0.2\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_0d2_trajectory.gif?9a473ca8217f776520d7329769907608" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tau10">\(\tau=1.0\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_1_trajectory.gif?c7ea6aedb1c2f1af58f8890f46eba96c" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tau50">\(\tau=5.0\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/r_exp_tau_5_trajectory.gif?bf6e8bf23b45a1e95365fc18b218bf5a" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="other-force-fields">Other force fields</h2> <p>We also explore alternative force laws to understand how sensitive the dynamics are to kernel choice.</p> <h3 id="fr1r01">\(f(r)=1/(r+0.1)\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/f_inv_r_0d1_trajectory.gif?6651e09775c18f1cefb22f09a631063e" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="fr1r1">\(f(r)=1/(r+1)\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/f_inv_r_1_trajectory.gif?b754fb3a54173565248545935084b4a5" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="explaining-balance">Explaining balance</h2> <p>The paper reports that the combination \(V^+ - V^-\) performs best, which is exactly what the physical picture suggests. For alternatives such as \(2V^+ - V^-\) or \(V^+ - 2V^-\), the system is no longer neutral. As a result, the negative particles either collapse toward the center of the positive particles or are driven too far away.</p> <p>We therefore focus on \(f(r) = r \exp(-r),\) and verify this intuition through toy experiments.</p> <h3 id="v---2v-">\(V^+ - 2V^-\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/1v+_2v-_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/1v+_2v-_trajectory.gif?5733d95856d570c1c37310f81d7b59d6" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="2v---v-">\(2V^+ - V^-\)</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 50%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/2v+_1v-_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/2v+_1v-_trajectory.gif?938e8cb9f71b345038ce72423c8513a6" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="more-examples">More examples</h2> <p>Finally, we keep \(f(r) = r \exp(-r)\) fixed and vary the data distribution.</p> <h3 id="circle">Circle</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/circle_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/circle_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/circle_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/circle_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/circle_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/circle_trajectory.gif?787a43cf5c283bd24f6abc177337b706" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="square">Square</h3> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-3/square_trajectory-480.webp 480w,/assets/img/blogs/diffusion-3/square_trajectory-800.webp 800w,/assets/img/blogs/diffusion-3/square_trajectory-1400.webp 1400w,/assets/img/blogs/diffusion-3/square_trajectory-1920.webp 1920w,/assets/img/blogs/diffusion-3/square_trajectory-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-3/square_trajectory.gif?1bf4af9bc03d61198e6801a0836c0f16" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Overall, drifting dynamics appear surprisingly intricate. While these “failure modes” may not necessarily arise in practical settings, further study is needed to understand how kernels should be chosen and what failure modes may emerge.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Kn4sDVouuwZpb1Nhe9bqtwQu2Tj0-Qw9?usp=sharing">here</a>.</p> <p>I want to thank Ruiqi Ni (@RuiqiNi) for finding a bug in my codes – the reduction should be over axis 0 rather than 1, and mean rather than sum.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{On the physical interpretation of drifting generative models}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Diffusion"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics 2 – Transformers fail to maintain physical cosistency for circular motion</title><link href="https://kindxiaoming.github.io/blog/2026/physics-2/" rel="alternate" type="text/html" title="Physics 2 – Transformers fail to maintain physical cosistency for circular motion"/><published>2026-02-05T00:00:00+00:00</published><updated>2026-02-05T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>Most current world models and video generative models suffer from <strong>physical or semantic inconsistency</strong>. While a generated video may appear coherent for a few seconds, it often starts producing inconsistent content over longer horizons—even though the new content may still look locally plausible. The origin of this inconsistency is difficult to pin down, because both the data (natural scenes) and the models are largely black boxes.</p> <p>In this article, we instead study a <strong>toy dataset</strong>—circular motion—which is highly controlled and well understood. By training transformers on this toy dataset, we uncover failure modes in the generated trajectories that closely resemble the inconsistency problems observed in large-scale world models.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider a circular motion dataset defined as follows. The radius is sampled as \(r \sim U[0.15, 0.35]\), and the angle evolves as \(\varphi_t = \omega t + \varphi_0,\) where \(\varphi_0 \sim U[0, 2\pi]\), the angular velocity is \(\omega = 0.5\), and \(t = 1, 2, \ldots, 50\). This corresponds to Cartesian coordinates \(x_t = r \cos \varphi_t, \quad y_t = r \sin \varphi_t.\)</p> <p>We follow the tokenization scheme of <a href="https://arxiv.org/pdf/2507.06952">Vafa et al.</a>. Each coordinate pair \((x, y)\) is discretized into tokens, with \(x\) and \(y\) tokenized independently. Specifically, the \(x\) coordinate is mapped to \(\lfloor N x \rfloor\), and the \(y\) coordinate is mapped to \(\lfloor N y \rfloor + N\). In total, this yields \(2N\) distinct tokens.</p> <p>The transformer takes in and outputs <strong>two tokens per time step</strong> (corresponding to \(x\) and \(y\)), rather than a single token. The training objective is the standard next-token(s) prediction cross-entropy loss.</p> <hr/> <h2 id="results">Results</h2> <p>After training, we condition the model on the first 10 points (shown in red), and then generate the next 90 points autoregressively (shown in blue).</p> <p>We show results for models of different sizes below.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_8_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_8_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_8_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_32_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_32_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_32_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_128_n_layer_1-480.webp 480w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-800.webp 800w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_128_n_layer_1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_128_n_layer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-2/n_embed_128_n_layer_2-480.webp 480w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-800.webp 800w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-1400.webp 1400w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-1920.webp 1920w,/assets/img/blogs/physics-2/n_embed_128_n_layer_2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-2/n_embed_128_n_layer_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <p>When the model is sufficiently expressive, we observe the following behavior:</p> <ul> <li>During the first few generation steps, the object follows the correct circular trajectory.</li> <li>Over longer generation horizons, the object remains approximately on the circle, but begins to jump all over it.</li> </ul> <p>We hypothesize that:</p> <ul> <li>The transformer is able to learn a <strong>conservation law</strong>—in this case, conservation of radius—and infer its value from the context.</li> <li>While the model initially follows the correct motion law (circular motion), it fails to maintain this dynamics over long contexts, leading to trajectory inconsistency.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1YHcqwvAk28M60XXwa9bJM_XXnv4-o77x?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026physics-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics 2 -- Transformers fail to maintain physical cosistency for circular motion}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="AI-for-Physics"/><category term="Physics"/><category term="World-model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Physics 1 – Attention can’t exactly simulate uniform linear motion</title><link href="https://kindxiaoming.github.io/blog/2026/physics-1/" rel="alternate" type="text/html" title="Physics 1 – Attention can’t exactly simulate uniform linear motion"/><published>2026-02-04T00:00:00+00:00</published><updated>2026-02-04T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/physics-1</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/physics-1/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a recent paper, <a href="https://arxiv.org/pdf/2507.06952">“What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models”</a>, the authors show that transformers fail to learn planetary motion governed by Newton’s second law under Newtonian gravity. However, the underlying failure modes are not well understood.</p> <p>As a physicist, I would like to further simplify the setup while preserving the failure mode. In this blog post, we study whether an attention layer can learn <strong>Newton’s first law</strong>—uniform linear motion. Given the current state \(x_t\) and the previous state \(x_{t-1}\), the next state is \(x_{t+1} = 2x_t - x_{t-1}.\) While this relation is trivial for an MLP mapping \((x_{t-1}, x_t) \mapsto x_{t+1}\) (since it is linear), it is not immediately clear whether an attention layer can learn it efficiently or exactly, because:</p> <ul> <li>linear coefficients must be constructed through softmax attention;</li> <li>softmax attention produces positive weights, whereas the coefficients 2 and −1 have opposite signs.</li> </ul> <p>These concerns may not be fatal, but they are suspicious enough to warrant a closer look through toy experiments.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>As Einstein put it, we want a model that is <em>as simple as possible, but not simpler</em>. In this spirit, I start from the simplest possible models, gradually add complexity (sometimes removing it again), and eventually arrive at an architecture that can perform the task reasonably well. I then inspect the weights and representations to understand what kind of algorithm has emerged.</p> <p>The exploration process is summarized below. Readers who are only interested in the final result can safely skip this part.</p> <ul> <li>I start with <code class="language-plaintext highlighter-rouge">input_dim = 3</code> (since Euclidean space is 3D), using a 1-layer, attention-only model with a single head and no positional embeddings. It does not learn.</li> <li>To break the symmetry between positions (coefficients 2 and −1), I add absolute positional embeddings (APE). It still does not learn.</li> <li>I then suspect the model is too narrow, since I tied the embedding dimension to the input dimension (3). I decouple them by adding a projection-up layer from 3D to 20D and a projection-down layer from 20D back to 3D. Now it finally learns.</li> <li>The model now feels overly complex, so I begin simplifying. I reduce <code class="language-plaintext highlighter-rouge">input_dim</code> from 3 to 1. It still learns (not surprising, since the task is easier).</li> <li>I reduce the embedding dimension to 4. It still learns.</li> <li>I reduce the embedding dimension to 2. It fails for one random seed, but succeeds for another.</li> <li>I reduce the embedding dimension to 1. With a lucky random seed (after trying a few), it can learn.</li> <li>With both <code class="language-plaintext highlighter-rouge">input_dim = 1</code> and <code class="language-plaintext highlighter-rouge">embedding_dim = 1</code>, the projection-up and projection-down layers seem unnecessary. However, after removing both, the model fails for all 10 random seeds I tried. If I remove only the projection-down layer but keep projection-up, it can still learn.</li> <li>I notice that when projection-up is learned, its weight is small (around 0.08). This suggests that to remove it entirely, I should manually rescale the input by a factor of 0.1. With this change, the network learns again.</li> <li>Finally, I try removing the bias terms in the attention layer. The network no longer learns. At this point, I stop—the model is already simple enough.</li> </ul> <p>We thus arrive at a model that is as simple as possible while still being able to perform the task \(x_{t+1} = 2x_t - x_{t-1}\) reasonably well. Note that cherry-picking random seeds is not an issue here: the goal is not to demonstrate superior performance (where cherry-picking would be inappropriate), but to gain insight into how simple networks represent and compute.</p> <hr/> <h2 id="a-simple-model-can-numerically-approximate-the-mapping">A simple model can numerically approximate the mapping</h2> <p>In summary, we now have a “transformer” model with:</p> <ul> <li>1D input (and 1D embedding),</li> <li>context length 2 (using \(x_0\) and \(x_1\) to predict \(x_2\)),</li> <li>absolute positional embeddings,</li> <li>a single-head, 1D attention layer,</li> <li>no MLPs.</li> </ul> <p>This model can predict \(x_2\) from \(x_1\) and \(x_0\) reasonably well, achieving an MSE loss of about \(10^{-3}\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/training-480.webp 480w,/assets/img/blogs/physics-1/training-800.webp 800w,/assets/img/blogs/physics-1/training-1400.webp 1400w,/assets/img/blogs/physics-1/training-1920.webp 1920w,/assets/img/blogs/physics-1/training-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The sticky plateau of the loss curve at the beginning of training seems to correspond with the learning of positional embeddings (\(p_0\) and \(p_1\) repulse).</p> <hr/> <h2 id="the-model-does-not-exactly-represent-the-mapping">The model does not exactly represent the mapping</h2> <p>We now want to better understand <em>how</em> the model implements the task. A first hypothesis is the following: when positional embeddings dominate the input (i.e., \(x_0, x_1 \ll p_0, p_1\)), the attention matrix becomes approximately constant—roughly independent of \(x_0\) and \(x_1\). In that case, the network would represent \(f(x) = W_v(\alpha_1 x_1 + \alpha_0 x_0),\) where \(\alpha_1 + \alpha_0 = 1\) and \(\alpha_1, \alpha_0 &gt; 0\) (positivity guaranteed by softmax). However, \(2x_1 - x_0\) does not belong to this family, since the two terms have opposite signs. Therefore, the network must be doing something more subtle—in particular, the attention weights must depend on \(x_0\) and \(x_1\).</p> <p>To investigate this, we visualize how \(\alpha_1\) depends on \(x_0\) and \(x_1\). Empirically, \(\alpha_1\) depends primarily on \(x_1\), and the relationship appears roughly linear.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 40%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/alpha1-480.webp 480w,/assets/img/blogs/physics-1/alpha1-800.webp 800w,/assets/img/blogs/physics-1/alpha1-1400.webp 1400w,/assets/img/blogs/physics-1/alpha1-1920.webp 1920w,/assets/img/blogs/physics-1/alpha1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/alpha1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A linear regression gives \(\alpha_1 \approx -0.006 x_0 + 0.068 x_1 + 0.300 \quad (R^2 = 0.996).\) This indicates that although softmax is nonlinear, it is being used only locally, so its dependence is well approximated by a first-order Taylor expansion.</p> <p>Since attention is the only nonlinear component of the model—and we can approximate it linearly—we can now write down the full computation graph explicitly. We use <em>Mathematica</em> for symbolic computation:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/physics-1/mathematica-480.webp 480w,/assets/img/blogs/physics-1/mathematica-800.webp 800w,/assets/img/blogs/physics-1/mathematica-1400.webp 1400w,/assets/img/blogs/physics-1/mathematica-1920.webp 1920w,/assets/img/blogs/physics-1/mathematica-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/physics-1/mathematica.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Ignoring quadratic terms, we obtain \(-0.088 x_0 + 0.171 x_1 \approx 0.088(-x_0 + 2x_1).\)</p> <hr/> <h2 id="comment">Comment</h2> <ul> <li>Although a 1-layer attention model can approximate \(x_{t+1} = 2x_t - x_{t-1}\), it does so only approximately—by leveraging a local Taylor expansion of the softmax—rather than computing the relation exactly.</li> <li>The linear mapping \(x_{t+1} = 2x_t - x_{t-1}\) has a clear physical interpretation as uniform linear motion. This blog shows that even such simple motion cannot be represented <em>exactly</em> by a 1-layer transformer. While adding depth and MLPs may improve the approximation (e.g., by effectively expanding around more nonlinearities to better cancel higher-order terms), I suspect the computation remains approximate rather than exact. This suggests that in order to build real “world models” or “AI Physicists”, we are gonna need model beyond transformers.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1N66LgKQTze8vv-x4qOWu3i01Hp1pWNv8?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026physics-1</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Physics 1 -- Attention can't exactly simulate uniform linear motion}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/physics-1/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="AI-for-Physics"/><category term="Physics"/><category term="World-model"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 4 – Flat directions (in weight space) are high frequency modes (in function space)</title><link href="https://kindxiaoming.github.io/blog/2026/depth-4/" rel="alternate" type="text/html" title="Depth 4 – Flat directions (in weight space) are high frequency modes (in function space)"/><published>2026-02-03T00:00:00+00:00</published><updated>2026-02-03T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-4</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-4/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>In a recent blog post <a href="/blog/2026/depth-3/">depth-3</a>, we found that the <em>normalized</em> eigen-spectrum aligns across depths. Although this alignment may not hold in general, it naturally raises an important question: <strong>what is the benefit of depth, then?</strong></p> <p>To address this question, it is not sufficient to only examine eigenvalues—we also need to understand the corresponding <strong>eigenvectors</strong>. I can think of two ways to visualize eigenvectors:</p> <ol> <li>Load an eigenvector back into a neural network and visualize the resulting network graph.</li> <li>Study how perturbations along an eigenvector change the network’s output.</li> </ol> <p>In this article, we adopt the second approach. By choosing tasks with <strong>1D input and 1D output</strong>, we can visualize each eigenvector as a <strong>1D curve</strong> that represents how the learned function changes under a weight perturbation along that eigenvector.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We consider MLPs with hidden width 20, and compare <strong>2-layer</strong> and <strong>3-layer</strong> architectures. The task is 1D regression, either \(f(x) = x^2 \quad \text{(smooth)}\) or \(f(x) = \sin(5x) \quad \text{(oscillatory)}.\) We sample \(x \sim U[-1, 1]\). The networks are trained using the MSE loss with the Adam optimizer.</p> <p>After training, we obtain the model parameters \(\theta\). We then compute the Hessian \(H \equiv \frac{\partial^2 \ell}{\partial \theta^2},\) and its eigenvalues \(\lambda_i\) and eigenvectors \(v_i\) satisfying \(H v_i = \lambda_i v_i.\)</p> <p>The trained network represents a function \(f(x; \theta)\). To understand the <em>functional meaning</em> of an eigenvector \(v_i\), we perturb the parameters along this direction and obtain the perturbed function \(f(x; \theta + a v_i),\) where \(a\) is a small scalar. We visualize the induced functional change as \(\delta f_i \equiv \frac{1}{a}\bigl(f(x; \theta + a v_i) - f(x; \theta)\bigr).\)</p> <hr/> <h2 id="results-for-fx--x2">Results for \(f(x) = x^2\)</h2> <p><strong>Eigenvalues:</strong></p> <p>(We take the absoluate value. Note that there are negative eigenvalues at the tail, after the dip)</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Eigenvectors (perturbed functions):</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_x2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="results-for-fx--sin5x">Results for \(f(x) = \sin(5x)\)</h2> <p><strong>Eigenvalues:</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_2layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvalues_3layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Eigenvectors (perturbed functions):</strong></p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_2layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-480.webp 480w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-800.webp 800w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-1400.webp 1400w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-1920.webp 1920w,/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-4/eigenvectors_3layer_mlp_sin5x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <ul> <li> <p>In both cases, we find that the leading eigenvalues and eigenmodes are very similar for 2-layer and 3-layer networks. However, the 3-layer networks exhibit more high-frequency modes in the tail of the spectrum. Notably, the highest-frequency modes correspond to eigenvalues that are close to zero.</p> </li> <li> <p>These results help explain why continual learning is challenging: because these eigenfunctions are not localized, modifying the model to fit a single new data point can induce changes in the function globally, rather than locally.</p> </li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1pNSXF_DPza5LPeKEHOuRxoJuPaOKpRba?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-4</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 4 -- Flat directions (in weight space) are high frequency modes (in function space)}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-4/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Depth"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Depth 3 – Fun facts about loss hessian eigenvalues</title><link href="https://kindxiaoming.github.io/blog/2026/depth-3/" rel="alternate" type="text/html" title="Depth 3 – Fun facts about loss hessian eigenvalues"/><published>2026-02-02T00:00:00+00:00</published><updated>2026-02-02T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/depth-3</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/depth-3/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>The loss landscape is a first-principles object governing neural network learning. To characterize the loss landscape, <em>sharpness</em> is commonly used. Specifically, sharpness is defined in terms of the eigenvalues of \(H \equiv \frac{\partial^2 \ell}{\partial \theta^2},\) where \(\ell\) is the loss function, \(\theta\) denotes the model parameters, and \(H\) is the Hessian. Although computing sharpness is prohibitively expensive for large models, it is essential for understanding training dynamics.</p> <p>In this article, we explore several fun and perhaps surprising facts about sharpness in toy models. While we ultimately hope these observations can shed light on larger models, the goal here is simply to make careful observations and document simple empirical findings.</p> <hr/> <h2 id="problem-setup">Problem setup</h2> <p>We use the ResNet toy model described in <a href="/blog/2026/depth-1/">depth-1</a>. The input (embedding) dimension is \(n_{\rm embd}\). Each block is a two-layer MLP with hidden dimension \(n_{\rm hidden}\). Residual connections are used, and \(n_{\rm block}\) blocks are stacked in depth to form the toy ResNet.</p> <p>We adopt a teacher–student regression setup: a teacher network generates the targets, and a student network of the same architecture (but with different initialization) is trained using the MSE loss.</p> <hr/> <h2 id="spectrum-is-gapped-or-eigenvalues-form-clusters">Spectrum is gapped, or eigenvalues form clusters</h2> <p>We set \(n_{\rm embd} = 10 \equiv N\), \(n_{\rm hidden} = 40 = 4N\), and \(n_{\rm block} = 2 \equiv D\). The total number of parameters is \(8N^2D = 1600\). Accordingly, the Hessian is a \(1600 \times 1600\) matrix with 1600 eigenvalues. We rank the eigenvalues from largest to smallest:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/gap-480.webp 480w,/assets/img/blogs/depth-3/gap-800.webp 800w,/assets/img/blogs/depth-3/gap-1400.webp 1400w,/assets/img/blogs/depth-3/gap-1920.webp 1920w,/assets/img/blogs/depth-3/gap-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/gap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that the spectrum (eigenvalue distribution) is clearly gapped:</p> <ul> <li>The first 9 eigenvalues form a cluster.</li> <li>The next 9 eigenvalues form another cluster.</li> <li>The first 100 eigenvalues are significantly larger than zero, while the remaining eigenvalues are close to zero.</li> </ul> <p>These observations naturally raise several interesting questions and hypotheses:</p> <ul> <li>Why 9? We hypothesize that this number is \(N - 1\) (where \(N\) is the embedding dimension). We will show this is true.</li> <li>Why two large clusters? We initially hypothesize that this is because \(n_{\rm block} = 2\). We will show this is false—there are always two clusters, regardless of how deep the ResNet is.</li> <li>Why 100? We hypothesize that this number is \(N^2\). We will show this is true.</li> </ul> <hr/> <h2 id="experiment-1-vary-n-embedding-dimension">Experiment 1: Vary \(N\) (embedding dimension)</h2> <p>We vary \(N \in \{10, 15, 20\}\) while keeping \(n_{\rm hidden} = 4N\) and using two blocks.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/N-480.webp 480w,/assets/img/blogs/depth-3/N-800.webp 800w,/assets/img/blogs/depth-3/N-1400.webp 1400w,/assets/img/blogs/depth-3/N-1920.webp 1920w,/assets/img/blogs/depth-3/N-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/N.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The number of eigenvalues in each cluster is \(N - 1\).</li> <li>The number of clusters remains equal to 2.</li> <li>The number of significantly non-zero eigenvalues is \(N^2\).</li> </ul> <hr/> <h2 id="experiment-2-vary-d-depth">Experiment 2: Vary \(D\) (depth)</h2> <p>We vary \(D \in \{2, 4, 8\}\) while keeping \(N = 10\) and \(n_{\rm hidden} = 4N\).</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/depth-480.webp 480w,/assets/img/blogs/depth-3/depth-800.webp 800w,/assets/img/blogs/depth-3/depth-1400.webp 1400w,/assets/img/blogs/depth-3/depth-1920.webp 1920w,/assets/img/blogs/depth-3/depth-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/depth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We observe that:</p> <ul> <li>The overall eigenvalue structure remains almost unchanged.</li> <li>The largest eigenvalue (i.e., sharpness) grows roughly linearly with depth.</li> </ul> <p>In fact, if we rescale the eigenvalues by the inverse of the depth, the normalized spectra approximately align across different depths.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 80%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/depth-3/depth-alignment-480.webp 480w,/assets/img/blogs/depth-3/depth-alignment-800.webp 800w,/assets/img/blogs/depth-3/depth-alignment-1400.webp 1400w,/assets/img/blogs/depth-3/depth-alignment-1920.webp 1920w,/assets/img/blogs/depth-3/depth-alignment-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/depth-3/depth-alignment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="question">Question</h2> <ul> <li>Although we made a few conclusions via induction, we did not provide sufficient explanation.</li> <li>The depth alignment result naturally raises the question: why do we need deep neural networks at all? Given the depth-alignment results, it is very likely that increasing depth mainly introduces more <em>small</em> eigenvalues, whereas increasing width introduces more <em>large</em> eigenvalues. When do large eigenvalues matter, and when do small eigenvalues matter? The eigen-spectrum provides a new perspective into understanding width-depth trade-off.</li> </ul> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1Y5BHija00MLjY6mdgCoJKWClnCgt4dAB?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026depth-3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Depth 3 -- Fun facts about loss hessian eigenvalues}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/depth-3/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Depth"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Diffusion 2 – Visualizing flow matching, temporal dynamics</title><link href="https://kindxiaoming.github.io/blog/2026/diffusion-2/" rel="alternate" type="text/html" title="Diffusion 2 – Visualizing flow matching, temporal dynamics"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/diffusion-2</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/diffusion-2/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reproducing Tianhong and Kaiming’s JIT paper, <a href="https://arxiv.org/pdf/2511.13720">“Back to Basics: Let Denoising Generative Models Denoise”</a>. The first step is to implement the v-prediction baseline on the spiral dataset (Figure 2 in the paper). Reproducing the baseline should be straightforward, but I ended up getting stuck for quite a while. Eventually, I realized that how \(t\) is sampled during training is crucial.</p> <p>Specifically, \(t\) is sampled from a logit-normal distribution: \(t = \frac{1}{1 + e^{-X}}, \quad X \sim \mathcal{N}(\mu, \sigma^2).\) However, the main paper does not clearly specify how \((\mu, \sigma)\) are chosen. I therefore experimented with different values to understand their effects. Note that \(t = 0\) corresponds to pure noise, while \(t = 1\) corresponds to clean data.</p> <hr/> <h2 id="tweaking-mu-and-sigma">Tweaking \(\mu\) and \(\sigma\)</h2> <p>We first fix \(\sigma = 2\) to ensure sufficient coverage of the time range, and sweep over \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/mu-480.webp 480w,/assets/img/blogs/diffusion-2/mu-800.webp 800w,/assets/img/blogs/diffusion-2/mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We find that \(\mu = 2\) yields the best generation results. We then fix \(\mu = 2\) and sweep over \(\sigma\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma-480.webp 480w,/assets/img/blogs/diffusion-2/sigma-800.webp 800w,/assets/img/blogs/diffusion-2/sigma-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Somewhat surprisingly, \(\sigma = 0\) can still lead to decent generation quality. We therefore fix \(\sigma = 0\) and vary \(\mu\):</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/sigma_0_mu-480.webp 480w,/assets/img/blogs/diffusion-2/sigma_0_mu-800.webp 800w,/assets/img/blogs/diffusion-2/sigma_0_mu-1400.webp 1400w,/assets/img/blogs/diffusion-2/sigma_0_mu-1920.webp 1920w,/assets/img/blogs/diffusion-2/sigma_0_mu-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/sigma_0_mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To better understand why \(\sigma = 0\) can still produce good samples, note that \(\sigma = 0\) means training only ever sees a single time \(t = \frac{1}{1 + e^{-\mu}}.\) Below, instead of training a neural network, we numerically estimate the true velocity field and visualize it directly.</p> <hr/> <h2 id="visualizing-true-velocities">Visualizing true velocities</h2> <p>For different values of \(t\), we visualize the velocity field using two color plots (a full vector-field visualization would be difficult to interpret due to potential multi-scale structure). Top: \(v_x\); bottom: \(v_y\). In each subplot, black dots indicate data samples, blue dots indicate generated samples at that time step, and red dashed lines show the zero-level sets.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 100%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion-2/v-vis-480.webp 480w,/assets/img/blogs/diffusion-2/v-vis-800.webp 800w,/assets/img/blogs/diffusion-2/v-vis-1400.webp 1400w,/assets/img/blogs/diffusion-2/v-vis-1920.webp 1920w,/assets/img/blogs/diffusion-2/v-vis-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion-2/v-vis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This makes it clear why training at a single \(t \sim 0.9\) can already yield high-quality samples: the zero-velocity contours align well with the spiral manifold. Under this velocity field, samples naturally converge toward the zero-velocity region.</p> <p>We also notice that that scale of \(v\) diverges as \(1/(1-t)\) when \(t\to 1\), which is another reason why predicting \(v\) is hard, besides JIT paper’s reasoning about manifolds.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/1G7JUb5HDh6hHgYirsVF7LAfPyNr0CKa2?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026diffusion-2</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Diffusion 2 -- Visualizing flow matching, temporal dynamics}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{February}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/diffusion-2/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Diffusion"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry><entry><title type="html">Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”</title><link href="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/" rel="alternate" type="text/html" title="Sparse attention 7 – Stack of causal attention creates implicit positional embedding, and explaning “Loss in the middle”"/><published>2026-01-31T00:00:00+00:00</published><updated>2026-01-31T00:00:00+00:00</updated><id>https://kindxiaoming.github.io/blog/2026/sparse-attention-7</id><content type="html" xml:base="https://kindxiaoming.github.io/blog/2026/sparse-attention-7/"><![CDATA[<p><strong>Author: Ziming Liu (刘子鸣)</strong></p> <hr/> <h2 id="motivation">Motivation</h2> <p>I was reading the paper <a href="https://arxiv.org/pdf/2501.00659">“Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph”</a>, which makes an interesting claim: even without explicit positional embeddings, multi-layer causal attention may implicitly encode positional information, despite the fact that a single-layer causal attention is permutation equivariant. Figure 1 from the paper is shown below:</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/fig1-480.webp 480w,/assets/img/blogs/sparse-attention-7/fig1-800.webp 800w,/assets/img/blogs/sparse-attention-7/fig1-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/fig1-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/fig1-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The goal of this article is to measure how well multi-layer causal attention can extract previous tokens based purely on position. Given a sequence of random tokens (with context length \(C\)), the target is the \(k^{\rm th}\) token in the sequence (\(0 \leq k \leq C-1\)). For example, when \(C = 5\) and \(k = 1\), example sequences are</p> \[[5]\ [2]\ [3]\ [7]\ [9] \to [2]\] \[[1]\ [9]\ [2]\ [5]\ [8] \to [9]\] <p>We set the vocabulary size to \(V = 10\), context length to \(C = 10\), and the number of attention heads to \(n_{\rm head} = 1\). We are interested in varying the number of layers \(n_{\rm layer}\), the embedding dimension \(n_{\rm embd}\), whether residual connections are used, and whether MLPs are included.</p> <hr/> <h2 id="2l-is-qualitatively-different-from-1l">2L is qualitatively different from 1L</h2> <p>For each \(0 \leq k \leq C-1\), we train the transformer to convergence and record the final loss. We then plot how the final loss depends on \(k\). This roughly measures positional bias: a high loss means that retrieving information from that position is difficult—the loss cannot be driven down even when the model is explicitly trained on the retrieval task.</p> <p>In <a href="/blog/2026/sparse-attention-2/">sparse-attention-2</a>, we showed that a 1-layer causal attention model cannot retrieve any previous token (\(k \neq C-1\)), but can retrieve the current token (\(k = C-1\)). This corresponds to the left plot below (high loss for \(k \neq 9\), and almost zero loss for \(k = 9\)). The right plot shows the result for a 2-layer causal attention model, which exhibits the well-known <a href="https://arxiv.org/pdf/2307.03172">“lost in the middle” phenomenon</a>: early and late tokens are easily retrieved, while tokens in the middle are difficult to extract.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/1L-2L-480.webp 480w,/assets/img/blogs/sparse-attention-7/1L-2L-800.webp 800w,/assets/img/blogs/sparse-attention-7/1L-2L-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/1L-2L-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/1L-2L-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/1L-2L.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="embedding-dimension-and-number-of-layers-do-not-change-the-qualitative-behavior">Embedding dimension and number of layers do not change the qualitative behavior</h2> <p>We experiment with embedding dimensions \(\{2, 10\}\) and number of layers \(\{2, 3\}\). The qualitative trend remains unchanged: only the first and last tokens can be retrieved reliably, while tokens in the middle remain difficult. This manifests as a long plateau in the loss curve across middle positions.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/embd-layer-480.webp 480w,/assets/img/blogs/sparse-attention-7/embd-layer-800.webp 800w,/assets/img/blogs/sparse-attention-7/embd-layer-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/embd-layer-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/embd-layer-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/embd-layer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="residual-connections-and-mlps-destroy-the-long-plateau">Residual connections and MLPs destroy the long plateau</h2> <p>When residual connections and MLPs are added, the long plateau disappears and is replaced by a more spiky structure.</p> <div class="row mt-3"> <div class="mt-3 mt-md-0" style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/sparse-attention-7/residual-mlp-480.webp 480w,/assets/img/blogs/sparse-attention-7/residual-mlp-800.webp 800w,/assets/img/blogs/sparse-attention-7/residual-mlp-1400.webp 1400w,/assets/img/blogs/sparse-attention-7/residual-mlp-1920.webp 1920w,/assets/img/blogs/sparse-attention-7/residual-mlp-2560.webp 2560w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/sparse-attention-7/residual-mlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="comment">Comment</h2> <p>This study suggests a simple mechanism underlying the “lost in the middle” phenomenon: even without explicit positional embeddings, stacking multiple causal-attention layers naturally induces an architectural bias toward losing information in the middle of the sequence.</p> <hr/> <h2 id="code">Code</h2> <p>Google Colab notebook available <a href="https://colab.research.google.com/drive/14OOPXMz4C40aIqOoWSxBzCH7hp3g3GZ9?usp=sharing">here</a>.</p> <hr/> <h2 id="citation">Citation</h2> <p>If you find this article useful, please cite it as:</p> <p><strong>BibTeX:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2026sparse-attention-7</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Sparse attention 7 -- Stack of causal attention creates implicit positional embedding, and explaning "Loss in the middle" }</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ziming}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{January}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://KindXiaoming.github.io/blog/2026/sparse-attention-7/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Physics-of-AI"/><category term="Sparse-attention"/><summary type="html"><![CDATA[Author: Ziming Liu (刘子鸣)]]></summary></entry></feed>